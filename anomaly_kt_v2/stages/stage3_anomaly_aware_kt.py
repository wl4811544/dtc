"""
ç¬¬ä¸‰é˜¶æ®µï¼šå¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ª

å°†ç¬¬ä¸€é˜¶æ®µçš„åŸºçº¿æ¨¡å‹å’Œç¬¬äºŒé˜¶æ®µçš„å¼‚å¸¸æ£€æµ‹å™¨èåˆï¼Œ
å®ç°å¼‚å¸¸æ„ŸçŸ¥çš„çŸ¥è¯†è¿½è¸ªï¼Œç›®æ ‡æå‡AUC 0.05-0.1ã€‚
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from anomaly_kt_v2.anomaly_aware import AnomalyAwareKT, AnomalyAwareTrainer
from anomaly_kt_v2.anomaly_detection import CausalAnomalyDetector
from anomaly_kt_v2.core.common import print_stage_header, print_training_summary
from DTransformer.model import DTransformer


def validate_stage3_parameters(args, dataset_config):
    """éªŒè¯ç¬¬ä¸‰é˜¶æ®µå‚æ•°"""
    print("ğŸ” éªŒè¯ç¬¬ä¸‰é˜¶æ®µå‚æ•°...")
    
    # æ£€æŸ¥å¿…éœ€çš„åŸºæœ¬å‚æ•°
    required_basic = ['device', 'output_dir', 'baseline_model_path', 'anomaly_detector_path']
    for param in required_basic:
        if not hasattr(args, param) or getattr(args, param) is None:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åŸºæœ¬å‚æ•°: {param}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.baseline_model_path):
        raise FileNotFoundError(f"åŸºçº¿æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.baseline_model_path}")
    
    if not os.path.exists(args.anomaly_detector_path):
        raise FileNotFoundError(f"å¼‚å¸¸æ£€æµ‹å™¨æ–‡ä»¶ä¸å­˜åœ¨: {args.anomaly_detector_path}")
    
    # æ£€æŸ¥å¿…éœ€çš„è®­ç»ƒå‚æ•°
    required_training = ['fusion_epochs', 'joint_epochs', 'finetune_epochs', 'learning_rate', 'patience']
    for param in required_training:
        if not hasattr(args, param):
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„è®­ç»ƒå‚æ•°: {param}")
    
    # è®¾ç½®å¯é€‰å‚æ•°çš„é»˜è®¤å€¼
    optional_params = {
        'with_pid': True,
        'fusion_type': 'attention',
        'enable_context_enhancement': True,
        'lambda_anomaly': 0.1,
        'freeze_pretrained': True
    }
    
    for param, default_value in optional_params.items():
        if not hasattr(args, param):
            setattr(args, param, default_value)
    
    print("âœ… ç¬¬ä¸‰é˜¶æ®µå‚æ•°éªŒè¯é€šè¿‡")


def print_stage3_parameters(args, dataset_config):
    """æ‰“å°ç¬¬ä¸‰é˜¶æ®µå‚æ•°é…ç½®"""
    print("\nğŸ“‹ ç¬¬ä¸‰é˜¶æ®µå‚æ•°é…ç½®:")
    
    print("  åŸºæœ¬å‚æ•°:")
    print(f"    è®¾å¤‡: {args.device}")
    print(f"    è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"    åŸºçº¿æ¨¡å‹: {args.baseline_model_path}")
    print(f"    å¼‚å¸¸æ£€æµ‹å™¨: {args.anomaly_detector_path}")
    print(f"    ä½¿ç”¨é—®é¢˜ID: {args.with_pid}")
    
    print("  èåˆå‚æ•°:")
    print(f"    èåˆç±»å‹: {args.fusion_type}")
    print(f"    ä¸Šä¸‹æ–‡å¢å¼º: {args.enable_context_enhancement}")
    print(f"    å†»ç»“é¢„è®­ç»ƒæ¨¡å‹: {args.freeze_pretrained}")
    print(f"    å¼‚å¸¸æŸå¤±æƒé‡: {args.lambda_anomaly}")
    
    print("  æ¸è¿›å¼è®­ç»ƒå‚æ•°:")
    print(f"    èåˆå±‚è®­ç»ƒè½®æ•°: {args.fusion_epochs}")
    print(f"    è”åˆè®­ç»ƒè½®æ•°: {args.joint_epochs}")
    print(f"    ç«¯åˆ°ç«¯å¾®è°ƒè½®æ•°: {args.finetune_epochs}")
    print(f"    å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"    æ—©åœè€å¿ƒ: {args.patience}")
    
    print("  æ•°æ®é›†å‚æ•°:")
    print(f"    é—®é¢˜æ€»æ•°: {dataset_config['n_questions']}")
    print(f"    é—®é¢˜IDæ€»æ•°: {dataset_config['n_pid']}")


def load_pretrained_models(args, dataset_config):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
    print("\nğŸ“„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    
    # åŠ è½½åŸºçº¿æ¨¡å‹
    print("ğŸ”§ åŠ è½½åŸºçº¿çŸ¥è¯†è¿½è¸ªæ¨¡å‹...")
    baseline_checkpoint = torch.load(args.baseline_model_path, map_location='cpu', weights_only=False)
    
    # ä»checkpointä¸­è·å–æ¨¡å‹é…ç½®
    if 'args' in baseline_checkpoint:
        baseline_args = baseline_checkpoint['args']
        d_model = getattr(baseline_args, 'd_model', 128)
        n_heads = getattr(baseline_args, 'n_heads', 8)
        n_layers = getattr(baseline_args, 'n_layers', 3)
        dropout = getattr(baseline_args, 'dropout', 0.2)
    else:
        # é»˜è®¤é…ç½®
        d_model = 128
        n_heads = 8
        n_layers = 3
        dropout = 0.2
    
    # åˆ›å»ºåŸºçº¿æ¨¡å‹
    baseline_model = DTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=d_model,
        n_heads=n_heads,
        n_layers=n_layers,
        dropout=dropout,
        n_know=dataset_config.get('n_know', 16)
    )
    
    # åŠ è½½æƒé‡
    baseline_model.load_state_dict(baseline_checkpoint['model_state_dict'])
    baseline_auc = baseline_checkpoint.get('auc', 0.0)
    
    print(f"âœ… åŸºçº¿æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
    print(f"  åŸºçº¿AUC: {baseline_auc:.4f}")
    
    # åŠ è½½å¼‚å¸¸æ£€æµ‹å™¨
    print("ğŸ”§ åŠ è½½å¼‚å¸¸æ£€æµ‹å™¨...")
    anomaly_checkpoint = torch.load(args.anomaly_detector_path, map_location='cpu', weights_only=False)
    
    # åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨
    anomaly_detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=d_model,
        n_heads=n_heads,
        n_layers=2,  # å¼‚å¸¸æ£€æµ‹å™¨é€šå¸¸å±‚æ•°è¾ƒå°‘
        dropout=0.1,
        window_size=10
    )
    
    # åŠ è½½æƒé‡
    anomaly_detector.load_state_dict(anomaly_checkpoint['model_state_dict'])
    anomaly_auc = anomaly_checkpoint.get('auc', 0.0)
    
    print(f"âœ… å¼‚å¸¸æ£€æµ‹å™¨åŠ è½½æˆåŠŸ")
    print(f"  æ£€æµ‹å™¨AUC: {anomaly_auc:.4f}")
    
    return baseline_model, anomaly_detector, d_model, baseline_auc


def train_anomaly_aware_kt(args, dataset_config, train_data, val_data):
    """
    è®­ç»ƒå¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡å‹
    
    Args:
        args: è®­ç»ƒå‚æ•°
        dataset_config: æ•°æ®é›†é…ç½®
        train_data: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_data: éªŒè¯æ•°æ®åŠ è½½å™¨
        
    Returns:
        str: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
    """
    print_stage_header("å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ª", 3)
    
    # éªŒè¯å‚æ•°
    validate_stage3_parameters(args, dataset_config)
    
    # æ‰“å°å‚æ•°é…ç½®
    print_stage3_parameters(args, dataset_config)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    baseline_model, anomaly_detector, d_model, baseline_auc = load_pretrained_models(args, dataset_config)
    
    # åˆ›å»ºå¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºå¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡å‹...")
    anomaly_aware_kt = AnomalyAwareKT(
        baseline_model=baseline_model,
        anomaly_detector=anomaly_detector,
        d_model=d_model,
        fusion_type=args.fusion_type,
        enable_context_enhancement=args.enable_context_enhancement,
        freeze_pretrained=args.freeze_pretrained,
        dropout=0.1
    )
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = anomaly_aware_kt.get_model_info()
    
    print(f"âœ… å¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  æ€»å‚æ•°æ•°: {model_info['total_parameters']:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {model_info['trainable_parameters']:,}")
    print(f"  å†»ç»“å‚æ•°: {model_info['frozen_parameters']:,}")
    print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {model_info['trainable_ratio']:.1%}")
    print(f"  èåˆç±»å‹: {model_info['fusion_type']}")
    print(f"  ä¸Šä¸‹æ–‡å¢å¼º: {model_info['context_enhancement']}")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    anomaly_aware_kt.to(args.device)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\nğŸš€ åˆ›å»ºæ¸è¿›å¼è®­ç»ƒå™¨...")
    save_dir = os.path.join(args.output_dir, 'anomaly_aware_kt')
    trainer = AnomalyAwareTrainer(
        model=anomaly_aware_kt,
        device=args.device,
        learning_rate=args.learning_rate,
        save_dir=save_dir,
        patience=args.patience
    )
    
    print(f"âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    print(f"  ä¿å­˜ç›®å½•: {save_dir}")
    
    # å¼€å§‹æ¸è¿›å¼è®­ç»ƒ
    print("\n" + "="*60)
    print("å¼€å§‹å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªè®­ç»ƒ...")
    print("="*60)
    
    try:
        training_result = trainer.progressive_train(
            train_loader=train_data,
            val_loader=val_data,
            stage1_epochs=args.fusion_epochs,
            stage2_epochs=args.joint_epochs,
            stage3_epochs=args.finetune_epochs,
            lambda_anomaly=args.lambda_anomaly
        )
        
        # æ‰“å°è®­ç»ƒæ€»ç»“
        print_training_summary("å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ª", training_result, save_dir)
        
        # è®¡ç®—æ€§èƒ½æå‡
        final_auc = training_result['best_auc']
        auc_improvement = final_auc - baseline_auc
        
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print(f"  åŸºçº¿æ¨¡å‹AUC: {baseline_auc:.4f}")
        print(f"  å¼‚å¸¸æ„ŸçŸ¥AUC: {final_auc:.4f}")
        print(f"  æ€§èƒ½æå‡: {auc_improvement:+.4f}")
        
        if auc_improvement >= 0.05:
            print(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡ï¼AUCæå‡ {auc_improvement:.4f} >= 0.05")
        elif auc_improvement >= 0.03:
            print(f"âœ… æ˜¾è‘—æå‡ï¼AUCæå‡ {auc_improvement:.4f}")
        elif auc_improvement > 0:
            print(f"ğŸ“ˆ æœ‰æ‰€æå‡ï¼ŒAUCæå‡ {auc_improvement:.4f}")
        else:
            print(f"âš ï¸ æ€§èƒ½æœªæå‡ï¼Œéœ€è¦è°ƒä¼˜")
        
        # æ¨¡å‹è·¯å¾„
        model_path = os.path.join(save_dir, 'best_anomaly_aware_kt.pt')
        print(f"\nğŸ’¾ å¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        return model_path
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
