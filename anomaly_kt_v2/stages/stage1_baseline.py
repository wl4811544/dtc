"""
Stage 1: åŸºçº¿DTransformeræ¨¡å‹è®­ç»ƒ

è®­ç»ƒæ ‡å‡†çš„DTransformerçŸ¥è¯†è¿½è¸ªæ¨¡å‹ä½œä¸ºåç»­å¼‚å¸¸æ„ŸçŸ¥è®­ç»ƒçš„åŸºçº¿
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from DTransformer.model import DTransformer
from DTransformer.eval import Evaluator
from anomaly_kt_v2.core.common import print_stage_header, print_training_summary


def validate_stage1_parameters(args, dataset_config):
    """éªŒè¯ç¬¬ä¸€é˜¶æ®µæ‰€éœ€çš„å‚æ•°

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        dataset_config: æ•°æ®é›†é…ç½®å­—å…¸

    Raises:
        ValueError: å¦‚æœç¼ºå°‘å¿…éœ€å‚æ•°
    """
    # æ£€æŸ¥åŸºæœ¬å‚æ•°
    required_basic_params = ['device', 'output_dir']
    for param in required_basic_params:
        if not hasattr(args, param) or getattr(args, param) is None:
            raise ValueError(f"Missing required parameter: {param}")

    # æ£€æŸ¥æ¨¡å‹å‚æ•°
    required_model_params = [
        'd_model', 'n_heads', 'n_know', 'n_layers',
        'dropout', 'lambda_cl', 'window'
    ]
    for param in required_model_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required model parameter: {param}")

    # æ£€æŸ¥è®­ç»ƒå‚æ•°
    required_training_params = ['kt_epochs', 'learning_rate', 'patience']
    for param in required_training_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required training parameter: {param}")

    # æ£€æŸ¥æ•°æ®é›†é…ç½®
    required_dataset_params = ['n_questions', 'n_pid']
    for param in required_dataset_params:
        if param not in dataset_config:
            raise ValueError(f"Missing required dataset parameter: {param}")

    # æ£€æŸ¥å¯é€‰å‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
    if not hasattr(args, 'with_pid'):
        args.with_pid = False
    if not hasattr(args, 'proj'):
        args.proj = False
    if not hasattr(args, 'hard_neg'):
        args.hard_neg = False
    if not hasattr(args, 'use_cl'):
        args.use_cl = False

    print("âœ… ç¬¬ä¸€é˜¶æ®µå‚æ•°éªŒè¯é€šè¿‡")


def print_stage1_parameters(args, dataset_config):
    """æ‰“å°ç¬¬ä¸€é˜¶æ®µå‚æ•°ä¿¡æ¯"""
    print("\nğŸ“‹ ç¬¬ä¸€é˜¶æ®µå‚æ•°é…ç½®:")
    print("  åŸºæœ¬å‚æ•°:")
    print(f"    è®¾å¤‡: {args.device}")
    print(f"    è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"    ä½¿ç”¨é—®é¢˜ID: {getattr(args, 'with_pid', False)}")

    print("  æ¨¡å‹å‚æ•°:")
    print(f"    æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"    æ³¨æ„åŠ›å¤´æ•°: {args.n_heads}")
    print(f"    çŸ¥è¯†æ¦‚å¿µæ•°: {args.n_know}")
    print(f"    å±‚æ•°: {args.n_layers}")
    print(f"    Dropout: {args.dropout}")
    print(f"    å¯¹æ¯”å­¦ä¹ æƒé‡: {args.lambda_cl}")
    print(f"    ä½¿ç”¨æŠ•å½±: {getattr(args, 'proj', False)}")
    print(f"    å›°éš¾è´Ÿæ ·æœ¬: {getattr(args, 'hard_neg', False)}")
    print(f"    çª—å£å¤§å°: {args.window}")

    print("  è®­ç»ƒå‚æ•°:")
    print(f"    è®­ç»ƒè½®æ•°: {args.kt_epochs}")
    print(f"    å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"    æ—©åœè€å¿ƒ: {args.patience}")
    print(f"    ä½¿ç”¨å¯¹æ¯”å­¦ä¹ : {getattr(args, 'use_cl', False)}")

    print("  æ•°æ®é›†å‚æ•°:")
    print(f"    é—®é¢˜æ€»æ•°: {dataset_config['n_questions']}")
    print(f"    é—®é¢˜IDæ€»æ•°: {dataset_config['n_pid']}")


def train_baseline_model(args, dataset_config, train_data, val_data):
    """è®­ç»ƒåŸºçº¿DTransformeræ¨¡å‹

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«æ¨¡å‹å’Œè®­ç»ƒé…ç½®
        dataset_config: æ•°æ®é›†é…ç½®ï¼ŒåŒ…å« n_questions, n_pid ç­‰
        train_data: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_data: éªŒè¯æ•°æ®åŠ è½½å™¨

    Returns:
        str: è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
    """
    # éªŒè¯å‚æ•°
    validate_stage1_parameters(args, dataset_config)

    # æ‰“å°é˜¶æ®µæ ‡é¢˜
    print_stage_header("åŸºçº¿DTransformeræ¨¡å‹è®­ç»ƒ", 1)

    # æ‰“å°å‚æ•°ä¿¡æ¯
    print_stage1_parameters(args, dataset_config)

    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºDTransformeræ¨¡å‹...")
    model = DTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_know=args.n_know,
        n_layers=args.n_layers,
        dropout=args.dropout,
        lambda_cl=args.lambda_cl,
        proj=args.proj,
        hard_neg=args.hard_neg,
        window=args.window
    )

    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # è®¾ç½®è®¾å¤‡
    model.to(args.device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    print("\nğŸš€ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluator()

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = os.path.join(args.output_dir, 'baseline')
    os.makedirs(save_dir, exist_ok=True)

    print(f"âœ… è®­ç»ƒå‡†å¤‡å®Œæˆ")
    print(f"  ä¿å­˜ç›®å½•: {save_dir}")
    print(f"  ä¼˜åŒ–å™¨: Adam")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")

    # å¼€å§‹è®­ç»ƒ
    print("\n" + "="*60)
    print("å¼€å§‹åŸºçº¿æ¨¡å‹è®­ç»ƒ...")
    print("="*60)

    # è®­ç»ƒå¾ªç¯
    best_auc = 0
    best_epoch = 0
    patience_counter = 0

    for epoch in range(1, args.kt_epochs + 1):
        print(f"\nEpoch {epoch}/{args.kt_epochs}")

        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        total_pred_loss = 0.0
        total_cl_loss = 0.0
        total_cnt = 0

        from tqdm import tqdm
        for batch in tqdm(train_data, desc="Training"):
            # è·å–æ‰¹æ¬¡æ•°æ®
            if args.with_pid:
                q, s, pid = batch.get("q", "s", "pid")
            else:
                q, s = batch.get("q", "s")
                pid = None

            q = q.to(args.device)
            s = s.to(args.device)
            if pid is not None:
                pid = pid.to(args.device)

            # å‰å‘ä¼ æ’­
            if args.use_cl:
                loss, pred_loss, cl_loss = model.get_cl_loss(q, s, pid)
                total_pred_loss += pred_loss.item()
                total_cl_loss += cl_loss.item()
            else:
                loss = model.get_loss(q, s, pid)
                pred_loss = loss
                cl_loss = torch.tensor(0.0)
                total_pred_loss += pred_loss.item()

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            total_cnt += 1

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / total_cnt
        avg_pred_loss = total_pred_loss / total_cnt
        avg_cl_loss = total_cl_loss / total_cnt if args.use_cl else 0.0

        # éªŒè¯é˜¶æ®µ
        model.eval()
        with torch.no_grad():
            # æ”¶é›†éªŒè¯æ•°æ®çš„é¢„æµ‹ç»“æœ
            for batch in val_data:
                if args.with_pid:
                    q, s, pid = batch.get("q", "s", "pid")
                else:
                    q, s = batch.get("q", "s")
                    pid = None

                q = q.to(args.device)
                s = s.to(args.device)
                if pid is not None:
                    pid = pid.to(args.device)

                # è·å–é¢„æµ‹ç»“æœ (åªå–logitsï¼Œå¿½ç•¥å…¶ä»–è¿”å›å€¼)
                pred, *_ = model.predict(q, s, pid)

                # å‡†å¤‡çœŸå®æ ‡ç­¾å’Œé¢„æµ‹å€¼
                # ä½¿ç”¨sigmoidå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
                pred_probs = torch.sigmoid(pred)

                # åªè¯„ä¼°æœ‰æ•ˆä½ç½® (s >= 0)
                valid_mask = s >= 0
                target_flat = s[valid_mask].float()
                pred_flat = pred_probs[valid_mask]

                evaluator.evaluate(target_flat, pred_flat)

            # è·å–è¯„ä¼°ç»“æœ
            val_metrics = evaluator.report()

            # é‡ç½®è¯„ä¼°å™¨ä¸ºä¸‹ä¸€è½®å‡†å¤‡
            evaluator = Evaluator()

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f"  Train Loss: {avg_loss:.4f} (Pred: {avg_pred_loss:.4f}, CL: {avg_cl_loss:.4f})")
        print(f"  Val AUC: {val_metrics['auc']:.4f}, Val ACC: {val_metrics['acc']:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['auc'] > best_auc:
            best_auc = val_metrics['auc']
            best_epoch = epoch
            patience_counter = 0

            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(save_dir, 'best_model.pt')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'auc': val_metrics['auc'],
                'acc': val_metrics['acc'],
                'args': args
            }, model_path)

            print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (AUC: {best_auc:.4f})")
        else:
            patience_counter += 1
            print(f"  â³ æ— æ”¹è¿› ({patience_counter}/{args.patience})")

        # æ—©åœæ£€æŸ¥
        if patience_counter >= args.patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼æœ€ä½³AUC: {best_auc:.4f} (Epoch {best_epoch})")
            break

    # è®­ç»ƒå®Œæˆ
    final_metrics = {
        'auc': best_auc,
        'acc': val_metrics['acc'],
        'best_epoch': best_epoch,
        'total_epochs': epoch
    }

    # æ‰“å°è®­ç»ƒæ€»ç»“
    print_training_summary("åŸºçº¿æ¨¡å‹", final_metrics, save_dir)

    model_path = os.path.join(save_dir, 'best_model.pt')
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model_path
