"""
ç¬¬äºŒé˜¶æ®µï¼šå¼‚å¸¸åˆ†ç±»å™¨è®­ç»ƒ

åŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒã€‚
å®Œå…¨åŸºäºDTransformeråŸå§‹ä»£ç ï¼Œä¸ä¾èµ–ä»»ä½•anomaly_ktæ¨¡å—ã€‚
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from anomaly_kt_v2.anomaly_detection import (
    CausalAnomalyDetector,
    CurriculumTrainer,
    AnomalyDetectionEvaluator
)
from anomaly_kt_v2.core.common import print_stage_header, print_training_summary


def validate_stage2_parameters(args, dataset_config):
    """éªŒè¯ç¬¬äºŒé˜¶æ®µå‚æ•°"""
    print("ğŸ” éªŒè¯ç¬¬äºŒé˜¶æ®µå‚æ•°...")
    
    # æ£€æŸ¥å¿…éœ€çš„åŸºæœ¬å‚æ•°
    required_basic = ['device', 'output_dir', 'baseline_model_path']
    for param in required_basic:
        if not hasattr(args, param) or getattr(args, param) is None:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åŸºæœ¬å‚æ•°: {param}")
    
    # æ£€æŸ¥å¿…éœ€çš„æ¨¡å‹å‚æ•°
    required_model = ['d_model', 'n_heads', 'n_layers', 'dropout']
    for param in required_model:
        if not hasattr(args, param):
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„æ¨¡å‹å‚æ•°: {param}")
    
    # æ£€æŸ¥å¿…éœ€çš„è®­ç»ƒå‚æ•°
    required_training = ['anomaly_epochs', 'learning_rate', 'patience']
    for param in required_training:
        if not hasattr(args, param):
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„è®­ç»ƒå‚æ•°: {param}")
    
    # æ£€æŸ¥æ•°æ®é›†é…ç½®
    required_dataset = ['n_questions', 'n_pid']
    for param in required_dataset:
        if param not in dataset_config:
            raise ValueError(f"æ•°æ®é›†é…ç½®ç¼ºå°‘å¿…éœ€å‚æ•°: {param}")
    
    # è®¾ç½®å¯é€‰å‚æ•°çš„é»˜è®¤å€¼
    optional_params = {
        'with_pid': True,
        'window_size': 10,
        'curriculum_type': 'linear',
        'initial_difficulty': 0.1,
        'final_difficulty': 0.8,
        'warmup_epochs': 5
    }
    
    for param, default_value in optional_params.items():
        if not hasattr(args, param):
            setattr(args, param, default_value)
    
    print("âœ… ç¬¬äºŒé˜¶æ®µå‚æ•°éªŒè¯é€šè¿‡")


def print_stage2_parameters(args, dataset_config):
    """æ‰“å°ç¬¬äºŒé˜¶æ®µå‚æ•°é…ç½®"""
    print("\nğŸ“‹ ç¬¬äºŒé˜¶æ®µå‚æ•°é…ç½®:")
    
    print("  åŸºæœ¬å‚æ•°:")
    print(f"    è®¾å¤‡: {args.device}")
    print(f"    è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"    åŸºçº¿æ¨¡å‹è·¯å¾„: {args.baseline_model_path}")
    print(f"    ä½¿ç”¨é—®é¢˜ID: {args.with_pid}")
    
    print("  å¼‚å¸¸æ£€æµ‹å™¨å‚æ•°:")
    print(f"    æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"    æ³¨æ„åŠ›å¤´æ•°: {args.n_heads}")
    print(f"    å±‚æ•°: {args.n_layers}")
    print(f"    Dropout: {args.dropout}")
    print(f"    ç»Ÿè®¡çª—å£å¤§å°: {args.window_size}")
    
    print("  è¯¾ç¨‹å­¦ä¹ å‚æ•°:")
    print(f"    è®­ç»ƒè½®æ•°: {args.anomaly_epochs}")
    print(f"    å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"    æ—©åœè€å¿ƒ: {args.patience}")
    print(f"    è¯¾ç¨‹ç±»å‹: {args.curriculum_type}")
    print(f"    åˆå§‹éš¾åº¦: {args.initial_difficulty}")
    print(f"    æœ€ç»ˆéš¾åº¦: {args.final_difficulty}")
    print(f"    é¢„çƒ­è½®æ•°: {args.warmup_epochs}")
    
    print("  æ•°æ®é›†å‚æ•°:")
    print(f"    é—®é¢˜æ€»æ•°: {dataset_config['n_questions']}")
    print(f"    é—®é¢˜IDæ€»æ•°: {dataset_config['n_pid']}")


def train_anomaly_classifier(args, dataset_config, train_data, val_data):
    """
    è®­ç»ƒå¼‚å¸¸åˆ†ç±»å™¨
    
    Args:
        args: è®­ç»ƒå‚æ•°
        dataset_config: æ•°æ®é›†é…ç½®
        train_data: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_data: éªŒè¯æ•°æ®åŠ è½½å™¨
        
    Returns:
        str: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
    """
    print_stage_header("å¼‚å¸¸åˆ†ç±»å™¨è®­ç»ƒ", 2)
    
    # éªŒè¯å‚æ•°
    validate_stage2_parameters(args, dataset_config)
    
    # æ‰“å°å‚æ•°é…ç½®
    print_stage2_parameters(args, dataset_config)
    
    # åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨
    print("\nğŸ”§ åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨...")
    detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        dropout=args.dropout,
        window_size=args.window_size
    )
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in detector.parameters())
    trainable_params = sum(p.numel() for p in detector.parameters() if p.requires_grad)
    
    print(f"âœ… å¼‚å¸¸æ£€æµ‹å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    detector.to(args.device)
    
    # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨
    print("\nğŸš€ åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨...")
    save_dir = os.path.join(args.output_dir, 'anomaly_classifier')
    trainer = CurriculumTrainer(
        model=detector,
        device=args.device,
        learning_rate=args.learning_rate,
        save_dir=save_dir,
        patience=args.patience,
        with_pid=args.with_pid
    )
    
    print(f"âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    print(f"  ä¿å­˜ç›®å½•: {save_dir}")
    
    # è¯¾ç¨‹å­¦ä¹ é…ç½®
    curriculum_config = {
        'initial_difficulty': args.initial_difficulty,
        'final_difficulty': args.final_difficulty,
        'schedule_type': args.curriculum_type,
        'warmup_epochs': args.warmup_epochs
    }
    
    print(f"\nğŸ“š è¯¾ç¨‹å­¦ä¹ é…ç½®:")
    for key, value in curriculum_config.items():
        print(f"  {key}: {value}")
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "="*60)
    print("å¼€å§‹å¼‚å¸¸åˆ†ç±»å™¨è®­ç»ƒ...")
    print("="*60)
    
    try:
        training_result = trainer.train(
            train_loader=train_data,
            val_loader=val_data,
            epochs=args.anomaly_epochs,
            curriculum_config=curriculum_config
        )
        
        # æ‰“å°è®­ç»ƒæ€»ç»“
        print_training_summary("å¼‚å¸¸åˆ†ç±»å™¨", training_result, save_dir)
        
        # æ¨¡å‹è·¯å¾„
        model_path = os.path.join(save_dir, 'best_anomaly_detector.pt')
        print(f"\nğŸ’¾ å¼‚å¸¸æ£€æµ‹å™¨å·²ä¿å­˜: {model_path}")
        
        # è¯„ä¼°æ¨¡å‹
        print("\nğŸ“Š è¯„ä¼°å¼‚å¸¸æ£€æµ‹å™¨æ€§èƒ½...")
        evaluator = AnomalyDetectionEvaluator()
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°
        detector.load_state_dict(torch.load(model_path, weights_only=False)['model_state_dict'])
        
        eval_result = evaluator.evaluate_model(
            model=detector,
            data_loader=val_data,
            device=args.device,
            anomaly_strategies=['random_flip', 'uniform_random', 'systematic_bias'],
            with_pid=args.with_pid
        )
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report_path = os.path.join(save_dir, 'evaluation_report.txt')
        report = evaluator.generate_report(eval_result, report_path)
        print(f"\nğŸ“‹ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°ç®€è¦è¯„ä¼°ç»“æœ
        if 'overall' in eval_result:
            overall = eval_result['overall']
            print(f"\nğŸ“ˆ å¼‚å¸¸æ£€æµ‹å™¨æ€§èƒ½:")
            print(f"  å¹³å‡AUC: {overall.get('avg_auc', 0):.4f}")
            print(f"  æ€§èƒ½ç­‰çº§: {overall.get('performance_grade', 'Unknown')}")
        
        return model_path
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def load_baseline_model_info(baseline_model_path: str) -> dict:
    """
    åŠ è½½åŸºçº¿æ¨¡å‹ä¿¡æ¯
    
    Args:
        baseline_model_path: åŸºçº¿æ¨¡å‹è·¯å¾„
        
    Returns:
        dict: åŸºçº¿æ¨¡å‹ä¿¡æ¯
    """
    if not os.path.exists(baseline_model_path):
        raise FileNotFoundError(f"åŸºçº¿æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {baseline_model_path}")
    
    # PyTorch 2.6+ å…¼å®¹æ€§ï¼šç¦ç”¨ weights_only ä»¥æ”¯æŒæ—§æ¨¡å‹æ–‡ä»¶
    checkpoint = torch.load(baseline_model_path, map_location='cpu', weights_only=False)
    
    # æå–æ¨¡å‹é…ç½®ä¿¡æ¯
    if 'args' in checkpoint:
        baseline_args = checkpoint['args']
        model_info = {
            'd_model': getattr(baseline_args, 'd_model', 128),
            'n_heads': getattr(baseline_args, 'n_heads', 8),
            'n_layers': getattr(baseline_args, 'n_layers', 3),
            'dropout': getattr(baseline_args, 'dropout', 0.2),
            'with_pid': getattr(baseline_args, 'with_pid', True),
            'baseline_auc': checkpoint.get('auc', 0.0),
            'baseline_epoch': checkpoint.get('epoch', 0)
        }
    else:
        # é»˜è®¤é…ç½®
        model_info = {
            'd_model': 128,
            'n_heads': 8,
            'n_layers': 3,
            'dropout': 0.2,
            'with_pid': True,
            'baseline_auc': checkpoint.get('auc', 0.0),
            'baseline_epoch': checkpoint.get('epoch', 0)
        }
    
    print(f"ğŸ“„ åŸºçº¿æ¨¡å‹ä¿¡æ¯:")
    print(f"  æ¨¡å‹ç»´åº¦: {model_info['d_model']}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {model_info['n_heads']}")
    print(f"  åŸºçº¿AUC: {model_info['baseline_auc']:.4f}")
    print(f"  è®­ç»ƒè½®æ•°: {model_info['baseline_epoch']}")
    
    return model_info
