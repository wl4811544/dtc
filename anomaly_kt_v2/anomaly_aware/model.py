"""
å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡å‹

å°†åŸºçº¿çŸ¥è¯†è¿½è¸ªæ¨¡å‹ä¸å¼‚å¸¸æ£€æµ‹å™¨èåˆï¼Œ
å®ç°å¼‚å¸¸æ„ŸçŸ¥çš„çŸ¥è¯†è¿½è¸ªï¼Œç›®æ ‡æå‡AUC 0.05-0.1ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple, Optional, Union
import sys
import os

# æ·»åŠ DTransformerè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from .fusion import AnomalyAwareFusion, ContextEnhancer


class AnomalyAwareKT(nn.Module):
    """
    å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡å‹
    
    æ ¸å¿ƒæ¶æ„ï¼š
    1. åŸºçº¿çŸ¥è¯†è¿½è¸ªæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
    2. å¼‚å¸¸æ£€æµ‹å™¨ï¼ˆå†»ç»“ï¼‰  
    3. å¼‚å¸¸æ„ŸçŸ¥èåˆå±‚ï¼ˆå¯è®­ç»ƒï¼‰
    """
    
    def __init__(self,
                 baseline_model: nn.Module,
                 anomaly_detector: nn.Module,
                 d_model: int,
                 fusion_type: str = 'attention',
                 enable_context_enhancement: bool = True,
                 freeze_pretrained: bool = True,
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡å‹
        
        Args:
            baseline_model: ç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„åŸºçº¿æ¨¡å‹
            anomaly_detector: ç¬¬äºŒé˜¶æ®µè®­ç»ƒçš„å¼‚å¸¸æ£€æµ‹å™¨
            d_model: æ¨¡å‹ç»´åº¦
            fusion_type: èåˆç±»å‹
            enable_context_enhancement: æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º
            freeze_pretrained: æ˜¯å¦å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
            dropout: Dropoutç‡
        """
        super().__init__()
        
        self.baseline_model = baseline_model
        self.anomaly_detector = anomaly_detector
        self.d_model = d_model
        self.enable_context_enhancement = enable_context_enhancement
        
        # å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
        if freeze_pretrained:
            self._freeze_pretrained_models()
        
        # å¼‚å¸¸æ„ŸçŸ¥èåˆå±‚
        self.anomaly_fusion = AnomalyAwareFusion(
            d_model=d_model,
            fusion_type=fusion_type,
            dropout=dropout
        )
        
        # ä¸Šä¸‹æ–‡å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
        if enable_context_enhancement:
            self.context_enhancer = ContextEnhancer(d_model=d_model)
        
        # æœ€ç»ˆé¢„æµ‹å±‚
        self.prediction_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1),
            nn.Sigmoid()
        )
        
        # å¤šä»»åŠ¡å­¦ä¹ çš„è¾…åŠ©æŸå¤±
        self.anomaly_consistency_head = nn.Sequential(
            nn.Linear(d_model, d_model // 4),
            nn.ReLU(),
            nn.Linear(d_model // 4, 1),
            nn.Sigmoid()
        )
        
    def _freeze_pretrained_models(self):
        """å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°"""
        for param in self.baseline_model.parameters():
            param.requires_grad = False
            
        for param in self.anomaly_detector.parameters():
            param.requires_grad = False
            
        print("âœ… é¢„è®­ç»ƒæ¨¡å‹å·²å†»ç»“")
        
    def unfreeze_pretrained_models(self):
        """è§£å†»é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç”¨äºç«¯åˆ°ç«¯å¾®è°ƒï¼‰"""
        for param in self.baseline_model.parameters():
            param.requires_grad = True
            
        for param in self.anomaly_detector.parameters():
            param.requires_grad = True
            
        print("ğŸ”“ é¢„è®­ç»ƒæ¨¡å‹å·²è§£å†»ï¼Œå¼€å§‹ç«¯åˆ°ç«¯è®­ç»ƒ")
    
    def forward(self, 
                q: torch.Tensor, 
                s: torch.Tensor, 
                pid: Optional[torch.Tensor] = None,
                return_anomaly_info: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            q: é—®é¢˜åºåˆ— [batch_size, seq_len]
            s: ç­”æ¡ˆåºåˆ— [batch_size, seq_len]
            pid: é—®é¢˜IDåºåˆ— [batch_size, seq_len]
            return_anomaly_info: æ˜¯å¦è¿”å›å¼‚å¸¸ä¿¡æ¯
            
        Returns:
            predictions: çŸ¥è¯†è¿½è¸ªé¢„æµ‹ [batch_size, seq_len]
            anomaly_info: å¼‚å¸¸ç›¸å…³ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        batch_size, seq_len = q.shape
        
        # 1. åŸºçº¿çŸ¥è¯†è¿½è¸ªæ¨¡å‹
        with torch.no_grad() if hasattr(self.baseline_model, 'training') and not self.baseline_model.training else torch.enable_grad():
            baseline_output = self.baseline_model.get_hidden_states(q, s, pid)  # [batch_size, seq_len, d_model]
        
        # 2. å¼‚å¸¸æ£€æµ‹
        with torch.no_grad() if hasattr(self.anomaly_detector, 'training') and not self.anomaly_detector.training else torch.enable_grad():
            anomaly_logits = self.anomaly_detector(q, s, pid)  # [batch_size, seq_len]
            anomaly_probs = torch.sigmoid(anomaly_logits)
        
        # 3. ä¸Šä¸‹æ–‡å¢å¼ºï¼ˆå¯é€‰ï¼‰
        if self.enable_context_enhancement:
            baseline_output = self.context_enhancer(baseline_output, anomaly_probs)
        
        # 4. å¼‚å¸¸æ„ŸçŸ¥èåˆ
        fused_output, adjustment_weights = self.anomaly_fusion(
            baseline_output, anomaly_probs
        )
        
        # 5. æœ€ç»ˆé¢„æµ‹
        predictions = self.prediction_head(fused_output).squeeze(-1)  # [batch_size, seq_len]
        
        if return_anomaly_info:
            # å¼‚å¸¸ä¸€è‡´æ€§é¢„æµ‹ï¼ˆç”¨äºå¤šä»»åŠ¡å­¦ä¹ ï¼‰
            anomaly_consistency = self.anomaly_consistency_head(fused_output).squeeze(-1)
            
            anomaly_info = {
                'anomaly_probs': anomaly_probs,
                'adjustment_weights': adjustment_weights,
                'anomaly_consistency': anomaly_consistency,
                'baseline_output': baseline_output,
                'fused_output': fused_output
            }
            return predictions, anomaly_info
        
        return predictions
    
    def get_loss(self, 
                 q: torch.Tensor, 
                 s: torch.Tensor, 
                 target: torch.Tensor,
                 pid: Optional[torch.Tensor] = None,
                 lambda_anomaly: float = 0.1) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æŸå¤±å‡½æ•°
        
        Args:
            q: é—®é¢˜åºåˆ—
            s: ç­”æ¡ˆåºåˆ—  
            target: ç›®æ ‡åºåˆ—ï¼ˆä¸‹ä¸€é¢˜çš„ç­”æ¡ˆï¼‰
            pid: é—®é¢˜IDåºåˆ—
            lambda_anomaly: å¼‚å¸¸ä¸€è‡´æ€§æŸå¤±æƒé‡
            
        Returns:
            losses: æŸå¤±å­—å…¸
        """
        # å‰å‘ä¼ æ’­
        predictions, anomaly_info = self.forward(q, s, pid, return_anomaly_info=True)
        
        # åˆ›å»ºæœ‰æ•ˆä½ç½®çš„æ©ç 
        mask = (target >= 0)
        
        # ä¸»è¦çŸ¥è¯†è¿½è¸ªæŸå¤±
        kt_loss = F.binary_cross_entropy(
            predictions[mask], 
            target[mask].float()
        )
        
        # å¼‚å¸¸ä¸€è‡´æ€§æŸå¤±ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰
        anomaly_consistency_loss = F.binary_cross_entropy(
            anomaly_info['anomaly_consistency'][mask],
            anomaly_info['anomaly_probs'][mask]
        )
        
        # æ€»æŸå¤±
        total_loss = kt_loss + lambda_anomaly * anomaly_consistency_loss
        
        return {
            'total_loss': total_loss,
            'kt_loss': kt_loss,
            'anomaly_consistency_loss': anomaly_consistency_loss,
            'lambda_anomaly': lambda_anomaly
        }
    
    def get_trainable_parameters(self) -> Dict[str, torch.nn.Parameter]:
        """è·å–å¯è®­ç»ƒå‚æ•°"""
        trainable_params = {}
        
        # èåˆå±‚å‚æ•°
        for name, param in self.anomaly_fusion.named_parameters():
            if param.requires_grad:
                trainable_params[f'fusion.{name}'] = param
        
        # ä¸Šä¸‹æ–‡å¢å¼ºå™¨å‚æ•°
        if self.enable_context_enhancement:
            for name, param in self.context_enhancer.named_parameters():
                if param.requires_grad:
                    trainable_params[f'context.{name}'] = param
        
        # é¢„æµ‹å¤´å‚æ•°
        for name, param in self.prediction_head.named_parameters():
            if param.requires_grad:
                trainable_params[f'prediction.{name}'] = param
        
        # å¼‚å¸¸ä¸€è‡´æ€§å¤´å‚æ•°
        for name, param in self.anomaly_consistency_head.named_parameters():
            if param.requires_grad:
                trainable_params[f'anomaly_consistency.{name}'] = param
        
        return trainable_params
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'frozen_parameters': frozen_params,
            'trainable_ratio': trainable_params / total_params,
            'fusion_type': self.anomaly_fusion.fusion_type,
            'context_enhancement': self.enable_context_enhancement,
            'd_model': self.d_model
        }
