"""
å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªè®­ç»ƒå™¨

å®žçŽ°æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼š
1. å†»ç»“é¢„è®­ç»ƒæ¨¡åž‹ï¼Œåªè®­ç»ƒèžåˆå±‚
2. è§£å†»å¼‚å¸¸æ£€æµ‹å™¨ï¼Œè”åˆè®­ç»ƒ
3. ç«¯åˆ°ç«¯å¾®è°ƒï¼ˆå¯é€‰ï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Tuple, Optional
import os
from tqdm import tqdm
import numpy as np
from sklearn.metrics import roc_auc_score

from .model import AnomalyAwareKT


class AnomalyAwareTrainer:
    """å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªè®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: AnomalyAwareKT,
                 device: str = 'cuda',
                 learning_rate: float = 0.001,
                 save_dir: str = 'output/stage3',
                 patience: int = 10):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªæ¨¡åž‹
            device: è®­ç»ƒè®¾å¤‡
            learning_rate: å­¦ä¹ çŽ‡
            save_dir: ä¿å­˜ç›®å½•
            patience: æ—©åœè€å¿ƒå€¼
        """
        self.model = model
        self.device = device
        self.save_dir = save_dir
        self.patience = patience
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼‰
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        self.optimizer = optim.Adam(trainable_params, lr=learning_rate)
        
        # è®­ç»ƒçŠ¶æ€
        self.best_auc = 0.0
        self.best_epoch = 0
        self.patience_counter = 0
        self.training_history = []
        
    def progressive_train(self,
                         train_loader,
                         val_loader,
                         stage1_epochs: int = 10,
                         stage2_epochs: int = 20,
                         stage3_epochs: int = 10,
                         lambda_anomaly: float = 0.1) -> Dict:
        """
        æ¸è¿›å¼è®­ç»ƒ
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            stage1_epochs: é˜¶æ®µ1è®­ç»ƒè½®æ•°ï¼ˆåªè®­ç»ƒèžåˆå±‚ï¼‰
            stage2_epochs: é˜¶æ®µ2è®­ç»ƒè½®æ•°ï¼ˆè”åˆè®­ç»ƒï¼‰
            stage3_epochs: é˜¶æ®µ3è®­ç»ƒè½®æ•°ï¼ˆç«¯åˆ°ç«¯å¾®è°ƒï¼‰
            lambda_anomaly: å¼‚å¸¸ä¸€è‡´æ€§æŸå¤±æƒé‡
            
        Returns:
            è®­ç»ƒç»“æžœ
        """
        print("ðŸš€ å¼€å§‹æ¸è¿›å¼è®­ç»ƒ")
        
        total_epochs = stage1_epochs + stage2_epochs + stage3_epochs
        current_epoch = 0
        
        # é˜¶æ®µ1ï¼šåªè®­ç»ƒèžåˆå±‚
        print(f"\nðŸ“š é˜¶æ®µ1ï¼šèžåˆå±‚è®­ç»ƒ ({stage1_epochs} epochs)")
        print("ðŸ”’ åŸºçº¿æ¨¡åž‹å’Œå¼‚å¸¸æ£€æµ‹å™¨å·²å†»ç»“")
        
        for epoch in range(1, stage1_epochs + 1):
            current_epoch += 1
            stage_info = {'stage': 1, 'stage_epoch': epoch, 'total_epoch': current_epoch}
            
            train_metrics = self._train_epoch(train_loader, lambda_anomaly, stage_info)
            val_metrics = self._validate_epoch(val_loader, lambda_anomaly)
            
            self._log_epoch_results(current_epoch, train_metrics, val_metrics, stage_info)
            
            # æ—©åœæ£€æŸ¥
            if self._early_stopping_check(val_metrics['auc'], current_epoch):
                break
        
        # é˜¶æ®µ2ï¼šè§£å†»å¼‚å¸¸æ£€æµ‹å™¨ï¼Œè”åˆè®­ç»ƒ
        if stage2_epochs > 0:
            print(f"\nðŸ”“ é˜¶æ®µ2ï¼šè”åˆè®­ç»ƒ ({stage2_epochs} epochs)")
            print("ðŸ”“ å¼‚å¸¸æ£€æµ‹å™¨å·²è§£å†»")
            
            # è§£å†»å¼‚å¸¸æ£€æµ‹å™¨
            for param in self.model.anomaly_detector.parameters():
                param.requires_grad = True
            
            # æ›´æ–°ä¼˜åŒ–å™¨
            trainable_params = [p for p in self.model.parameters() if p.requires_grad]
            self.optimizer = optim.Adam(trainable_params, lr=self.optimizer.param_groups[0]['lr'] * 0.5)
            
            for epoch in range(1, stage2_epochs + 1):
                current_epoch += 1
                stage_info = {'stage': 2, 'stage_epoch': epoch, 'total_epoch': current_epoch}
                
                train_metrics = self._train_epoch(train_loader, lambda_anomaly, stage_info)
                val_metrics = self._validate_epoch(val_loader, lambda_anomaly)
                
                self._log_epoch_results(current_epoch, train_metrics, val_metrics, stage_info)
                
                if self._early_stopping_check(val_metrics['auc'], current_epoch):
                    break
        
        # é˜¶æ®µ3ï¼šç«¯åˆ°ç«¯å¾®è°ƒ
        if stage3_epochs > 0:
            print(f"\nðŸŽ¯ é˜¶æ®µ3ï¼šç«¯åˆ°ç«¯å¾®è°ƒ ({stage3_epochs} epochs)")
            print("ðŸ”“ æ‰€æœ‰æ¨¡åž‹å·²è§£å†»")
            
            # è§£å†»æ‰€æœ‰æ¨¡åž‹
            self.model.unfreeze_pretrained_models()
            
            # æ›´æ–°ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨æ›´å°çš„å­¦ä¹ çŽ‡ï¼‰
            trainable_params = [p for p in self.model.parameters() if p.requires_grad]
            self.optimizer = optim.Adam(trainable_params, lr=self.optimizer.param_groups[0]['lr'] * 0.1)
            
            for epoch in range(1, stage3_epochs + 1):
                current_epoch += 1
                stage_info = {'stage': 3, 'stage_epoch': epoch, 'total_epoch': current_epoch}
                
                train_metrics = self._train_epoch(train_loader, lambda_anomaly, stage_info)
                val_metrics = self._validate_epoch(val_loader, lambda_anomaly)
                
                self._log_epoch_results(current_epoch, train_metrics, val_metrics, stage_info)
                
                if self._early_stopping_check(val_metrics['auc'], current_epoch):
                    break
        
        # è®­ç»ƒå®Œæˆ
        final_result = {
            'best_auc': self.best_auc,
            'best_epoch': self.best_epoch,
            'total_epochs': current_epoch,
            'training_history': self.training_history,
            'model_info': self.model.get_model_info()
        }
        
        print(f"\nðŸŽ‰ æ¸è¿›å¼è®­ç»ƒå®Œæˆï¼")
        print(f"ðŸ“ˆ æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")
        
        return final_result
    
    def _train_epoch(self, train_loader, lambda_anomaly: float, stage_info: Dict) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_losses = {'total': 0.0, 'kt': 0.0, 'anomaly': 0.0}
        all_predictions = []
        all_targets = []
        
        desc = f"Stage {stage_info['stage']} Training"
        for batch in tqdm(train_loader, desc=desc):
            # èŽ·å–æ‰¹æ¬¡æ•°æ®
            q, s = batch.get("q", "s")
            
            # å®‰å…¨åœ°èŽ·å–pidå­—æ®µ
            try:
                pid = batch.get("pid")
            except (KeyError, AttributeError):
                pid = None
            
            q = q.to(self.device)
            s = s.to(self.device)
            if pid is not None:
                pid = pid.to(self.device)
            
            # æž„å»ºç›®æ ‡åºåˆ—ï¼ˆä¸‹ä¸€é¢˜çš„ç­”æ¡ˆï¼‰
            target = s[:, 1:].clone()  # å‘å‰ç§»åŠ¨ä¸€ä½
            q_input = q[:, :-1]        # åŽ»æŽ‰æœ€åŽä¸€ä½
            s_input = s[:, :-1]        # åŽ»æŽ‰æœ€åŽä¸€ä½
            if pid is not None:
                pid_input = pid[:, :-1]
            else:
                pid_input = None
            
            # å‰å‘ä¼ æ’­
            losses = self.model.get_loss(q_input, s_input, target, pid_input, lambda_anomaly)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            losses['total_loss'].backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            total_losses['total'] += losses['total_loss'].item()
            total_losses['kt'] += losses['kt_loss'].item()
            total_losses['anomaly'] += losses['anomaly_consistency_loss'].item()
            
            # æ”¶é›†é¢„æµ‹ç»“æžœç”¨äºŽAUCè®¡ç®—
            with torch.no_grad():
                predictions = self.model(q_input, s_input, pid_input)
                mask = (target >= 0)
                
                all_predictions.extend(predictions[mask].cpu().numpy())
                all_targets.extend(target[mask].cpu().numpy())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {k: v / len(train_loader) for k, v in total_losses.items()}
        
        # è®¡ç®—AUC
        if all_predictions:
            try:
                train_auc = roc_auc_score(all_targets, all_predictions)
            except:
                train_auc = 0.5
        else:
            train_auc = 0.5
        
        return {
            'auc': train_auc,
            'total_loss': avg_losses['total'],
            'kt_loss': avg_losses['kt'],
            'anomaly_loss': avg_losses['anomaly'],
            'sample_count': len(all_predictions)
        }
    
    def _validate_epoch(self, val_loader, lambda_anomaly: float) -> Dict:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_losses = {'total': 0.0, 'kt': 0.0, 'anomaly': 0.0}
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                # èŽ·å–æ‰¹æ¬¡æ•°æ®
                q, s = batch.get("q", "s")
                
                try:
                    pid = batch.get("pid")
                except (KeyError, AttributeError):
                    pid = None
                
                q = q.to(self.device)
                s = s.to(self.device)
                if pid is not None:
                    pid = pid.to(self.device)
                
                # æž„å»ºç›®æ ‡åºåˆ—
                target = s[:, 1:].clone()
                q_input = q[:, :-1]
                s_input = s[:, :-1]
                if pid is not None:
                    pid_input = pid[:, :-1]
                else:
                    pid_input = None
                
                # å‰å‘ä¼ æ’­
                losses = self.model.get_loss(q_input, s_input, target, pid_input, lambda_anomaly)
                predictions = self.model(q_input, s_input, pid_input)
                
                # è®°å½•æŸå¤±
                total_losses['total'] += losses['total_loss'].item()
                total_losses['kt'] += losses['kt_loss'].item()
                total_losses['anomaly'] += losses['anomaly_consistency_loss'].item()
                
                # æ”¶é›†é¢„æµ‹ç»“æžœ
                mask = (target >= 0)
                all_predictions.extend(predictions[mask].cpu().numpy())
                all_targets.extend(target[mask].cpu().numpy())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {k: v / len(val_loader) for k, v in total_losses.items()}
        
        # è®¡ç®—AUC
        if all_predictions:
            try:
                val_auc = roc_auc_score(all_targets, all_predictions)
            except:
                val_auc = 0.5
        else:
            val_auc = 0.5
        
        return {
            'auc': val_auc,
            'total_loss': avg_losses['total'],
            'kt_loss': avg_losses['kt'],
            'anomaly_loss': avg_losses['anomaly'],
            'sample_count': len(all_predictions)
        }
    
    def _log_epoch_results(self, epoch: int, train_metrics: Dict, val_metrics: Dict, stage_info: Dict):
        """è®°å½•epochç»“æžœ"""
        print(f"\nEpoch {epoch} (Stage {stage_info['stage']}-{stage_info['stage_epoch']})")
        print(f"è®­ç»ƒ - AUC: {train_metrics['auc']:.4f}, "
              f"Total Loss: {train_metrics['total_loss']:.4f}, "
              f"KT Loss: {train_metrics['kt_loss']:.4f}")
        print(f"éªŒè¯ - AUC: {val_metrics['auc']:.4f}, "
              f"Total Loss: {val_metrics['total_loss']:.4f}, "
              f"æ ·æœ¬: {val_metrics['sample_count']}")
        
        # è®°å½•è®­ç»ƒåŽ†å²
        epoch_result = {
            'epoch': epoch,
            'stage_info': stage_info,
            'train_metrics': train_metrics,
            'val_metrics': val_metrics
        }
        self.training_history.append(epoch_result)
    
    def _early_stopping_check(self, val_auc: float, epoch: int) -> bool:
        """æ—©åœæ£€æŸ¥"""
        if val_auc > self.best_auc:
            self.best_auc = val_auc
            self.best_epoch = epoch
            self.patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡åž‹
            self._save_model(epoch, val_auc)
            print(f"âœ… æ–°çš„æœ€ä½³æ¨¡åž‹ (AUC: {self.best_auc:.4f})")
            return False
        else:
            self.patience_counter += 1
            print(f"â³ æ— æ”¹è¿› ({self.patience_counter}/{self.patience})")
            
            if self.patience_counter >= self.patience:
                print(f"ðŸ›‘ æ—©åœè§¦å‘ï¼æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")
                return True
        
        return False
    
    def _save_model(self, epoch: int, auc: float):
        """ä¿å­˜æ¨¡åž‹"""
        model_path = os.path.join(self.save_dir, 'best_anomaly_aware_kt.pt')
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'auc': auc,
            'best_auc': self.best_auc,
            'model_info': self.model.get_model_info()
        }, model_path)
    
    def load_model(self, model_path: str) -> Dict:
        """åŠ è½½æ¨¡åž‹"""
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.best_auc = checkpoint.get('best_auc', 0.0)
        
        return checkpoint.get('model_info', {})
