#!/usr/bin/env python
"""
ç¬¬äºŒé˜¶æ®µæ‰§è¡Œè„šæœ¬ï¼šå¼‚å¸¸åˆ†ç±»å™¨è®­ç»ƒ

åŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒè„šæœ¬ã€‚
å®Œå…¨åŸºäºDTransformeråŸå§‹ä»£ç ï¼Œä¸ä¾èµ–ä»»ä½•anomaly_ktæ¨¡å—ã€‚
"""

import os
import sys
import argparse
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(project_root))  # æ·»åŠ ä¸Šçº§ç›®å½•ä»¥è®¿é—®DTransformer
sys.path.append(project_root)  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•

from DTransformer.data import KTData
from anomaly_kt_v2.core.common import prepare_data, setup_output_directory, save_config, print_stage_header
from anomaly_kt_v2.configs import load_auto_config, merge_config_with_args
from anomaly_kt_v2.stages.stage2_anomaly_classifier import train_anomaly_classifier, load_baseline_model_info


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='ç¬¬äºŒé˜¶æ®µï¼šå¼‚å¸¸åˆ†ç±»å™¨è®­ç»ƒ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True,
                       choices=['assist09', 'assist17', 'algebra05', 'statics'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--model_type', default='basic',
                       choices=['basic', 'extended'],
                       help='æ¨¡å‹ç±»å‹: basic(åŸºç¡€æ¨¡å‹) æˆ– extended(æ‰©å±•æ¨¡å‹)')
    parser.add_argument('--baseline_model_path', required=True,
                       help='ç¬¬ä¸€é˜¶æ®µåŸºçº¿æ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_dir', default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('-p', '--with_pid', action='store_true', default=True,
                       help='ä½¿ç”¨é—®é¢˜ID')
    
    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--auto_config', action='store_true',
                       help='è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=16, help='è®­ç»ƒæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--test_batch_size', type=int, default=32, help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')
    
    # å¼‚å¸¸æ£€æµ‹å™¨å‚æ•°
    parser.add_argument('--d_model', type=int, default=128, help='æ¨¡å‹éšè—ç»´åº¦')
    parser.add_argument('--n_heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--n_layers', type=int, default=2, help='å¼‚å¸¸æ£€æµ‹å™¨å±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutç‡')
    parser.add_argument('--window_size', type=int, default=10, help='ç»Ÿè®¡ç‰¹å¾çª—å£å¤§å°')
    
    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    parser.add_argument('--anomaly_epochs', type=int, default=50, help='å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--curriculum_type', default='linear',
                       choices=['linear', 'exponential', 'cosine', 'step'],
                       help='è¯¾ç¨‹è°ƒåº¦ç±»å‹')
    parser.add_argument('--initial_difficulty', type=float, default=0.1, help='åˆå§‹éš¾åº¦')
    parser.add_argument('--final_difficulty', type=float, default=0.8, help='æœ€ç»ˆéš¾åº¦')
    parser.add_argument('--warmup_epochs', type=int, default=5, help='é¢„çƒ­è½®æ•°')
    
    return parser


def apply_model_type_presets(args):
    """æ ¹æ®æ¨¡å‹ç±»å‹åº”ç”¨é¢„è®¾é…ç½®"""
    if args.model_type == 'basic':
        # åŸºç¡€æ¨¡å‹é¢„è®¾
        args.d_model = 128
        args.n_heads = 8
        args.experiment_suffix = "basic"
        print("ğŸ”§ åº”ç”¨åŸºç¡€æ¨¡å‹é¢„è®¾é…ç½®")
        print("  - å¼‚å¸¸æ£€æµ‹å™¨: d_model=128, n_heads=8")
        
    elif args.model_type == 'extended':
        # æ‰©å±•æ¨¡å‹é¢„è®¾
        args.d_model = 256
        args.n_heads = 16
        args.experiment_suffix = "extended"
        print("ğŸš€ åº”ç”¨æ‰©å±•æ¨¡å‹é¢„è®¾é…ç½®")
        print("  - å¼‚å¸¸æ£€æµ‹å™¨: d_model=256, n_heads=16")


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # æ‰“å°é˜¶æ®µæ ‡é¢˜
    print_stage_header("å¼‚å¸¸åˆ†ç±»å™¨è®­ç»ƒ", 2)
    
    # åº”ç”¨æ¨¡å‹ç±»å‹é¢„è®¾
    apply_model_type_presets(args)
    
    # éªŒè¯åŸºçº¿æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.baseline_model_path):
        print(f"âŒ åŸºçº¿æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.baseline_model_path}")
        return
    
    # åŠ è½½åŸºçº¿æ¨¡å‹ä¿¡æ¯
    print("\nğŸ“„ åŠ è½½åŸºçº¿æ¨¡å‹ä¿¡æ¯...")
    baseline_info = load_baseline_model_info(args.baseline_model_path)
    
    # ä»åŸºçº¿æ¨¡å‹ç»§æ‰¿é…ç½®
    args.d_model = baseline_info['d_model']
    args.n_heads = baseline_info['n_heads']
    args.with_pid = baseline_info['with_pid']
    
    print(f"âœ… å·²ä»åŸºçº¿æ¨¡å‹ç»§æ‰¿é…ç½®")
    print(f"  æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {args.n_heads}")
    print(f"  ä½¿ç”¨é—®é¢˜ID: {args.with_pid}")
    
    # é…ç½®æ–‡ä»¶å¤„ç†
    if args.auto_config:
        # è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶
        print(f"\nğŸ“„ è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶...")
        config_name = f"{args.dataset}_stage2"
        config = load_auto_config(args.dataset, 'stage2')
        if config:
            print(f"âœ… å·²è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_name}.yaml")
            merge_config_with_args(config, args)
            print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
            print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°è‡ªåŠ¨é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    
    elif args.config:
        # ç”¨æˆ·æŒ‡å®šäº†é…ç½®æ–‡ä»¶
        from anomaly_kt_v2.configs import load_config
        config = load_config(args.config)
        merge_config_with_args(config, args)
        print(f"ğŸ“„ å·²åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
        print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")
    
    # è®¾ç½®è¾“å‡ºç›®å½• (åŒ…å«æ¨¡å‹ç±»å‹)
    stage_name = f"stage2_{args.model_type}"
    args.output_dir = setup_output_directory(args.output_dir, args.dataset, stage_name)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # ä¿å­˜é…ç½®
    config_path = save_config(vars(args), args.output_dir)
    print(f"ğŸ“„ é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    try:
        # å‡†å¤‡æ•°æ®
        print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
        train_data, val_data, test_data, dataset_config = prepare_data(
            args.dataset, args.data_dir, args.batch_size, args.test_batch_size
        )
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"  æ•°æ®é›†: {args.dataset}")
        print(f"  é—®é¢˜æ•°é‡: {dataset_config['n_questions']}")
        print(f"  é—®é¢˜IDæ•°é‡: {dataset_config['n_pid']}")
        
        # å¼€å§‹è®­ç»ƒ
        model_path = train_anomaly_classifier(args, dataset_config, train_data, val_data)
        
        # è®­ç»ƒæˆåŠŸ
        print(f"\nğŸ‰ ç¬¬äºŒé˜¶æ®µè®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ’¾ å¼‚å¸¸æ£€æµ‹å™¨: {model_path}")
        
        # ç¬¬ä¸‰é˜¶æ®µå‘½ä»¤ç¤ºä¾‹
        print(f"\nğŸ“‹ ç¬¬ä¸‰é˜¶æ®µå‘½ä»¤ç¤ºä¾‹:")
        print(f"python scripts/run_stage3_anomaly_aware_kt.py \\")
        print(f"    --dataset {args.dataset} \\")
        print(f"    --model_type {args.model_type} \\")
        print(f"    --baseline_model_path {args.baseline_model_path} \\")
        print(f"    --anomaly_detector_path {model_path} \\")
        print(f"    --device {args.device}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
