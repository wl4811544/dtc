#!/usr/bin/env python
"""
ç¬¬ä¸‰é˜¶æ®µæ‰§è¡Œè„šæœ¬ï¼šå¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ª

å°†åŸºçº¿æ¨¡å‹å’Œå¼‚å¸¸æ£€æµ‹å™¨èåˆï¼Œå®ç°å¼‚å¸¸æ„ŸçŸ¥çš„çŸ¥è¯†è¿½è¸ªï¼Œ
ç›®æ ‡æå‡AUC 0.05-0.1ã€‚
"""

import os
import sys
import argparse
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(project_root))  # æ·»åŠ ä¸Šçº§ç›®å½•ä»¥è®¿é—®DTransformer
sys.path.append(project_root)  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•

from DTransformer.data import KTData
from anomaly_kt_v2.core.common import prepare_data, setup_output_directory, save_config, print_stage_header
from anomaly_kt_v2.configs import load_auto_config, merge_config_with_args
from anomaly_kt_v2.stages.stage3_anomaly_aware_kt import train_anomaly_aware_kt


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='ç¬¬ä¸‰é˜¶æ®µï¼šå¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ª')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True,
                       choices=['assist09', 'assist17', 'algebra05', 'statics'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--model_type', default='basic',
                       choices=['basic', 'extended'],
                       help='æ¨¡å‹ç±»å‹: basic(åŸºç¡€æ¨¡å‹) æˆ– extended(æ‰©å±•æ¨¡å‹)')
    parser.add_argument('--baseline_model_path', required=True,
                       help='ç¬¬ä¸€é˜¶æ®µåŸºçº¿æ¨¡å‹è·¯å¾„')
    parser.add_argument('--anomaly_detector_path', required=True,
                       help='ç¬¬äºŒé˜¶æ®µå¼‚å¸¸æ£€æµ‹å™¨è·¯å¾„')
    parser.add_argument('--data_dir', default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('-p', '--with_pid', action='store_true', default=True,
                       help='ä½¿ç”¨é—®é¢˜ID')
    
    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--auto_config', action='store_true',
                       help='è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=16, help='è®­ç»ƒæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--test_batch_size', type=int, default=32, help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')
    
    # èåˆå‚æ•°
    parser.add_argument('--fusion_type', default='attention',
                       choices=['attention', 'gating', 'weighted'],
                       help='èåˆç±»å‹')
    parser.add_argument('--enable_context_enhancement', action='store_true', default=True,
                       help='å¯ç”¨ä¸Šä¸‹æ–‡å¢å¼º')
    parser.add_argument('--freeze_pretrained', action='store_true', default=True,
                       help='å†»ç»“é¢„è®­ç»ƒæ¨¡å‹')
    parser.add_argument('--lambda_anomaly', type=float, default=0.1,
                       help='å¼‚å¸¸ä¸€è‡´æ€§æŸå¤±æƒé‡')
    
    # æ¸è¿›å¼è®­ç»ƒå‚æ•°
    parser.add_argument('--fusion_epochs', type=int, default=10,
                       help='èåˆå±‚è®­ç»ƒè½®æ•°')
    parser.add_argument('--joint_epochs', type=int, default=20,
                       help='è”åˆè®­ç»ƒè½®æ•°')
    parser.add_argument('--finetune_epochs', type=int, default=10,
                       help='ç«¯åˆ°ç«¯å¾®è°ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=10,
                       help='æ—©åœè€å¿ƒå€¼')
    
    return parser


def apply_model_type_presets(args):
    """æ ¹æ®æ¨¡å‹ç±»å‹åº”ç”¨é¢„è®¾é…ç½®"""
    if args.model_type == 'basic':
        # åŸºç¡€æ¨¡å‹é¢„è®¾
        args.experiment_suffix = "basic"
        print("ğŸ”§ åº”ç”¨åŸºç¡€æ¨¡å‹é¢„è®¾é…ç½®")
        print("  - å¼‚å¸¸æ„ŸçŸ¥èåˆ: æ ‡å‡†é…ç½®")
        
    elif args.model_type == 'extended':
        # æ‰©å±•æ¨¡å‹é¢„è®¾
        args.experiment_suffix = "extended"
        print("ğŸš€ åº”ç”¨æ‰©å±•æ¨¡å‹é¢„è®¾é…ç½®")
        print("  - å¼‚å¸¸æ„ŸçŸ¥èåˆ: å¢å¼ºé…ç½®")


def validate_model_paths(args):
    """éªŒè¯æ¨¡å‹è·¯å¾„"""
    print("\nğŸ” éªŒè¯æ¨¡å‹è·¯å¾„...")
    
    # éªŒè¯åŸºçº¿æ¨¡å‹
    if not os.path.exists(args.baseline_model_path):
        print(f"âŒ åŸºçº¿æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.baseline_model_path}")
        return False
    
    # éªŒè¯å¼‚å¸¸æ£€æµ‹å™¨
    if not os.path.exists(args.anomaly_detector_path):
        print(f"âŒ å¼‚å¸¸æ£€æµ‹å™¨æ–‡ä»¶ä¸å­˜åœ¨: {args.anomaly_detector_path}")
        return False
    
    print("âœ… æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡")
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # æ‰“å°é˜¶æ®µæ ‡é¢˜
    print_stage_header("å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ª", 3)
    
    # åº”ç”¨æ¨¡å‹ç±»å‹é¢„è®¾
    apply_model_type_presets(args)
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    if not validate_model_paths(args):
        return
    
    # é…ç½®æ–‡ä»¶å¤„ç†
    if args.auto_config:
        # è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶
        print(f"\nğŸ“„ è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶...")
        config_name = f"{args.dataset}_stage3"
        config = load_auto_config(args.dataset, 'stage3')
        if config:
            print(f"âœ… å·²è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶: {config_name}.yaml")
            merge_config_with_args(config, args)
            print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
            print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°è‡ªåŠ¨é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    
    elif args.config:
        # ç”¨æˆ·æŒ‡å®šäº†é…ç½®æ–‡ä»¶
        from anomaly_kt_v2.configs import load_config
        config = load_config(args.config)
        merge_config_with_args(config, args)
        print(f"ğŸ“„ å·²åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
        print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")
    
    # è®¾ç½®è¾“å‡ºç›®å½• (åŒ…å«æ¨¡å‹ç±»å‹)
    stage_name = f"stage3_{args.model_type}"
    args.output_dir = setup_output_directory(args.output_dir, args.dataset, stage_name)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # ä¿å­˜é…ç½®
    config_path = save_config(vars(args), args.output_dir)
    print(f"ğŸ“„ é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    try:
        # å‡†å¤‡æ•°æ®
        print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
        train_data, val_data, test_data, dataset_config = prepare_data(
            args.dataset, args.data_dir, args.batch_size, args.test_batch_size
        )
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"  æ•°æ®é›†: {args.dataset}")
        print(f"  é—®é¢˜æ•°é‡: {dataset_config['n_questions']}")
        print(f"  é—®é¢˜IDæ•°é‡: {dataset_config['n_pid']}")
        
        # å¼€å§‹è®­ç»ƒ
        model_path = train_anomaly_aware_kt(args, dataset_config, train_data, val_data)
        
        # è®­ç»ƒæˆåŠŸ
        print(f"\nğŸ‰ ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ’¾ å¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹: {model_path}")
        
        # æ€§èƒ½è¯„ä¼°å»ºè®®
        print(f"\nğŸ“‹ åç»­è¯„ä¼°å»ºè®®:")
        print(f"1. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½")
        print(f"2. ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œè¯¦ç»†å¯¹æ¯”")
        print(f"3. åˆ†æå¼‚å¸¸æ£€æµ‹çš„è´¡çŒ®")
        print(f"4. è¿›è¡Œæ¶ˆèç ”ç©¶éªŒè¯å„ç»„ä»¶æ•ˆæœ")
        
        # è¯„ä¼°å‘½ä»¤ç¤ºä¾‹
        print(f"\nğŸ“‹ è¯„ä¼°å‘½ä»¤ç¤ºä¾‹:")
        print(f"python scripts/evaluate_final_performance.py \\")
        print(f"    --dataset {args.dataset} \\")
        print(f"    --baseline_model {args.baseline_model_path} \\")
        print(f"    --anomaly_aware_model {model_path} \\")
        print(f"    --device {args.device}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
