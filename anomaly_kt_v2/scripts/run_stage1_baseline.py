#!/usr/bin/env python
"""
Stage 1: åŸºçº¿æ¨¡å‹è®­ç»ƒè„šæœ¬

è®­ç»ƒæ ‡å‡†çš„DTransformerçŸ¥è¯†è¿½è¸ªæ¨¡å‹ä½œä¸ºåç»­å¼‚å¸¸æ„ŸçŸ¥è®­ç»ƒçš„åŸºçº¿
"""

import os
import sys
import argparse
import torch
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(project_root))  # æ·»åŠ ä¸Šçº§ç›®å½•ä»¥è®¿é—®DTransformer
sys.path.append(project_root)  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•

from anomaly_kt_v2.core.common import prepare_data, setup_output_directory, save_config, print_stage_header
from anomaly_kt_v2.configs import load_auto_config, merge_config_with_args
from anomaly_kt_v2.stages.stage1_baseline import train_baseline_model


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Stage 1: åŸºçº¿æ¨¡å‹è®­ç»ƒ')

    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument('--config', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAMLæ ¼å¼)')
    parser.add_argument('--auto_config', action='store_true',
                       help='æ ¹æ®æ•°æ®é›†è‡ªåŠ¨é€‰æ‹©é…ç½®æ–‡ä»¶')

    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True,
                       choices=['assist09', 'assist17', 'algebra05', 'statics'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--model_type', default='basic',
                       choices=['basic', 'extended'],
                       help='æ¨¡å‹ç±»å‹: basic(åŸºç¡€æ¨¡å‹) æˆ– extended(æ‰©å±•æ¨¡å‹)')
    parser.add_argument('--data_dir', default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', default=None, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('-p', '--with_pid', action='store_true', default=True,
                       help='ä½¿ç”¨é—®é¢˜ID')

    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=16, help='è®­ç»ƒæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--test_batch_size', type=int, default=32, help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')

    # åŸºçº¿æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--n_heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--n_know', type=int, default=16, help='çŸ¥è¯†æ¦‚å¿µæ•°')
    parser.add_argument('--n_layers', type=int, default=3, help='ç½‘ç»œå±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.2, help='Dropoutç‡')
    parser.add_argument('--lambda_cl', type=float, default=0.1, help='å¯¹æ¯”å­¦ä¹ æƒé‡')
    parser.add_argument('--proj', action='store_true', default=True, help='ä½¿ç”¨æŠ•å½±å±‚')
    parser.add_argument('--hard_neg', action='store_true', default=False, help='ä½¿ç”¨å›°éš¾è´Ÿæ ·æœ¬')
    parser.add_argument('--window', type=int, default=1, help='æ³¨æ„åŠ›çª—å£å¤§å°')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--kt_epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, default=10, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--use_cl', action='store_true', default=True, help='ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ')

    return parser


def apply_model_type_presets(args):
    """æ ¹æ®æ¨¡å‹ç±»å‹åº”ç”¨é¢„è®¾é…ç½®"""
    if args.model_type == 'basic':
        # åŸºç¡€æ¨¡å‹é¢„è®¾ (ä¸æ‚¨çš„åŸºç¡€æ¨¡å‹é…ç½®ä¸€è‡´)
        args.d_model = 128
        args.n_heads = 8
        args.experiment_suffix = "basic"
        print("ğŸ”§ åº”ç”¨åŸºç¡€æ¨¡å‹é¢„è®¾é…ç½®")
        print("  - d_model: 128, n_heads: 8")
        print("  - ä¸æ‚¨çš„åŸºç¡€æ¨¡å‹è®­ç»ƒé…ç½®ä¸€è‡´")
        print("  - é¢„æœŸAUC: 0.7407")

    elif args.model_type == 'extended':
        # æ‰©å±•æ¨¡å‹é¢„è®¾ (ä¸æ‚¨çš„æ‰©å±•æ¨¡å‹é…ç½®ä¸€è‡´)
        args.d_model = 256
        args.n_heads = 16
        args.experiment_suffix = "extended"
        print("ğŸš€ åº”ç”¨æ‰©å±•æ¨¡å‹é¢„è®¾é…ç½®")
        print("  - d_model: 256, n_heads: 16")
        print("  - ä¸æ‚¨çš„æ‰©å±•æ¨¡å‹è®­ç»ƒé…ç½®ä¸€è‡´")
        print("  - é¢„æœŸAUC: 0.7404")


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # æ‰“å°é˜¶æ®µæ ‡é¢˜
    print_stage_header("åŸºçº¿æ¨¡å‹è®­ç»ƒ", 1)

    # åº”ç”¨æ¨¡å‹ç±»å‹é¢„è®¾
    apply_model_type_presets(args)

    # å¤„ç†é…ç½®æ–‡ä»¶
    config = {}
    if args.config:
        # ç”¨æˆ·æŒ‡å®šäº†é…ç½®æ–‡ä»¶
        from anomaly_kt_v2.configs import load_config
        config = load_config(args.config)
        print(f"ğŸ“„ å·²åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    elif args.auto_config:
        # è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶
        config = load_auto_config(args.dataset, 'baseline')
        if config:
            print(f"ğŸ“„ å·²è‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶: {args.dataset}_baseline.yaml")
        else:
            print("ğŸ”§ ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆæœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼‰")
    else:
        print("ğŸ”§ ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°å’Œé»˜è®¤å€¼")

    # åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
    if config:
        print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
        merge_config_with_args(config, args)
        print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")

    # è®¾ç½®è¾“å‡ºç›®å½• (åŒ…å«æ¨¡å‹ç±»å‹)
    stage_name = f"stage1_{args.model_type}"
    args.output_dir = setup_output_directory(args.output_dir, args.dataset, stage_name)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # ä¿å­˜é…ç½®
    config_save_path = save_config(vars(args), args.output_dir)
    print(f"ğŸ“„ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

    # å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data, dataset_config = prepare_data(
        args.dataset, args.data_dir, args.batch_size, args.test_batch_size
    )

    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  é—®é¢˜æ•°é‡: {dataset_config['n_questions']}")
    print(f"  é—®é¢˜IDæ•°é‡: {dataset_config.get('n_pid', 0)}")

    try:
        # è¿è¡Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
        model_path = train_baseline_model(args, dataset_config, train_data, val_data)

        print("\n" + "="*60)
        print("ğŸ‰ ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå®Œæˆ!")
        print("="*60)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

        # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æ€§èƒ½")
        print("2. è¿è¡Œç¬¬äºŒé˜¶æ®µè®­ç»ƒå¼‚å¸¸åˆ†ç±»å™¨")
        print("3. æˆ–è€…ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ¨ç†")

        print(f"\nğŸ“‹ ç¬¬äºŒé˜¶æ®µå‘½ä»¤ç¤ºä¾‹:")
        print(f"python scripts/run_stage2_anomaly_classifier.py \\")
        print(f"    --dataset {args.dataset} \\")
        print(f"    --model_type {args.model_type} \\")
        print(f"    --baseline_model_path {model_path} \\")
        print(f"    --device {args.device}")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
