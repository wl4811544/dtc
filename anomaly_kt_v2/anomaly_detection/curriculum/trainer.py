"""
è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨

å®ç°åŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒã€‚
å®Œå…¨åŸºäºDTransformeråŸå§‹ä»£ç ï¼Œä¸ä¾èµ–ä»»ä½•anomaly_ktæ¨¡å—ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Tuple, Optional
import os
from tqdm import tqdm

from .scheduler import CurriculumScheduler
from .difficulty_estimator import DifficultyEstimator
from ..generators.curriculum_generator import CurriculumAnomalyGenerator
from ..generators.baseline_generator import BaselineAnomalyGenerator


class CurriculumTrainer:
    """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self,
                 model: nn.Module,
                 device: str = 'cuda',
                 learning_rate: float = 0.001,
                 save_dir: str = 'output/stage2',
                 patience: int = 10,
                 with_pid: bool = True):
        """
        åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨

        Args:
            model: å¼‚å¸¸æ£€æµ‹æ¨¡å‹
            device: è®­ç»ƒè®¾å¤‡
            learning_rate: å­¦ä¹ ç‡
            save_dir: ä¿å­˜ç›®å½•
            patience: æ—©åœè€å¿ƒå€¼
            with_pid: æ˜¯å¦ä½¿ç”¨é—®é¢˜ID
        """
        self.model = model
        self.device = device
        self.save_dir = save_dir
        self.patience = patience
        self.with_pid = with_pid
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        # ç»„ä»¶
        self.curriculum_generator = CurriculumAnomalyGenerator()
        self.baseline_generator = BaselineAnomalyGenerator()
        self.difficulty_estimator = DifficultyEstimator()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_auc = 0.0
        self.best_epoch = 0
        self.patience_counter = 0
        
    def train(self,
              train_loader,
              val_loader,
              epochs: int = 50,
              curriculum_config: Optional[Dict] = None) -> Dict:
        """
        æ‰§è¡Œè¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            curriculum_config: è¯¾ç¨‹å­¦ä¹ é…ç½®
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        # é»˜è®¤è¯¾ç¨‹é…ç½®
        if curriculum_config is None:
            curriculum_config = {
                'initial_difficulty': 0.1,
                'final_difficulty': 0.8,
                'schedule_type': 'linear',
                'warmup_epochs': 5
            }
        
        # åˆ›å»ºè¯¾ç¨‹è°ƒåº¦å™¨
        scheduler = CurriculumScheduler(
            total_epochs=epochs,
            **curriculum_config
        )
        
        print(f"å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒï¼Œå…±{epochs}è½®")
        print(f"è¯¾ç¨‹é…ç½®: {curriculum_config}")
        
        training_history = []
        
        for epoch in range(1, epochs + 1):
            print(f"\nEpoch {epoch}/{epochs}")
            
            # æ›´æ–°è¯¾ç¨‹çŠ¶æ€
            curriculum_state = scheduler.step(epoch)
            print(f"è¯¾ç¨‹çŠ¶æ€: éš¾åº¦={curriculum_state['difficulty']:.3f}, "
                  f"é˜¶æ®µ={curriculum_state['phase']}")
            
            # è®­ç»ƒé˜¶æ®µ
            train_metrics = self._train_epoch(train_loader, curriculum_state)
            
            # éªŒè¯é˜¶æ®µ
            val_metrics = self._validate_epoch(val_loader)
            
            # è®°å½•è®­ç»ƒå†å²
            epoch_result = {
                'epoch': epoch,
                'curriculum_state': curriculum_state,
                'train_metrics': train_metrics,
                'val_metrics': val_metrics
            }
            training_history.append(epoch_result)
            
            # æ‰“å°è¯¦ç»†ç»“æœ
            print(f"è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, "
                  f"AUC: {train_metrics['auc']:.4f}, "
                  f"P: {train_metrics.get('precision', 0):.3f}, "
                  f"R: {train_metrics.get('recall', 0):.3f}, "
                  f"F1: {train_metrics.get('f1', 0):.3f}")
            print(f"éªŒè¯ - Loss: {val_metrics['loss']:.4f}, "
                  f"AUC: {val_metrics['auc']:.4f}, "
                  f"P: {val_metrics.get('precision', 0):.3f}, "
                  f"R: {val_metrics.get('recall', 0):.3f}, "
                  f"F1: {val_metrics.get('f1', 0):.3f}, "
                  f"æ ·æœ¬: {val_metrics['sample_count']}")
            
            # æ—©åœæ£€æŸ¥
            if val_metrics['auc'] > self.best_auc:
                self.best_auc = val_metrics['auc']
                self.best_epoch = epoch
                self.patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self._save_model(epoch, val_metrics)
                print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹ (AUC: {self.best_auc:.4f})")
            else:
                self.patience_counter += 1
                print(f"â³ æ— æ”¹è¿› ({self.patience_counter}/{self.patience})")
            
            if self.patience_counter >= self.patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼æœ€ä½³AUC: {self.best_auc:.4f} (Epoch {self.best_epoch})")
                break
        
        # è®­ç»ƒå®Œæˆ
        final_result = {
            'best_auc': self.best_auc,
            'best_epoch': self.best_epoch,
            'total_epochs': epoch,
            'training_history': training_history
        }
        
        return final_result
    
    def _train_epoch(self, train_loader, curriculum_state: Dict) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        all_difficulties = []
        
        for batch in tqdm(train_loader, desc="Training"):
            # è·å–æ‰¹æ¬¡æ•°æ®
            if self.with_pid:
                q, s, pid = batch.get("q", "s", "pid")
            else:
                q, s = batch.get("q", "s")
                pid = None
            
            q = q.to(self.device)
            s = s.to(self.device)
            if pid is not None:
                pid = pid.to(self.device)
            
            # ç”Ÿæˆå¼‚å¸¸æ•°æ®
            s_anomaly, anomaly_labels, difficulty_scores = self._generate_curriculum_data(
                q, s, curriculum_state
            )
            
            # å‰å‘ä¼ æ’­
            logits = self.model(q, s_anomaly, pid)
            loss = self.model.get_loss(q, s_anomaly, anomaly_labels, pid)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            # è®°å½•ç»“æœ
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºè¯„ä¼°
            with torch.no_grad():
                pred_probs = torch.sigmoid(logits)
                mask = (s_anomaly >= 0)
                
                all_predictions.extend(pred_probs[mask].cpu().numpy())
                all_labels.extend(anomaly_labels[mask].cpu().numpy())
                all_difficulties.extend(difficulty_scores[mask].cpu().numpy())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        avg_loss = total_loss / len(train_loader)

        # è¯„ä¼°æ£€æµ‹æ€§èƒ½
        if all_predictions:
            predictions_tensor = torch.tensor(all_predictions)
            labels_tensor = torch.tensor(all_labels)
            difficulties_tensor = torch.tensor(all_difficulties)

            difficulty_result = self.difficulty_estimator.estimate_detection_difficulty(
                predictions_tensor, labels_tensor, difficulties_tensor
            )

            basic_metrics = difficulty_result['basic_metrics']
            train_auc = basic_metrics['auc']
            precision = basic_metrics['precision']
            recall = basic_metrics['recall']
            f1 = basic_metrics['f1']
        else:
            train_auc = 0.0
            precision = 0.0
            recall = 0.0
            f1 = 0.0

        return {
            'loss': avg_loss,
            'auc': train_auc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'sample_count': len(all_predictions)
        }
    
    def _validate_epoch(self, val_loader) -> Dict:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                # è·å–æ‰¹æ¬¡æ•°æ®
                if self.with_pid:
                    q, s, pid = batch.get("q", "s", "pid")
                else:
                    q, s = batch.get("q", "s")
                    pid = None
                
                q = q.to(self.device)
                s = s.to(self.device)
                if pid is not None:
                    pid = pid.to(self.device)
                
                # ç”ŸæˆéªŒè¯å¼‚å¸¸æ•°æ®ï¼ˆä½¿ç”¨å›ºå®šç­–ç•¥ï¼‰
                s_anomaly, anomaly_labels = self.baseline_generator.generate_baseline_anomalies(
                    q, s, strategy='random_flip', anomaly_ratio=0.1
                )
                
                # å‰å‘ä¼ æ’­
                logits = self.model(q, s_anomaly, pid)
                loss = self.model.get_loss(q, s_anomaly, anomaly_labels, pid)
                
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                pred_probs = torch.sigmoid(logits)
                mask = (s_anomaly >= 0)
                
                all_predictions.extend(pred_probs[mask].cpu().numpy())
                all_labels.extend(anomaly_labels[mask].cpu().numpy())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        avg_loss = total_loss / len(val_loader)

        if all_predictions:
            predictions_tensor = torch.tensor(all_predictions)
            labels_tensor = torch.tensor(all_labels)

            difficulty_result = self.difficulty_estimator.estimate_detection_difficulty(
                predictions_tensor, labels_tensor
            )

            basic_metrics = difficulty_result['basic_metrics']
            val_auc = basic_metrics['auc']
            precision = basic_metrics['precision']
            recall = basic_metrics['recall']
            f1 = basic_metrics['f1']
        else:
            val_auc = 0.0
            precision = 0.0
            recall = 0.0
            f1 = 0.0

        return {
            'loss': avg_loss,
            'auc': val_auc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'sample_count': len(all_predictions)
        }
    
    def _generate_curriculum_data(self, q: torch.Tensor, s: torch.Tensor,
                                 curriculum_state: Dict) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """æ ¹æ®è¯¾ç¨‹çŠ¶æ€ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        difficulty_levels = [1, 2, 3, 4]
        level_weights = curriculum_state['level_weights']
        anomaly_ratio = curriculum_state['anomaly_ratio']
        
        # ç”Ÿæˆè¯¾ç¨‹å¼‚å¸¸
        s_anomaly, anomaly_labels, difficulty_scores = self.curriculum_generator.generate_curriculum_anomalies(
            q, s,
            difficulty_levels=difficulty_levels,
            level_weights=level_weights,
            anomaly_ratio=anomaly_ratio,
            include_baseline=True,
            baseline_ratio=0.3
        )
        
        return s_anomaly, anomaly_labels, difficulty_scores
    
    def _save_model(self, epoch: int, metrics: Dict):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.save_dir, 'best_anomaly_detector.pt')
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': metrics,
            'best_auc': self.best_auc
        }, model_path)
    
    def load_model(self, model_path: str) -> Dict:
        """åŠ è½½æ¨¡å‹"""
        # PyTorch 2.6+ å…¼å®¹æ€§ï¼šç¦ç”¨ weights_only ä»¥æ”¯æŒæ—§æ¨¡å‹æ–‡ä»¶
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.best_auc = checkpoint.get('best_auc', 0.0)
        
        return checkpoint.get('metrics', {})
