"""
å¼‚å¸¸æ£€æµ‹è¯„ä¼°å™¨

æä¾›å…¨é¢çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½è¯„ä¼°åŠŸèƒ½ã€‚
å®Œå…¨åŸºäºDTransformeråŸå§‹ä»£ç ï¼Œä¸ä¾èµ–ä»»ä½•anomaly_ktæ¨¡å—ã€‚
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, auc,
    confusion_matrix, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns


class AnomalyDetectionEvaluator:
    """å¼‚å¸¸æ£€æµ‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.evaluation_history = []
    
    def evaluate_model(self,
                      model: torch.nn.Module,
                      data_loader,
                      device: str = 'cuda',
                      anomaly_strategies: List[str] = None,
                      with_pid: bool = True) -> Dict:
        """
        å…¨é¢è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹
        
        Args:
            model: å¼‚å¸¸æ£€æµ‹æ¨¡å‹
            data_loader: æ•°æ®åŠ è½½å™¨
            device: è®¾å¤‡
            anomaly_strategies: è¦æµ‹è¯•çš„å¼‚å¸¸ç­–ç•¥åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if anomaly_strategies is None:
            anomaly_strategies = ['random_flip', 'uniform_random', 'systematic_bias']
        
        model.eval()
        results = {}
        
        # å¯¹æ¯ç§å¼‚å¸¸ç­–ç•¥è¿›è¡Œè¯„ä¼°
        for strategy in anomaly_strategies:
            print(f"è¯„ä¼°å¼‚å¸¸ç­–ç•¥: {strategy}")
            strategy_result = self._evaluate_strategy(model, data_loader, device, strategy, with_pid)
            results[strategy] = strategy_result
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡
        overall_result = self._compute_overall_metrics(results)
        results['overall'] = overall_result
        
        # ä¿å­˜è¯„ä¼°å†å²
        self.evaluation_history.append(results)
        
        return results
    
    def _evaluate_strategy(self,
                          model: torch.nn.Module,
                          data_loader,
                          device: str,
                          strategy: str,
                          with_pid: bool = True) -> Dict:
        """è¯„ä¼°ç‰¹å®šå¼‚å¸¸ç­–ç•¥"""
        from .generators.baseline_generator import BaselineAnomalyGenerator
        
        generator = BaselineAnomalyGenerator()
        
        all_predictions = []
        all_labels = []
        all_losses = []
        
        with torch.no_grad():
            for batch in data_loader:
                # è·å–æ‰¹æ¬¡æ•°æ®
                if with_pid:
                    q, s, pid = batch.get("q", "s", "pid")
                else:
                    q, s = batch.get("q", "s")
                    pid = None
                
                q = q.to(device)
                s = s.to(device)
                if pid is not None:
                    pid = pid.to(device)
                
                # ç”Ÿæˆå¼‚å¸¸æ•°æ®
                s_anomaly, anomaly_labels = generator.generate_baseline_anomalies(
                    q, s, strategy=strategy, anomaly_ratio=0.2
                )
                
                # æ¨¡å‹é¢„æµ‹
                logits = model(q, s_anomaly, pid)
                loss = model.get_loss(q, s_anomaly, anomaly_labels, pid)
                
                # æ”¶é›†ç»“æœ
                pred_probs = torch.sigmoid(logits)
                mask = (s_anomaly >= 0)
                
                all_predictions.extend(pred_probs[mask].cpu().numpy())
                all_labels.extend(anomaly_labels[mask].cpu().numpy())
                all_losses.append(loss.item())
        
        # è®¡ç®—æŒ‡æ ‡
        if all_predictions:
            metrics = self._compute_metrics(
                np.array(all_predictions),
                np.array(all_labels)
            )
            metrics['avg_loss'] = np.mean(all_losses)
            metrics['sample_count'] = len(all_predictions)
            metrics['anomaly_ratio'] = np.mean(all_labels)
        else:
            metrics = self._empty_metrics()
        
        return metrics
    
    def _compute_metrics(self, predictions: np.ndarray, labels: np.ndarray) -> Dict:
        """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
        # åŸºç¡€æŒ‡æ ‡
        try:
            auc_score = roc_auc_score(labels, predictions)
        except:
            auc_score = 0.5
        
        try:
            precision, recall, _ = precision_recall_curve(labels, predictions)
            pr_auc = auc(recall, precision)
        except:
            pr_auc = 0.0
        
        # å¤šä¸ªé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
        thresholds = [0.3, 0.5, 0.7]
        threshold_metrics = {}
        
        for threshold in thresholds:
            pred_binary = (predictions > threshold).astype(int)
            
            # æ··æ·†çŸ©é˜µ
            tn, fp, fn, tp = confusion_matrix(labels, pred_binary).ravel()
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            threshold_metrics[f'threshold_{threshold}'] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'f1': f1,
                'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)
            }
        
        # é¢„æµ‹åˆ†å¸ƒåˆ†æ
        distribution_metrics = self._analyze_prediction_distribution(predictions, labels)
        
        return {
            'auc': auc_score,
            'pr_auc': pr_auc,
            'threshold_metrics': threshold_metrics,
            'distribution_metrics': distribution_metrics
        }
    
    def _analyze_prediction_distribution(self, predictions: np.ndarray, labels: np.ndarray) -> Dict:
        """åˆ†æé¢„æµ‹åˆ†å¸ƒ"""
        # åˆ†ç¦»æ­£å¸¸å’Œå¼‚å¸¸æ ·æœ¬çš„é¢„æµ‹
        normal_pred = predictions[labels == 0]
        anomaly_pred = predictions[labels == 1]
        
        # ç»Ÿè®¡æŒ‡æ ‡
        normal_stats = {
            'mean': normal_pred.mean() if len(normal_pred) > 0 else 0.0,
            'std': normal_pred.std() if len(normal_pred) > 0 else 0.0,
            'median': np.median(normal_pred) if len(normal_pred) > 0 else 0.0
        }
        
        anomaly_stats = {
            'mean': anomaly_pred.mean() if len(anomaly_pred) > 0 else 0.0,
            'std': anomaly_pred.std() if len(anomaly_pred) > 0 else 0.0,
            'median': np.median(anomaly_pred) if len(anomaly_pred) > 0 else 0.0
        }
        
        # åˆ†ç¦»åº¦æŒ‡æ ‡
        separation = abs(anomaly_stats['mean'] - normal_stats['mean'])
        
        # é‡å åº¦åˆ†æ
        overlap_ratio = self._compute_overlap_ratio(normal_pred, anomaly_pred)
        
        return {
            'normal_stats': normal_stats,
            'anomaly_stats': anomaly_stats,
            'separation': separation,
            'overlap_ratio': overlap_ratio
        }
    
    def _compute_overlap_ratio(self, normal_pred: np.ndarray, anomaly_pred: np.ndarray) -> float:
        """è®¡ç®—é¢„æµ‹åˆ†å¸ƒçš„é‡å æ¯”ä¾‹"""
        if len(normal_pred) == 0 or len(anomaly_pred) == 0:
            return 0.0
        
        # ä½¿ç”¨ç›´æ–¹å›¾ä¼°è®¡é‡å 
        bins = np.linspace(0, 1, 21)
        normal_hist, _ = np.histogram(normal_pred, bins=bins, density=True)
        anomaly_hist, _ = np.histogram(anomaly_pred, bins=bins, density=True)
        
        # è®¡ç®—é‡å é¢ç§¯
        overlap = np.minimum(normal_hist, anomaly_hist).sum()
        total = (normal_hist.sum() + anomaly_hist.sum()) / 2
        
        return overlap / total if total > 0 else 0.0
    
    def _compute_overall_metrics(self, strategy_results: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆæŒ‡æ ‡"""
        if not strategy_results:
            return self._empty_metrics()
        
        # æ”¶é›†æ‰€æœ‰ç­–ç•¥çš„AUC
        aucs = []
        pr_aucs = []
        sample_counts = []
        
        for strategy, result in strategy_results.items():
            if 'auc' in result:
                aucs.append(result['auc'])
                pr_aucs.append(result.get('pr_auc', 0.0))
                sample_counts.append(result.get('sample_count', 0))
        
        if not aucs:
            return self._empty_metrics()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_auc = np.mean(aucs)
        std_auc = np.std(aucs)
        avg_pr_auc = np.mean(pr_aucs)
        total_samples = sum(sample_counts)
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        performance_grade = self._assess_performance_grade(avg_auc)
        
        return {
            'avg_auc': avg_auc,
            'std_auc': std_auc,
            'avg_pr_auc': avg_pr_auc,
            'total_samples': total_samples,
            'num_strategies': len(aucs),
            'performance_grade': performance_grade,
            'strategy_aucs': dict(zip(strategy_results.keys(), aucs))
        }
    
    def _assess_performance_grade(self, auc: float) -> str:
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        if auc >= 0.9:
            return 'Excellent'
        elif auc >= 0.8:
            return 'Good'
        elif auc >= 0.7:
            return 'Fair'
        elif auc >= 0.6:
            return 'Poor'
        else:
            return 'Very Poor'
    
    def _empty_metrics(self) -> Dict:
        """è¿”å›ç©ºæŒ‡æ ‡"""
        return {
            'auc': 0.5,
            'pr_auc': 0.0,
            'avg_loss': float('inf'),
            'sample_count': 0,
            'anomaly_ratio': 0.0
        }
    
    def generate_report(self, results: Dict, save_path: Optional[str] = None) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("å¼‚å¸¸æ£€æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        report_lines.append("=" * 60)
        
        # ç»¼åˆæŒ‡æ ‡
        if 'overall' in results:
            overall = results['overall']
            report_lines.append(f"\nğŸ“Š ç»¼åˆæ€§èƒ½:")
            report_lines.append(f"  å¹³å‡AUC: {overall.get('avg_auc', 0):.4f} Â± {overall.get('std_auc', 0):.4f}")
            report_lines.append(f"  å¹³å‡PR-AUC: {overall.get('avg_pr_auc', 0):.4f}")
            report_lines.append(f"  æ€§èƒ½ç­‰çº§: {overall.get('performance_grade', 'Unknown')}")
            report_lines.append(f"  æµ‹è¯•æ ·æœ¬æ•°: {overall.get('total_samples', 0)}")
        
        # å„ç­–ç•¥è¯¦ç»†ç»“æœ
        report_lines.append(f"\nğŸ“‹ å„å¼‚å¸¸ç­–ç•¥è¯¦ç»†ç»“æœ:")
        for strategy, result in results.items():
            if strategy == 'overall':
                continue
            
            report_lines.append(f"\n  {strategy}:")
            report_lines.append(f"    AUC: {result.get('auc', 0):.4f}")
            report_lines.append(f"    PR-AUC: {result.get('pr_auc', 0):.4f}")
            report_lines.append(f"    æ ·æœ¬æ•°: {result.get('sample_count', 0)}")
            report_lines.append(f"    å¼‚å¸¸æ¯”ä¾‹: {result.get('anomaly_ratio', 0):.3f}")
            
            # é˜ˆå€¼æŒ‡æ ‡
            if 'threshold_metrics' in result:
                report_lines.append(f"    é˜ˆå€¼æŒ‡æ ‡:")
                for threshold, metrics in result['threshold_metrics'].items():
                    report_lines.append(f"      {threshold}: F1={metrics.get('f1', 0):.3f}, "
                                      f"Precision={metrics.get('precision', 0):.3f}, "
                                      f"Recall={metrics.get('recall', 0):.3f}")
        
        report_text = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
        
        return report_text
