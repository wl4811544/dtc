#!/usr/bin/env python3
"""
å®Œç¾ç ”ç©¶è®¡åˆ’çš„é…ç½®æ–‡ä»¶ç”Ÿæˆå™¨

ä¸ºæ‰€æœ‰å®éªŒé…ç½®ç”Ÿæˆå¯¹åº”çš„é…ç½®æ–‡ä»¶
"""

import os
import yaml
from pathlib import Path


def create_base_config():
    """åˆ›å»ºåŸºç¡€é…ç½®æ¨¡æ¿"""
    return {
        'dataset': {
            'name': 'assist17',
            'features': ['q', 's', 'pid', 'it', 'at'],
            'has_temporal': True,
            'has_pid': True,
            'complexity_factor': 1.0,
            'max_sequence_length': 200
        },
        'model': {
            'd_model': 128,
            'n_heads': 8,
            'n_layers': 3,
            'dropout': 0.2,
            'lambda_cl': 0.1,
            'n_know': 16
        },
        'training': {
            'batch_size': 16,
            'learning_rate': 0.001,
            'weight_decay': 1e-5,
            'patience': 10
        },
        'curriculum': {
            'strategy': 'hybrid',
            'curriculum_epochs': 100,
            'anomaly_ratio': 0.1,
            'baseline_ratio': 0.05,
            'max_patience': 5
        },
        'detector': {
            'hidden_dim': 256,
            'num_layers': 3,
            'dropout': 0.3,
            'window_size': 10,
            'learning_rate': 1e-4
        }
    }


def create_expanded_config():
    """åˆ›å»ºæ‰©å±•é…ç½®"""
    config = create_base_config()
    config['model'].update({
        'd_model': 256,
        'n_heads': 16,
        'n_layers': 3
    })
    config['detector'].update({
        'hidden_dim': 512,
        'num_layers': 4,
        'learning_rate': 5e-5
    })
    config['training'].update({
        'batch_size': 12,  # å‡å°‘æ‰¹æ¬¡å¤§å°é€‚åº”æ›´å¤§æ¨¡å‹
        'patience': 15
    })
    config['curriculum'].update({
        'curriculum_epochs': 120,
        'max_patience': 8
    })
    return config


def create_deep_config():
    """åˆ›å»ºæ·±åº¦é…ç½®"""
    config = create_base_config()
    config['model'].update({
        'd_model': 128,
        'n_heads': 8,
        'n_layers': 4  # å¢åŠ å±‚æ•°
    })
    config['detector'].update({
        'hidden_dim': 256,
        'num_layers': 4,
        'learning_rate': 8e-5
    })
    config['training'].update({
        'patience': 12
    })
    config['curriculum'].update({
        'curriculum_epochs': 110,
        'max_patience': 6
    })
    return config


def create_assist09_config():
    """åˆ›å»ºASSIST09é…ç½®"""
    config = create_base_config()
    config['dataset'].update({
        'name': 'assist09',
        'complexity_factor': 0.9,
        'max_sequence_length': 150
    })
    config['curriculum'].update({
        'curriculum_epochs': 80,  # è¾ƒå°æ•°æ®é›†ç”¨è¾ƒå°‘è½®æ•°
        'strategy': 'hybrid'
    })
    return config


def create_algebra05_config():
    """åˆ›å»ºAlgebra05é…ç½®"""
    config = create_base_config()
    config['dataset'].update({
        'name': 'algebra05',
        'has_temporal': False,
        'complexity_factor': 0.7,
        'max_sequence_length': 80
    })
    config['curriculum'].update({
        'curriculum_epochs': 60,  # å°æ•°æ®é›†ç”¨æ›´å°‘è½®æ•°
        'strategy': 'time_driven'  # å°æ•°æ®é›†ç”¨æ—¶é—´é©±åŠ¨
    })
    config['training'].update({
        'batch_size': 8,  # å°æ•°æ®é›†ç”¨å°æ‰¹æ¬¡
        'patience': 8
    })
    return config


def flatten_config_for_stage2(config):
    """å°†é…ç½®æ‰å¹³åŒ–ï¼Œä¾¿äºç¬¬äºŒé˜¶æ®µä½¿ç”¨"""
    flat_config = {}
    
    # åŸºæœ¬å‚æ•°
    flat_config.update({
        'curriculum_strategy': config['curriculum']['strategy'],
        'curriculum_epochs': config['curriculum']['curriculum_epochs'],
        'anomaly_ratio': config['curriculum']['anomaly_ratio'],
        'baseline_ratio': config['curriculum']['baseline_ratio'],
        'max_patience': config['curriculum']['max_patience'],
        
        'detector_hidden_dim': config['detector']['hidden_dim'],
        'detector_num_layers': config['detector']['num_layers'],
        'detector_dropout': config['detector']['dropout'],
        'detector_window_size': config['detector']['window_size'],
        
        'learning_rate': config['detector']['learning_rate'],
        'patience': config['training']['patience'],
        'batch_size': config['training']['batch_size'],
        'difficulty_estimation': True
    })
    
    return flat_config


def save_config(config, filename):
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    config_dir = Path('anomaly_aware_kt/configs')
    config_dir.mkdir(parents=True, exist_ok=True)
    
    filepath = config_dir / filename
    with open(filepath, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print(f"âœ… å·²åˆ›å»ºé…ç½®æ–‡ä»¶: {filepath}")


def main():
    """ç”Ÿæˆæ‰€æœ‰é…ç½®æ–‡ä»¶"""
    print("ğŸ”§ ç”Ÿæˆå®Œç¾ç ”ç©¶è®¡åˆ’çš„é…ç½®æ–‡ä»¶...")
    
    configs = {
        # ASSIST17é…ç½®
        'assist17_base.yaml': create_base_config(),
        'assist17_expanded.yaml': create_expanded_config(),
        'assist17_deep.yaml': create_deep_config(),
        
        # å…¶ä»–æ•°æ®é›†é…ç½®
        'assist09_base.yaml': create_assist09_config(),
        'algebra05_base.yaml': create_algebra05_config(),
    }
    
    # ç”Ÿæˆç¬¬ä¸€é˜¶æ®µé…ç½®æ–‡ä»¶
    for filename, config in configs.items():
        save_config(config, filename)
    
    # ç”Ÿæˆç¬¬äºŒé˜¶æ®µé…ç½®æ–‡ä»¶
    stage2_configs = {}
    for filename, config in configs.items():
        stage2_filename = filename.replace('.yaml', '_curriculum.yaml')
        stage2_config = config.copy()
        stage2_config.update(flatten_config_for_stage2(config))
        stage2_configs[stage2_filename] = stage2_config
    
    for filename, config in stage2_configs.items():
        save_config(config, filename)
    
    print(f"\nğŸ“Š æ€»è®¡ç”Ÿæˆ {len(configs) + len(stage2_configs)} ä¸ªé…ç½®æ–‡ä»¶")
    print("\nğŸ¯ é…ç½®æ–‡ä»¶ç”¨é€”:")
    print("  ç¬¬ä¸€é˜¶æ®µ: assist17_base.yaml, assist17_expanded.yaml, assist17_deep.yaml")
    print("  ç¬¬äºŒé˜¶æ®µ: assist17_base_curriculum.yaml, assist17_expanded_curriculum.yaml, assist17_deep_curriculum.yaml")
    print("  è·¨æ•°æ®é›†: assist09_base.yaml, algebra05_base.yaml")


if __name__ == "__main__":
    main()
