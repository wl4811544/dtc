"""
ËÆ≠ÁªÉÂô®Ê®°Âùó

Êèê‰æõÂºÇÂ∏∏Ê£ÄÊµãÂô®ÂíåÁü•ËØÜËøΩË∏™Ê®°ÂûãÁöÑËÆ≠ÁªÉÂäüËÉΩ„ÄÇ
"""

import os
import json
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from tqdm import tqdm
from collections import defaultdict
from typing import Dict, Optional, Tuple

from .generator import AnomalyGenerator
from .evaluator import AnomalyEvaluator


class BaseTrainer:
    """Âü∫Á°ÄËÆ≠ÁªÉÂô®"""

    def __init__(self,
                 model: nn.Module,
                 device: str = 'cpu',
                 save_dir: str = 'output',
                 patience: int = 10):
        self.model = model.to(device)
        self.device = device
        self.save_dir = save_dir
        self.patience = patience

        os.makedirs(save_dir, exist_ok=True)

        self.history = defaultdict(list)
        self.best_metrics = {}
        self.best_epoch = 0

    def save_checkpoint(self, epoch: int, optimizer, metrics: Dict, is_best: bool = False):
        """‰øùÂ≠òÊ£ÄÊü•ÁÇπ"""
        checkpoint = {
            'epoch': epoch + 1,  # ‰øùÂ≠òÂÆûÈôÖËΩÆÊ¨°
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metrics': metrics,
            'history': dict(self.history)
        }

        # ÊÄªÊòØ‰øùÂ≠òÂΩìÂâçËΩÆÊ¨°ÁöÑÊ®°Âûã
        epoch_path = os.path.join(self.save_dir, f'model_epoch_{epoch+1}.pt')
        try:
            torch.save(checkpoint, epoch_path)
            print(f"  üíæ ‰øùÂ≠òÊ®°Âûã: epoch_{epoch+1}.pt")
        except Exception as e:
            print(f"  ‚ùå ‰øùÂ≠òÂ§±Ë¥• epoch_{epoch+1}: {e}")

        # Â¶ÇÊûúÊòØÊúÄ‰Ω≥Ê®°ÂûãÔºåÈ¢ùÂ§ñ‰øùÂ≠ò‰∏Ä‰ªΩ
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_model.pt')
            try:
                torch.save(checkpoint, best_path)
                print(f"  üèÜ ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã: best_model.pt")
            except Exception as e:
                print(f"  ‚ùå ‰øùÂ≠òÊúÄ‰Ω≥Ê®°ÂûãÂ§±Ë¥•: {e}")

    def load_checkpoint(self, path: str, optimizer: Optional = None):
        """Âä†ËΩΩÊ£ÄÊü•ÁÇπ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])

        if optimizer is not None:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        return checkpoint.get('epoch', 0), checkpoint.get('metrics', {})


class AnomalyDetectorTrainer(BaseTrainer):
    """ÂºÇÂ∏∏Ê£ÄÊµãÂô®ËÆ≠ÁªÉÂô®"""

    def __init__(self, model: nn.Module, device: str = 'cpu',
                 save_dir: str = 'output/detector', patience: int = 10):
        super().__init__(model, device, save_dir, patience)
        self.generator = AnomalyGenerator()
        self.evaluator = AnomalyEvaluator()

    def train(self,
              train_loader,
              val_loader,
              epochs: int = 30,
              learning_rate: float = 1e-3,
              anomaly_ratio: float = 0.1,
              optimize_for: str = 'f1_score') -> Dict:
        """ËÆ≠ÁªÉÂºÇÂ∏∏Ê£ÄÊµãÂô®"""

        # ‰ºòÂåñÂô®ÂíåË∞ÉÂ∫¶Âô®
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-5
        )
        scheduler = ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=5
        )

        # ‰øùÂ≠òÈÖçÁΩÆ
        config = {
            'epochs': epochs,
            'learning_rate': learning_rate,
            'anomaly_ratio': anomaly_ratio,
            'optimize_for': optimize_for
        }
        with open(os.path.join(self.save_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)

        # ËÆ≠ÁªÉÂæ™ÁéØ
        no_improve = 0

        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")

            # ËÆ≠ÁªÉ
            train_loss = self._train_epoch(
                train_loader, optimizer, anomaly_ratio
            )

            # È™åËØÅ
            val_metrics = self._validate_epoch(
                val_loader, anomaly_ratio
            )

            # ËÆ∞ÂΩï
            self.history['train_loss'].append(train_loss)
            for k, v in val_metrics.items():
                self.history[f'val_{k}'].append(v)

            # Ë∞ÉÊï¥Â≠¶‰π†Áéá
            scheduler.step(val_metrics[optimize_for])

            # ÊâìÂç∞ÊåáÊ†á
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val - Recall: {val_metrics['recall']:.3f}, "
                  f"Precision: {val_metrics['precision']:.3f}, "
                  f"F1: {val_metrics['f1_score']:.3f}, "
                  f"AUC: {val_metrics['auc_roc']:.3f}")

            # Ê£ÄÊü•ÊúÄ‰Ω≥
            current_score = val_metrics[optimize_for]
            if not self.best_metrics or current_score > self.best_metrics.get(optimize_for, 0):
                self.best_metrics = val_metrics.copy()
                self.best_epoch = epoch + 1
                self.save_checkpoint(epoch, optimizer, val_metrics, is_best=True)
                no_improve = 0
                print(f"‚úì New best {optimize_for}: {current_score:.4f}")
            else:
                no_improve += 1

            # Êó©ÂÅú
            if no_improve >= self.patience:
                print(f"Early stopping after {self.patience} epochs")
                break

        print(f"\nBest epoch: {self.best_epoch}")
        print(f"Best metrics: {self.best_metrics}")

        return self.best_metrics

    def _train_epoch(self, train_loader, optimizer, anomaly_ratio):
        """ËÆ≠ÁªÉ‰∏Ä‰∏™epoch"""
        self.model.train()
        total_loss = 0
        n_batches = 0

        for batch in tqdm(train_loader, desc="Training"):
            q, s, pid = self._get_batch_data(batch)

            # ÁîüÊàêÂºÇÂ∏∏
            s_anomaly, labels = self.generator.generate_anomalies(
                q, s, anomaly_ratio=anomaly_ratio
            )
            labels = labels.to(self.device)

            # ÂâçÂêë‰º†Êí≠
            loss = self.model.get_loss(q, s_anomaly, labels, pid)

            # ÂèçÂêë‰º†Êí≠
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            n_batches += 1

        return total_loss / n_batches

    def _validate_epoch(self, val_loader, anomaly_ratio):
        """È™åËØÅ‰∏Ä‰∏™epoch"""
        self.model.eval()
        self.evaluator.reset()

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                q, s, pid = self._get_batch_data(batch)

                # ÁîüÊàêÂºÇÂ∏∏
                s_anomaly, labels = self.generator.generate_anomalies(
                    q, s, anomaly_ratio=anomaly_ratio
                )

                # È¢ÑÊµã
                predictions = self.model(q, s_anomaly, pid)

                # Êõ¥Êñ∞ËØÑ‰º∞Âô®
                self.evaluator.update(predictions, labels, q, s)

        return self.evaluator.compute_metrics()

    def _get_batch_data(self, batch):
        """Ëé∑ÂèñÊâπÊ¨°Êï∞ÊçÆ"""
        if len(batch.data) == 2:
            q, s = batch.get("q", "s")
            pid = None
        else:
            q, s, pid = batch.get("q", "s", "pid")

        q = q.to(self.device)
        s = s.to(self.device)
        if pid is not None:
            pid = pid.to(self.device)

        return q, s, pid


class KnowledgeTracingTrainer(BaseTrainer):
    """Áü•ËØÜËøΩË∏™Ê®°ÂûãËÆ≠ÁªÉÂô®"""

    def __init__(self, model: nn.Module, device: str = 'cpu',
                 save_dir: str = 'output/kt_model', patience: int = 10):
        super().__init__(model, device, save_dir, patience)

    def train(self,
              train_loader,
              val_loader,
              epochs: int = 100,
              learning_rate: float = 1e-3,
              use_cl: bool = True) -> Dict:
        """ËÆ≠ÁªÉÁü•ËØÜËøΩË∏™Ê®°Âûã"""

        # ‰ºòÂåñÂô®
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-5
        )

        # ‰øùÂ≠òÈÖçÁΩÆ
        config = {
            'epochs': epochs,
            'learning_rate': learning_rate,
            'use_cl': use_cl,
            'anomaly_weight': getattr(self.model, 'anomaly_weight', 0)
        }
        with open(os.path.join(self.save_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)

        # ËÆ≠ÁªÉÂæ™ÁéØ
        no_improve = 0

        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")

            # ËÆ≠ÁªÉ
            train_metrics = self._train_epoch(
                train_loader, optimizer, use_cl
            )

            # È™åËØÅ
            val_metrics = self._validate_epoch(val_loader)

            # ËÆ∞ÂΩï
            for k, v in train_metrics.items():
                self.history[f'train_{k}'].append(v)
            for k, v in val_metrics.items():
                self.history[f'val_{k}'].append(v)

            # ÊâìÂç∞
            print(f"Train Loss: {train_metrics['loss']:.4f}")
            print(f"Val - ACC: {val_metrics['acc']:.3f}, "
                  f"AUC: {val_metrics['auc']:.3f}, "
                  f"MAE: {val_metrics['mae']:.3f}")

            # Ê£ÄÊü•ÊúÄ‰Ω≥
            current_auc = val_metrics['auc']
            if not self.best_metrics or current_auc > self.best_metrics.get('auc', 0):
                self.best_metrics = val_metrics.copy()
                self.best_epoch = epoch + 1
                self.save_checkpoint(epoch, optimizer, val_metrics, is_best=True)
                no_improve = 0
                print(f"‚úì New best AUC: {current_auc:.4f}")
            else:
                no_improve += 1

            # Êó©ÂÅú
            if no_improve >= self.patience:
                print(f"Early stopping after {self.patience} epochs")
                break

        return self.best_metrics

    def _train_epoch(self, train_loader, optimizer, use_cl):
        """ËÆ≠ÁªÉ‰∏Ä‰∏™epoch"""
        self.model.train()

        total_loss = 0
        total_pred_loss = 0
        total_cl_loss = 0
        n_batches = 0

        for batch in tqdm(train_loader, desc="Training"):
            q, s, pid = self._get_batch_data(batch)

            # ËÆ°ÁÆóÊçüÂ§±
            if use_cl and hasattr(self.model, 'get_cl_loss'):
                loss, pred_loss, cl_loss = self.model.get_cl_loss(q, s, pid)
                total_pred_loss += pred_loss.item() if pred_loss is not None else 0
                total_cl_loss += cl_loss.item() if cl_loss is not None else 0
            else:
                loss = self.model.get_loss(q, s, pid)

            # ÂèçÂêë‰º†Êí≠
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            n_batches += 1

        metrics = {'loss': total_loss / n_batches}
        if use_cl:
            metrics['pred_loss'] = total_pred_loss / n_batches
            metrics['cl_loss'] = total_cl_loss / n_batches

        return metrics

    def _validate_epoch(self, val_loader):
        """È™åËØÅ‰∏Ä‰∏™epoch"""
        self.model.eval()

        from DTransformer.eval import Evaluator
        evaluator = Evaluator()

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                q, s, pid = self._get_batch_data(batch)

                # È¢ÑÊµã
                if hasattr(self.model, 'predict_with_anomaly'):
                    y, *_ = self.model.predict_with_anomaly(q, s, pid)
                else:
                    y, *_ = self.model.predict(q, s, pid)

                # ËØÑ‰º∞
                evaluator.evaluate(s, torch.sigmoid(y))

        return evaluator.report()

    def _get_batch_data(self, batch):
        """Ëé∑ÂèñÊâπÊ¨°Êï∞ÊçÆ"""
        if len(batch.data) == 2:
            q, s = batch.get("q", "s")
            pid = None
        else:
            q, s, pid = batch.get("q", "s", "pid")

        q = q.to(self.device)
        s = s.to(self.device)
        if pid is not None:
            pid = pid.to(self.device)

        return q, s, pid