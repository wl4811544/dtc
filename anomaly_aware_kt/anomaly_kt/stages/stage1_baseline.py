"""
Stage 1: Baseline DTransformer Model Training

ä» full_pipeline.py ä¸­æå–çš„ç¬¬ä¸€é˜¶æ®µä»£ç ï¼Œè®­ç»ƒåŸºçº¿DTransformeræ¨¡å‹
"""

import os
import sys
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from DTransformer.model import DTransformer
from anomaly_kt.trainer import KnowledgeTracingTrainer


def validate_stage1_parameters(args, dataset_config):
    """éªŒè¯ç¬¬ä¸€é˜¶æ®µæ‰€éœ€çš„å‚æ•°

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        dataset_config: æ•°æ®é›†é…ç½®å­—å…¸

    Raises:
        ValueError: å¦‚æœç¼ºå°‘å¿…éœ€å‚æ•°
    """
    # æ£€æŸ¥åŸºæœ¬å‚æ•°
    required_basic_params = ['device', 'output_dir']
    for param in required_basic_params:
        if not hasattr(args, param) or getattr(args, param) is None:
            raise ValueError(f"Missing required parameter: {param}")

    # æ£€æŸ¥æ¨¡å‹å‚æ•°
    required_model_params = [
        'd_model', 'n_heads', 'n_know', 'n_layers',
        'dropout', 'lambda_cl', 'window'
    ]
    for param in required_model_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required model parameter: {param}")

    # æ£€æŸ¥è®­ç»ƒå‚æ•°
    required_training_params = ['kt_epochs', 'learning_rate', 'patience']
    for param in required_training_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required training parameter: {param}")

    # æ£€æŸ¥æ•°æ®é›†é…ç½®
    required_dataset_params = ['n_questions', 'n_pid']
    for param in required_dataset_params:
        if param not in dataset_config:
            raise ValueError(f"Missing required dataset parameter: {param}")

    # æ£€æŸ¥å¯é€‰å‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
    if not hasattr(args, 'with_pid'):
        args.with_pid = False
    if not hasattr(args, 'proj'):
        args.proj = False
    if not hasattr(args, 'hard_neg'):
        args.hard_neg = False
    if not hasattr(args, 'use_cl'):
        args.use_cl = False

    print("âœ“ ç¬¬ä¸€é˜¶æ®µå‚æ•°éªŒè¯é€šè¿‡")


def print_stage1_parameters(args, dataset_config):
    """æ‰“å°ç¬¬ä¸€é˜¶æ®µå‚æ•°ä¿¡æ¯"""
    print("\nğŸ“‹ ç¬¬ä¸€é˜¶æ®µå‚æ•°é…ç½®:")
    print("  åŸºæœ¬å‚æ•°:")
    print(f"    è®¾å¤‡: {args.device}")
    print(f"    è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"    ä½¿ç”¨é—®é¢˜ID: {getattr(args, 'with_pid', False)}")

    print("  æ¨¡å‹å‚æ•°:")
    print(f"    æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"    æ³¨æ„åŠ›å¤´æ•°: {args.n_heads}")
    print(f"    çŸ¥è¯†æ¦‚å¿µæ•°: {args.n_know}")
    print(f"    å±‚æ•°: {args.n_layers}")
    print(f"    Dropout: {args.dropout}")
    print(f"    å¯¹æ¯”å­¦ä¹ æƒé‡: {args.lambda_cl}")
    print(f"    ä½¿ç”¨æŠ•å½±: {getattr(args, 'proj', False)}")
    print(f"    å›°éš¾è´Ÿæ ·æœ¬: {getattr(args, 'hard_neg', False)}")
    print(f"    çª—å£å¤§å°: {args.window}")

    print("  è®­ç»ƒå‚æ•°:")
    print(f"    è®­ç»ƒè½®æ•°: {args.kt_epochs}")
    print(f"    å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"    æ—©åœè€å¿ƒ: {args.patience}")
    print(f"    ä½¿ç”¨å¯¹æ¯”å­¦ä¹ : {getattr(args, 'use_cl', False)}")

    print("  æ•°æ®é›†å‚æ•°:")
    print(f"    é—®é¢˜æ€»æ•°: {dataset_config['n_questions']}")
    print(f"    é—®é¢˜IDæ€»æ•°: {dataset_config['n_pid']}")


def train_baseline_model(args, dataset_config, train_data, val_data):
    """è®­ç»ƒåŸºçº¿DTransformeræ¨¡å‹

    è¿™æ˜¯ä» full_pipeline.py ä¸­ç›´æ¥æå–çš„ç¬¬ä¸€é˜¶æ®µä»£ç 

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«æ¨¡å‹å’Œè®­ç»ƒé…ç½®
        dataset_config: æ•°æ®é›†é…ç½®ï¼ŒåŒ…å« n_questions, n_pid ç­‰
        train_data: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_data: éªŒè¯æ•°æ®åŠ è½½å™¨

    Returns:
        str: è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
    """
    # éªŒè¯å‚æ•°
    validate_stage1_parameters(args, dataset_config)

    print("\n" + "="*60)
    print("PHASE 1: Training Baseline DTransformer")
    print("="*60)

    # æ‰“å°å‚æ•°ä¿¡æ¯
    print_stage1_parameters(args, dataset_config)

    # åˆ›å»ºæ¨¡å‹
    model = DTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_know=args.n_know,
        n_layers=args.n_layers,
        dropout=args.dropout,
        lambda_cl=args.lambda_cl,
        proj=args.proj,
        hard_neg=args.hard_neg,
        window=args.window
    )

    # è®­ç»ƒå™¨
    trainer = KnowledgeTracingTrainer(
        model=model,
        device=args.device,
        save_dir=os.path.join(args.output_dir, 'baseline'),
        patience=args.patience
    )

    # è®­ç»ƒ
    baseline_metrics = trainer.train(
        train_loader=train_data,
        val_loader=val_data,
        epochs=args.kt_epochs,
        learning_rate=args.learning_rate,
        use_cl=args.use_cl
    )

    print(f"\nBaseline training completed!")
    print(f"Best AUC: {baseline_metrics['auc']:.4f}")

    return os.path.join(args.output_dir, 'baseline', 'best_model.pt')
