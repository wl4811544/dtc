"""
Stage 2: Curriculum Learning Anomaly Detection Training

åŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹è®­ç»ƒæ¨¡å—ï¼Œå®ç°æˆ‘ä»¬è®¾è®¡çš„åˆ›æ–°æ–¹æ³•
"""

import os
import sys
import torch
import numpy as np
from typing import Dict, Any, Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from DTransformer.model import DTransformer
from anomaly_kt.curriculum_learning import (
    CurriculumScheduler,
    CurriculumAnomalyGenerator,
    DifficultyEstimator,
    BaselineAnomalyGenerator
)
from anomaly_kt.detector import CausalAnomalyDetector
from anomaly_kt.trainer import AnomalyDetectorTrainer
from anomaly_kt.curriculum_learning.curriculum_trainer import CurriculumTrainer


def validate_stage2_parameters(args, dataset_config):
    """éªŒè¯ç¬¬äºŒé˜¶æ®µæ‰€éœ€çš„å‚æ•°
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        dataset_config: æ•°æ®é›†é…ç½®å­—å…¸
        
    Raises:
        ValueError: å¦‚æœç¼ºå°‘å¿…éœ€å‚æ•°
    """
    # æ£€æŸ¥åŸºæœ¬å‚æ•°
    required_basic_params = ['device', 'output_dir', 'baseline_model_path']
    for param in required_basic_params:
        if not hasattr(args, param) or getattr(args, param) is None:
            raise ValueError(f"Missing required parameter: {param}")
    
    # æ£€æŸ¥åŸºçº¿æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.baseline_model_path):
        raise ValueError(f"Baseline model file not found: {args.baseline_model_path}")
    
    # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ å‚æ•°
    required_curriculum_params = [
        'curriculum_strategy', 'curriculum_epochs', 'anomaly_ratio'
    ]
    for param in required_curriculum_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required curriculum parameter: {param}")
    
    # æ£€æŸ¥å¼‚å¸¸æ£€æµ‹å™¨å‚æ•°
    required_detector_params = [
        'detector_hidden_dim', 'detector_num_layers', 'detector_num_heads', 'detector_dropout'
    ]
    for param in required_detector_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required detector parameter: {param}")
    
    # æ£€æŸ¥è®­ç»ƒå‚æ•°
    required_training_params = ['learning_rate', 'patience']
    for param in required_training_params:
        if not hasattr(args, param):
            raise ValueError(f"Missing required training parameter: {param}")
    
    # è®¾ç½®é»˜è®¤å€¼
    if not hasattr(args, 'baseline_ratio'):
        args.baseline_ratio = 0.05
    if not hasattr(args, 'max_patience'):
        args.max_patience = 5
    if not hasattr(args, 'difficulty_estimation'):
        args.difficulty_estimation = True
    
    print("âœ“ ç¬¬äºŒé˜¶æ®µå‚æ•°éªŒè¯é€šè¿‡")


def print_stage2_parameters(args, dataset_config):
    """æ‰“å°ç¬¬äºŒé˜¶æ®µå‚æ•°ä¿¡æ¯"""
    print("\nğŸ“‹ ç¬¬äºŒé˜¶æ®µå‚æ•°é…ç½®:")
    print("  åŸºæœ¬å‚æ•°:")
    print(f"    è®¾å¤‡: {args.device}")
    print(f"    è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"    åŸºçº¿æ¨¡å‹: {args.baseline_model_path}")
    
    print("  è¯¾ç¨‹å­¦ä¹ å‚æ•°:")
    print(f"    è°ƒåº¦ç­–ç•¥: {args.curriculum_strategy}")
    print(f"    è®­ç»ƒè½®æ•°: {args.curriculum_epochs}")
    print(f"    å¼‚å¸¸æ¯”ä¾‹: {args.anomaly_ratio}")
    print(f"    åŸºçº¿å¼‚å¸¸æ¯”ä¾‹: {args.baseline_ratio}")
    print(f"    æœ€å¤§è€å¿ƒå€¼: {args.max_patience}")
    
    print("  å¼‚å¸¸æ£€æµ‹å™¨å‚æ•°:")
    print(f"    éšè—å±‚ç»´åº¦: {args.detector_hidden_dim}")
    print(f"    å±‚æ•°: {args.detector_num_layers}")
    print(f"    æ³¨æ„åŠ›å¤´æ•°: {args.detector_num_heads}")
    print(f"    Dropout: {args.detector_dropout}")
    
    print("  è®­ç»ƒå‚æ•°:")
    print(f"    å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"    æ—©åœè€å¿ƒ: {args.patience}")
    print(f"    éš¾åº¦è¯„ä¼°: {args.difficulty_estimation}")

    print("  æ•°æ®é›†å‚æ•°:")
    print(f"    é—®é¢˜æ€»æ•°: {dataset_config['n_questions']}")
    print(f"    é—®é¢˜IDæ€»æ•°: {dataset_config['n_pid']}")


def load_baseline_model(model_path: str, dataset_config: Dict, args) -> DTransformer:
    """åŠ è½½åŸºçº¿æ¨¡å‹

    Args:
        model_path: åŸºçº¿æ¨¡å‹æ–‡ä»¶è·¯å¾„
        dataset_config: æ•°æ®é›†é…ç½®
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        åŠ è½½çš„åŸºçº¿æ¨¡å‹
    """
    print(f"\nğŸ“¥ åŠ è½½åŸºçº¿æ¨¡å‹: {model_path}")

    # ä»ç¬¬ä¸€é˜¶æ®µçš„é…ç½®æ–‡ä»¶ä¸­è¯»å–æ­£ç¡®çš„æ¨¡å‹å‚æ•°
    stage1_config_path = os.path.join(os.path.dirname(model_path), '..', 'config.yaml')
    stage1_config = {}

    if os.path.exists(stage1_config_path):
        import yaml
        with open(stage1_config_path, 'r') as f:
            stage1_config = yaml.safe_load(f)
        print(f"âœ“ ä»ç¬¬ä¸€é˜¶æ®µé…ç½®æ–‡ä»¶è¯»å–å‚æ•°: {stage1_config_path}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¬ä¸€é˜¶æ®µé…ç½®æ–‡ä»¶: {stage1_config_path}")

    # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆä½¿ç”¨ç¬¬ä¸€é˜¶æ®µçš„å®é™…å‚æ•°ï¼‰
    model = DTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if stage1_config.get('with_pid', False) else 0,
        d_model=stage1_config.get('d_model', 256),
        n_heads=stage1_config.get('n_heads', 16),
        n_know=stage1_config.get('n_know', 16),
        n_layers=stage1_config.get('n_layers', 3),
        dropout=stage1_config.get('dropout', 0.2),
        lambda_cl=stage1_config.get('lambda_cl', 0.1),
        proj=stage1_config.get('proj', True),
        hard_neg=stage1_config.get('hard_neg', False),
        window=stage1_config.get('window', 1)
    )

    print(f"âœ“ åŸºçº¿æ¨¡å‹å‚æ•°:")
    print(f"  d_model: {stage1_config.get('d_model', 256)}")
    print(f"  n_heads: {stage1_config.get('n_heads', 16)}")
    print(f"  n_layers: {stage1_config.get('n_layers', 3)}")
    print(f"  dropout: {stage1_config.get('dropout', 0.2)}")
    print(f"  with_pid: {stage1_config.get('with_pid', False)}")
    
    # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå…¼å®¹PyTorch 2.6+ï¼‰
    checkpoint = torch.load(model_path, map_location=args.device, weights_only=False)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ“ æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ (æ¥è‡ªcheckpoint)")
        if 'auc' in checkpoint:
            print(f"  åŸºçº¿æ¨¡å‹AUC: {checkpoint['auc']:.4f}")
    else:
        model.load_state_dict(checkpoint)
        print(f"âœ“ æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ (ç›´æ¥åŠ è½½)")
    
    model.to(args.device)
    model.eval()  # åŸºçº¿æ¨¡å‹ç”¨äºç‰¹å¾æå–ï¼Œè®¾ä¸ºè¯„ä¼°æ¨¡å¼
    
    return model


def create_anomaly_detector(dataset_config: Dict, baseline_model: DTransformer, args) -> CausalAnomalyDetector:
    """åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨

    Args:
        dataset_config: æ•°æ®é›†é…ç½®
        baseline_model: åŸºçº¿æ¨¡å‹
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        å¼‚å¸¸æ£€æµ‹å™¨å®ä¾‹
    """
    print("\nğŸ” åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨...")

    # ä»åŸºçº¿æ¨¡å‹è·å–å®é™…çš„æ¨¡å‹å‚æ•°
    d_model = baseline_model.q_embed.embedding_dim  # ä»åµŒå…¥å±‚è·å–d_model
    n_heads = baseline_model.n_heads  # ç›´æ¥è·å–n_heads

    # ä½¿ç”¨ä¸åŸºçº¿æ¨¡å‹ä¸€è‡´çš„å‚æ•°ï¼ˆç¡®ä¿å…¼å®¹æ€§ï¼‰
    detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config.get('n_pid', 0),
        d_model=d_model,  # ä½¿ç”¨åŸºçº¿æ¨¡å‹çš„å®é™…ç»´åº¦
        n_heads=n_heads,  # ä½¿ç”¨åŸºçº¿æ¨¡å‹çš„å®é™…å¤´æ•°
        n_layers=getattr(args, 'detector_num_layers', 3),
        dropout=getattr(args, 'detector_dropout', 0.2),
        window_size=getattr(args, 'detector_window_size', 10)
    )
    
    detector.to(args.device)
    
    print(f"âœ“ å¼‚å¸¸æ£€æµ‹å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  é—®é¢˜æ€»æ•°: {dataset_config['n_questions']}")
    print(f"  é—®é¢˜IDæ€»æ•°: {dataset_config.get('n_pid', 0)}")
    print(f"  éšè—ç»´åº¦: {d_model} (ä¸åŸºçº¿æ¨¡å‹ä¸€è‡´)")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {n_heads} (ä¸åŸºçº¿æ¨¡å‹ä¸€è‡´)")
    print(f"  å±‚æ•°: {getattr(args, 'detector_num_layers', 3)}")
    print(f"  Dropout: {getattr(args, 'detector_dropout', 0.2)}")
    
    return detector


def train_curriculum_anomaly_detector(args, dataset_config, train_data, val_data) -> str:
    """è®­ç»ƒè¯¾ç¨‹å­¦ä¹ å¼‚å¸¸æ£€æµ‹å™¨
    
    è¿™æ˜¯ç¬¬äºŒé˜¶æ®µçš„æ ¸å¿ƒå‡½æ•°ï¼Œå®ç°åŸºäºè¯¾ç¨‹å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹è®­ç»ƒ
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«è¯¾ç¨‹å­¦ä¹ å’Œå¼‚å¸¸æ£€æµ‹é…ç½®
        dataset_config: æ•°æ®é›†é…ç½®ï¼ŒåŒ…å« n_questions, n_pid ç­‰
        train_data: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_data: éªŒè¯æ•°æ®åŠ è½½å™¨
        
    Returns:
        str: è®­ç»ƒå¥½çš„å¼‚å¸¸æ£€æµ‹å™¨æ–‡ä»¶è·¯å¾„
    """
    # éªŒè¯å‚æ•°
    validate_stage2_parameters(args, dataset_config)
    
    print("\n" + "="*60)
    print("PHASE 2: Curriculum Learning Anomaly Detection Training")
    print("="*60)
    
    # æ‰“å°å‚æ•°ä¿¡æ¯
    print_stage2_parameters(args, dataset_config)
    
    # 1. åŠ è½½åŸºçº¿æ¨¡å‹
    baseline_model = load_baseline_model(args.baseline_model_path, dataset_config, args)
    
    # 2. åˆ›å»ºå¼‚å¸¸æ£€æµ‹å™¨ï¼ˆä½¿ç”¨ä¸åŸºçº¿æ¨¡å‹ä¸€è‡´çš„å‚æ•°ï¼‰
    anomaly_detector = create_anomaly_detector(dataset_config, baseline_model, args)
    
    # 3. åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ ç»„ä»¶
    print("\nğŸ“ åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ ç»„ä»¶...")
    
    scheduler = CurriculumScheduler(
        strategy=args.curriculum_strategy,
        dataset_name=args.dataset,
        total_epochs=args.curriculum_epochs
    )
    
    curriculum_generator = CurriculumAnomalyGenerator(args.dataset)
    
    difficulty_estimator = DifficultyEstimator(args.dataset) if args.difficulty_estimation else None
    
    baseline_generator = BaselineAnomalyGenerator()
    
    print(f"âœ“ è¯¾ç¨‹è°ƒåº¦å™¨: {args.curriculum_strategy} ç­–ç•¥")
    print(f"âœ“ å¼‚å¸¸ç”Ÿæˆå™¨: {args.dataset} æ•°æ®é›†ä¼˜åŒ–")
    print(f"âœ“ éš¾åº¦è¯„ä¼°å™¨: {'å¯ç”¨' if args.difficulty_estimation else 'ç¦ç”¨'}")
    
    # 4. åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨
    print("\nğŸš€ åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨...")

    # å…ˆåˆ›å»ºåŸºç¡€è®­ç»ƒå™¨
    base_trainer = AnomalyDetectorTrainer(
        model=anomaly_detector,
        device=args.device,
        save_dir=os.path.join(args.output_dir, 'curriculum_anomaly'),
        patience=args.patience
    )

    # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨
    trainer = CurriculumTrainer(
        base_trainer=base_trainer,
        dataset_name=args.dataset,
        strategy=args.curriculum_strategy,
        total_epochs=args.curriculum_epochs,
        output_dir=os.path.join(args.output_dir, 'curriculum_learning')
    )
    
    print(f"âœ“ è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    print(f"  ä¿å­˜ç›®å½•: {os.path.join(args.output_dir, 'curriculum_anomaly')}")

    # 5. å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
    print("\n" + "="*60)
    print("å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ...")
    print("="*60)

    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    import torch.optim as optim
    optimizer = optim.Adam(anomaly_detector.parameters(), lr=args.learning_rate)
    criterion = torch.nn.BCEWithLogitsLoss()

    # ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ–¹æ³•
    training_metrics = trainer.train_with_curriculum(
        train_loader=train_data,
        val_loader=val_data,
        model=anomaly_detector,
        optimizer=optimizer,
        criterion=criterion,
        epochs=args.curriculum_epochs
    )

    print(f"\nCurriculum learning training completed!")
    print(f"Best AUC: {training_metrics.get('auc', 0.0):.4f}")
    print(f"Best F1: {training_metrics.get('f1', 0.0):.4f}")
    
    # 7. ä¿å­˜è®­ç»ƒæ€»ç»“
    save_training_summary(args, training_metrics, scheduler)
    
    return os.path.join(args.output_dir, 'curriculum_anomaly', 'best_model.pt')


def save_training_summary(args, metrics: Dict, scheduler: CurriculumScheduler):
    """ä¿å­˜è®­ç»ƒæ€»ç»“"""
    import json
    from datetime import datetime
    
    summary = {
        'experiment_info': {
            'dataset': args.dataset,
            'strategy': args.curriculum_strategy,
            'timestamp': datetime.now().isoformat(),
            'baseline_model': args.baseline_model_path
        },
        'curriculum_config': {
            'total_epochs': args.curriculum_epochs,
            'anomaly_ratio': args.anomaly_ratio,
            'baseline_ratio': args.baseline_ratio,
            'max_patience': args.max_patience
        },
        'final_metrics': metrics,
        'curriculum_summary': scheduler.get_schedule_summary(),
        'phase_transitions': scheduler.phase_history
    }
    
    summary_path = os.path.join(args.output_dir, 'curriculum_training_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“å·²ä¿å­˜: {summary_path}")


def test_curriculum_components(args, dataset_config):
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ ç»„ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•è¯¾ç¨‹å­¦ä¹ ç»„ä»¶...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len = 4, 20
    q = torch.randint(0, dataset_config['n_questions'], (batch_size, seq_len))
    s = torch.randint(0, 2, (batch_size, seq_len))
    
    # æµ‹è¯•åŸºçº¿å¼‚å¸¸ç”Ÿæˆå™¨
    baseline_gen = BaselineAnomalyGenerator()
    s_baseline, labels_baseline = baseline_gen.generate_baseline_anomalies(
        q, s, strategy='random_flip', anomaly_ratio=0.2
    )
    print(f"âœ“ åŸºçº¿å¼‚å¸¸ç”Ÿæˆ: {labels_baseline.sum().item()} ä¸ªå¼‚å¸¸")
    
    # æµ‹è¯•è¯¾ç¨‹å¼‚å¸¸ç”Ÿæˆå™¨
    curriculum_gen = CurriculumAnomalyGenerator(args.dataset)
    s_curr, labels_curr, diff_curr = curriculum_gen.generate_curriculum_anomalies(
        q, s,
        difficulty_levels=[1, 2],
        level_weights={1: 0.7, 2: 0.3},
        anomaly_ratio=0.1
    )
    print(f"âœ“ è¯¾ç¨‹å¼‚å¸¸ç”Ÿæˆ: {labels_curr.sum().item()} ä¸ªå¼‚å¸¸")
    
    # æµ‹è¯•è°ƒåº¦å™¨
    scheduler = CurriculumScheduler(args.curriculum_strategy, args.dataset, 10)
    schedule_info = scheduler.update(0, {'auc': 0.7, 'f1': 0.65})
    print(f"âœ“ è¯¾ç¨‹è°ƒåº¦å™¨: Phase {schedule_info['current_phase']}")
    
    print("âœ… æ‰€æœ‰ç»„ä»¶æµ‹è¯•é€šè¿‡!")
    return True
