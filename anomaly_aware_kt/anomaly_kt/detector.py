"""
å› æœå¼‚å¸¸æ£€æµ‹å™¨æ¨¡å—

å®ç°ä¸¥æ ¼éµå®ˆæ—¶åºå› æœæ€§çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple
import math
import random


class CausalAnomalyDetector(nn.Module):
    """å› æœå¼‚å¸¸æ£€æµ‹å™¨ - æ£€æµ‹ç­”é¢˜åºåˆ—ä¸­çš„å¼‚å¸¸è¡Œä¸º"""

    def __init__(self,
                 n_questions: int,
                 n_pid: int = 0,
                 d_model: int = 128,
                 n_heads: int = 8,
                 n_layers: int = 2,
                 dropout: float = 0.1,
                 window_size: int = 10):
        """
        Args:
            n_questions: é—®é¢˜æ€»æ•°
            n_pid: é—®é¢˜éƒ¨åˆ†IDæ€»æ•°
            d_model: æ¨¡å‹éšè—å±‚ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            n_layers: Transformerå±‚æ•°
            dropout: Dropoutç‡
            window_size: ç»Ÿè®¡ç‰¹å¾çª—å£å¤§å°
        """
        super().__init__()

        self.n_questions = n_questions
        self.window_size = window_size
        self.d_model = d_model

        # åµŒå…¥å±‚
        self.q_embed = nn.Embedding(n_questions + 1, d_model)
        self.s_embed = nn.Embedding(2, d_model)
        self.position_embed = nn.Embedding(512, d_model)

        if n_pid > 0:
            self.pid_embed = nn.Embedding(n_pid + 1, d_model // 2)
        else:
            self.pid_embed = None

        # ç‰¹å¾èåˆ
        self.feature_fusion = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Transformerç¼–ç å™¨
        self.transformer_layers = nn.ModuleList([
            TransformerLayer(d_model, n_heads, dropout)
            for _ in range(n_layers)
        ])

        # ç»Ÿè®¡ç‰¹å¾æå–
        self.stat_features = nn.Sequential(
            nn.Linear(6, 32),
            nn.ReLU(),
            nn.Linear(32, d_model // 4)
        )

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model + d_model // 4, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, 1)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, q: torch.Tensor, s: torch.Tensor,
                pid: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = q.shape
        device = q.device

        # å¤„ç†mask
        mask = (s >= 0)
        q_masked = q.masked_fill(~mask, 0)
        s_masked = s.masked_fill(~mask, 0)

        # åµŒå…¥
        q_emb = self.q_embed(q_masked)
        s_emb = self.s_embed(s_masked)

        # ä½ç½®ç¼–ç 
        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.position_embed(positions)

        # èåˆç‰¹å¾
        combined = torch.cat([q_emb, s_emb], dim=-1)
        h = self.feature_fusion(combined) + pos_emb

        if self.pid_embed is not None and pid is not None:
            pid_masked = pid.masked_fill(~mask, 0)
            pid_emb = self.pid_embed(pid_masked)
            h[:, :, :self.d_model//2] += pid_emb

        h = self.dropout(h)

        # åˆ›å»ºå› æœæ©ç 
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).bool()

        # Transformerç¼–ç 
        for layer in self.transformer_layers:
            h = layer(h, causal_mask, mask)

        # æå–ç»Ÿè®¡ç‰¹å¾
        stat_feat = self._extract_statistics(s_masked, mask)
        stat_emb = self.stat_features(stat_feat)

        # ç»„åˆç‰¹å¾å¹¶åˆ†ç±»
        combined = torch.cat([h, stat_emb], dim=-1)
        logits = self.classifier(combined).squeeze(-1)

        # åº”ç”¨mask
        logits = logits.masked_fill(~mask, -1e9)

        # ç›´æ¥è¿”å›logitsï¼Œè®©æŸå¤±å‡½æ•°å¤„ç†sigmoid
        return logits

    def _extract_statistics(self, s: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """æå–ç»Ÿè®¡ç‰¹å¾ï¼ˆåªåŸºäºå†å²ï¼‰"""
        batch_size, seq_len = s.shape
        device = s.device
        features = []

        for t in range(seq_len):
            # å†å²çª—å£
            start = max(0, t - self.window_size + 1)
            end = t + 1

            window_s = s[:, start:end]
            window_mask = mask[:, start:end]

            # è®¡ç®—ç‰¹å¾
            feat = self._compute_window_features(window_s, window_mask, t, seq_len)
            features.append(feat)

        return torch.stack(features, dim=1)

    def _compute_window_features(self, window_s: torch.Tensor, window_mask: torch.Tensor,
                                t: int, seq_len: int) -> torch.Tensor:
        """è®¡ç®—çª—å£ç»Ÿè®¡ç‰¹å¾"""
        batch_size = window_s.shape[0]
        device = window_s.device

        # æœ‰æ•ˆæ•°é‡
        valid_count = window_mask.sum(dim=1, keepdim=True).float()

        # 1. æ­£ç¡®ç‡
        correct_rate = (window_s * window_mask).sum(dim=1, keepdim=True).float() / (valid_count + 1e-6)

        # 2. æœ€è¿‘5ä¸ªçš„æ­£ç¡®ç‡
        recent_k = min(5, window_s.shape[1])
        if recent_k > 0:
            recent_s = window_s[:, -recent_k:]
            recent_mask = window_mask[:, -recent_k:]
            recent_count = recent_mask.sum(dim=1, keepdim=True).float()
            recent_correct = (recent_s * recent_mask).sum(dim=1, keepdim=True).float() / (recent_count + 1e-6)
        else:
            recent_correct = torch.zeros(batch_size, 1, device=device)

        # 3. å˜åŒ–ç‡
        change_count = torch.zeros(batch_size, 1, device=device)
        if window_s.shape[1] > 1:
            for i in range(1, window_s.shape[1]):
                valid_trans = window_mask[:, i] * window_mask[:, i-1]
                changes = (window_s[:, i] != window_s[:, i-1]) * valid_trans
                change_count += changes.unsqueeze(1).float()
        change_rate = change_count / (valid_count + 1e-6)

        # 4. æœ€å¤§è¿ç»­é•¿åº¦
        max_consecutive = self._compute_max_consecutive(window_s, window_mask)

        # 5. ç›¸å¯¹ä½ç½®
        relative_position = torch.full((batch_size, 1), t / max(seq_len, 100), device=device)

        # 6. çª—å£è¦†ç›–ç‡
        window_coverage = valid_count / self.window_size

        return torch.cat([
            correct_rate,
            recent_correct,
            change_rate,
            max_consecutive / self.window_size,
            relative_position,
            window_coverage
        ], dim=1)

    def _compute_max_consecutive(self, window_s: torch.Tensor,
                                window_mask: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—æœ€å¤§è¿ç»­ç›¸åŒç­”æ¡ˆé•¿åº¦"""
        batch_size = window_s.shape[0]
        device = window_s.device
        max_consecutive = torch.zeros(batch_size, 1, device=device)

        for b in range(batch_size):
            if window_mask[b].sum() == 0:
                continue

            max_len = 0
            current_len = 0
            prev_val = -1

            for i in range(window_s.shape[1]):
                if window_mask[b, i]:
                    if window_s[b, i] == prev_val:
                        current_len += 1
                    else:
                        max_len = max(max_len, current_len)
                        current_len = 1
                        prev_val = window_s[b, i].item()

            max_consecutive[b] = max(max_len, current_len)

        return max_consecutive

    def get_loss(self, q: torch.Tensor, s: torch.Tensor,
                 labels: torch.Tensor, pid: Optional[torch.Tensor] = None) -> torch.Tensor:
        """è®¡ç®—æŸå¤±å‡½æ•°"""
        logits = self(q, s, pid)  # ç°åœ¨ç›´æ¥è¿”å›logits

        mask = (s >= 0)
        valid_logits = logits[mask]
        valid_labels = labels[mask].float()

        # è®¡ç®—ç±»åˆ«æƒé‡
        pos_count = valid_labels.sum()
        neg_count = len(valid_labels) - pos_count

        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆè®­ç»ƒåˆæœŸï¼‰
        if hasattr(self, 'training') and self.training and random.random() < 0.01:  # 1%æ¦‚ç‡æ‰“å°
            pred_probs = torch.sigmoid(valid_logits)
            print(f"ğŸ”§ Batch stats - Pos: {pos_count.item()}, Neg: {neg_count.item()}, "
                  f"Logits mean: {valid_logits.mean().item():.3f}, "
                  f"Pred mean: {pred_probs.mean().item():.3f}")

        # ä½¿ç”¨æ›´å¼ºçš„ç±»åˆ«æƒé‡
        if pos_count > 0:
            pos_weight = neg_count / pos_count
            pos_weight = torch.clamp(pos_weight, min=1.0, max=20.0)  # å¢åŠ ä¸Šé™åˆ°20
        else:
            pos_weight = torch.tensor(10.0)

        pos_weight = pos_weight.to(logits.device)

        # ç›´æ¥ä½¿ç”¨ BCEWithLogitsLoss
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        loss = criterion(valid_logits, valid_labels)

        return loss


class TransformerLayer(nn.Module):
    """å› æœTransformerå±‚"""

    def __init__(self, d_model: int, n_heads: int, dropout: float):
        super().__init__()

        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )

        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model)
        )

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, causal_mask: torch.Tensor,
                padding_mask: torch.Tensor) -> torch.Tensor:
        # å‡†å¤‡æ³¨æ„åŠ›æ©ç 
        attn_mask = ~causal_mask  # MultiheadAttentionéœ€è¦Trueè¡¨ç¤ºè¢«maskçš„ä½ç½®

        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.attention(
            x, x, x,
            attn_mask=attn_mask,
            key_padding_mask=~padding_mask
        )
        x = self.norm1(x + self.dropout(attn_out))

        # å‰é¦ˆ
        ff_out = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_out))

        return x