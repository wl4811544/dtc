"""
ç»Ÿä¸€çš„å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒå™¨

æ”¯æŒä¸‰ç§è®­ç»ƒç­–ç•¥ï¼šBasic, Enhanced, Aggressive
"""

import os
import json
import torch
import torch.nn as nn
from tqdm import tqdm
from collections import defaultdict
from typing import Dict, Optional, Any

from .trainer import BaseTrainer
from .training_strategies import StrategyFactory, TrainingStrategy
from .evaluator import AnomalyEvaluator


class UnifiedAnomalyTrainer(BaseTrainer):
    """ç»Ÿä¸€çš„å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒå™¨"""

    def __init__(self,
                 model: nn.Module,
                 device: str = 'cpu',
                 save_dir: str = 'output/detector',
                 patience: int = 10,
                 strategy: str = 'basic'):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è®­ç»ƒå™¨

        Args:
            model: å¼‚å¸¸æ£€æµ‹æ¨¡å‹
            device: è®¾å¤‡
            save_dir: ä¿å­˜ç›®å½•
            patience: æ—©åœè€å¿ƒå€¼
            strategy: è®­ç»ƒç­–ç•¥ ('basic', 'enhanced', 'aggressive')
        """
        super().__init__(model, device, save_dir, patience)

        # åˆ›å»ºè®­ç»ƒç­–ç•¥
        self.strategy = StrategyFactory.create_strategy(
            strategy, model, device, save_dir, patience
        )
        self.strategy_name = strategy

        # è¯„ä¼°å™¨
        self.evaluator = AnomalyEvaluator()

        print(f"ğŸ¯ ä½¿ç”¨è®­ç»ƒç­–ç•¥: {self.strategy.get_strategy_name()}")

    def train(self,
              train_loader,
              val_loader,
              epochs: Optional[int] = None,
              learning_rate: Optional[float] = None,
              anomaly_ratio: Optional[float] = None,
              optimize_for: Optional[str] = None,
              **kwargs) -> Dict[str, float]:
        """
        è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•° (Noneåˆ™ä½¿ç”¨ç­–ç•¥é»˜è®¤å€¼)
            learning_rate: å­¦ä¹ ç‡ (Noneåˆ™ä½¿ç”¨ç­–ç•¥é»˜è®¤å€¼)
            anomaly_ratio: å¼‚å¸¸æ¯”ä¾‹ (Noneåˆ™ä½¿ç”¨ç­–ç•¥é»˜è®¤å€¼)
            optimize_for: ä¼˜åŒ–ç›®æ ‡ (Noneåˆ™ä½¿ç”¨ç­–ç•¥é»˜è®¤å€¼)
            **kwargs: å…¶ä»–å‚æ•°
        """

        # è·å–ç­–ç•¥é»˜è®¤å‚æ•°
        default_params = self.strategy.get_default_params()

        # åˆå¹¶å‚æ•°
        config = {
            'epochs': epochs or default_params.get('epochs', 30),
            'learning_rate': learning_rate or default_params.get('learning_rate', 1e-3),
            'anomaly_ratio': anomaly_ratio or default_params.get('anomaly_ratio', 0.1),
            'optimize_for': optimize_for or default_params.get('optimize_for', 'f1_score'),
            'strategy': self.strategy_name,
            **{k: v for k, v in default_params.items() if k not in ['epochs', 'learning_rate', 'anomaly_ratio', 'optimize_for']},
            **kwargs
        }

        # ä¿å­˜é…ç½®
        with open(os.path.join(self.save_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)

        print(f"\nğŸ“‹ è®­ç»ƒé…ç½® ({self.strategy.get_strategy_name()} Strategy):")
        for key, value in config.items():
            if not key.startswith('_'):
                print(f"  {key}: {value}")

        # æ˜¾ç¤ºä¼˜åŒ–ç›®æ ‡å»ºè®®
        self._print_optimization_advice(config['optimize_for'], self.strategy_name)

        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = self.strategy.create_optimizer(config['learning_rate'])
        scheduler = self.strategy.create_scheduler(optimizer, total_epochs=config['epochs'])

        # è®­ç»ƒå¾ªç¯
        no_improve = 0
        best_score = 0
        start_epoch = config.get('start_epoch', 0)  # æ”¯æŒä»æŒ‡å®šè½®æ¬¡å¼€å§‹

        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ ({self.strategy.get_strategy_name()} Strategy)")
        if start_epoch > 0:
            print(f"ğŸ”„ ä»ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ")
        print(f"ğŸ“Š æ€»è½®æ•°: {config['epochs']}, ä¼˜åŒ–ç›®æ ‡: {config['optimize_for']}")
        print(f"âš™ï¸  å­¦ä¹ ç‡: {config['learning_rate']}, å¼‚å¸¸æ¯”ä¾‹: {config['anomaly_ratio']}")

        # å¯ç”¨è°ƒè¯•æ¨¡å¼
        self._debug_anomaly_density = True

        for epoch in range(start_epoch, config['epochs']):
            print(f"\n{'='*20} Epoch {epoch+1}/{config['epochs']} {'='*20}")

            # è®­ç»ƒä¸€ä¸ªepoch
            print("ğŸ”„ è®­ç»ƒä¸­...")
            train_metrics = self.strategy.train_epoch(
                train_loader, optimizer, scheduler, epoch,
                total_epochs=config['epochs'],
                **config
            )

            # éªŒè¯
            print("ğŸ“Š éªŒè¯ä¸­...")
            self._density_samples = []  # é‡ç½®å¯†åº¦æ ·æœ¬
            val_metrics = self._validate_epoch(val_loader, config['anomaly_ratio'], epoch)

            # è®¡ç®—å¹³å‡å¼‚å¸¸å¯†åº¦
            if hasattr(self, '_density_samples') and self._density_samples:
                avg_density = sum(self._density_samples) / len(self._density_samples)
                val_metrics['actual_anomaly_density'] = avg_density

            # è®¡ç®—é¢„æµ‹ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            pred_stats = None
            if hasattr(self, '_pred_stats') and self._pred_stats:
                pred_stats = {
                    'mean': sum(s['mean'] for s in self._pred_stats) / len(self._pred_stats),
                    'max': max(s['max'] for s in self._pred_stats),
                    'min': min(s['min'] for s in self._pred_stats),
                    'labels_mean': sum(s['labels_mean'] for s in self._pred_stats) / len(self._pred_stats)
                }

            # è®°å½•å†å²
            self.history['train_loss'].append(train_metrics['loss'])
            for k, v in val_metrics.items():
                self.history[f'val_{k}'].append(v)

            # æ‰“å°ç»“æœ
            self._print_epoch_results(epoch + 1, train_metrics, val_metrics, config, pred_stats)

            # è°ƒæ•´å­¦ä¹ ç‡
            if scheduler is not None:
                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(val_metrics[config['optimize_for']])
                else:
                    scheduler.step()

            # è®°å½•å½“å‰å­¦ä¹ ç‡
            self._current_lr = optimizer.param_groups[0]['lr']

            # æ€»æ˜¯ä¿å­˜å½“å‰è½®æ¬¡æ¨¡å‹
            self.save_checkpoint(epoch, optimizer, val_metrics, is_best=False)

            # æ£€æŸ¥æœ€ä½³æ¨¡å‹
            current_score = val_metrics[config['optimize_for']]
            if current_score > best_score:
                best_score = current_score
                self.best_metrics = val_metrics.copy()
                self.best_epoch = epoch + 1
                self.save_checkpoint(epoch, optimizer, val_metrics, is_best=True)
                no_improve = 0
                print(f"  âœ… æ–°çš„æœ€ä½³ {config['optimize_for']}: {current_score:.4f}")
            else:
                no_improve += 1

            # ä¿å­˜ç‰¹æ®Šé‡Œç¨‹ç¢‘æ¨¡å‹
            self._save_milestone_models(epoch, optimizer, val_metrics)

            # æ™ºèƒ½åœæ­¢æ£€æŸ¥
            should_stop, stop_reason = self._check_intelligent_stopping(
                epoch + 1, config['epochs'], val_metrics,
                pred_stats,
                config['optimize_for'], no_improve, self.patience
            )

            if should_stop:
                print(f"\nğŸ›‘ æ™ºèƒ½åœæ­¢è§¦å‘: {stop_reason}")
                break

            # ä¼ ç»Ÿæ—©åœæ£€æŸ¥
            elif no_improve >= self.patience:
                print(f"\nâ¹ï¸  æ—©åœ: {no_improve} è½®æ— æ”¹å–„")
                break

            # æ¿€è¿›ç­–ç•¥çš„ç‰¹æ®Šå¤„ç†
            if self.strategy_name == 'aggressive':
                self._aggressive_emergency_check(epoch, val_metrics, optimizer, config)

        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"  ğŸ† æœ€ä½³è½®æ¬¡: {self.best_epoch}")
        print(f"  ğŸ“ˆ æœ€ä½³ {config['optimize_for']}: {best_score:.4f}")

        return self.best_metrics

    def _validate_epoch(self, val_loader, anomaly_ratio: float, epoch: int = 0) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        self.evaluator.reset()

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                q, s, pid = self._get_batch_data(batch)

                # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„å¼‚å¸¸ç”Ÿæˆæ–¹å¼
                if hasattr(self.strategy.generator, 'generate_anomalies_enhanced'):
                    # Enhanced/Aggressive ç­–ç•¥ä½¿ç”¨å¢å¼ºç”Ÿæˆå™¨
                    s_anomaly, labels = self.strategy.generator.generate_anomalies_enhanced(
                        q, s,
                        anomaly_ratio=anomaly_ratio,
                        min_anomaly_density=0.3 if self.strategy_name == 'enhanced' else 0.4,
                        progressive_difficulty=False,  # éªŒè¯æ—¶ä¸ä½¿ç”¨æ¸è¿›å¼
                        epoch=0
                    )
                else:
                    # Basic ç­–ç•¥ä½¿ç”¨åŸå§‹ç”Ÿæˆå™¨
                    s_anomaly, labels = self.strategy.generator.generate_anomalies(
                        q, s, anomaly_ratio=anomaly_ratio
                    )

                # è°ƒè¯•ï¼šæ£€æŸ¥å¼‚å¸¸å¯†åº¦
                if hasattr(self, '_debug_anomaly_density'):
                    valid_mask = (s >= 0)
                    if valid_mask.sum() > 0:
                        actual_density = labels[valid_mask].float().mean().item()
                        if not hasattr(self, '_density_samples'):
                            self._density_samples = []
                        self._density_samples.append(actual_density)

                # é¢„æµ‹
                logits = self.model(q, s_anomaly, pid)
                predictions = torch.sigmoid(logits)  # è½¬æ¢ä¸ºæ¦‚ç‡

                # æ”¶é›†é¢„æµ‹ç»Ÿè®¡ï¼ˆæ¯è½®éƒ½æ”¶é›†ï¼‰
                if predictions.dim() > 1:
                    pred_probs = predictions.squeeze(-1)
                else:
                    pred_probs = predictions

                # ç»Ÿè®¡é¢„æµ‹åˆ†å¸ƒ
                pred_mean = pred_probs.mean().item()
                pred_max = pred_probs.max().item()
                pred_min = pred_probs.min().item()

                if not hasattr(self, '_pred_stats'):
                    self._pred_stats = []
                self._pred_stats.append({
                    'mean': pred_mean,
                    'max': pred_max,
                    'min': pred_min,
                    'labels_mean': labels.float().mean().item()
                })

                # æ›´æ–°è¯„ä¼°å™¨
                self.evaluator.update(predictions, labels, q, s)

        return self.evaluator.compute_metrics()

    def _print_epoch_results(self, epoch: int, train_metrics: Dict, val_metrics: Dict, config: Dict, pred_stats: Dict = None):
        """æ‰“å°epochç»“æœ"""
        print(f"\nğŸ“Š Epoch {epoch} ç»“æœ:")

        # è®­ç»ƒæŒ‡æ ‡
        train_info = [f"Loss: {train_metrics['loss']:.4f}"]
        if 'positive_ratio' in train_metrics:
            train_info.append(f"æ­£æ ·æœ¬æ¯”ä¾‹: {train_metrics['positive_ratio']:.1%}")
        if 'anomaly_ratio' in train_metrics:
            train_info.append(f"å¼‚å¸¸æ¯”ä¾‹: {train_metrics['anomaly_ratio']:.1%}")

        print(f"  ğŸ“ˆ è®­ç»ƒ - {', '.join(train_info)}")

        # éªŒè¯æŒ‡æ ‡
        print(f"  ğŸ“Š éªŒè¯ - Recall: {val_metrics['recall']:.3f}, "
              f"Precision: {val_metrics['precision']:.3f}, "
              f"F1: {val_metrics['f1_score']:.3f}, "
              f"AUC: {val_metrics['auc_roc']:.3f}")

        # æ˜¾ç¤ºå®é™…å¼‚å¸¸å¯†åº¦
        if 'actual_anomaly_density' in val_metrics:
            print(f"  ğŸ” å®é™…å¼‚å¸¸å¯†åº¦: {val_metrics['actual_anomaly_density']:.1%}")

        # æ¯è½®æ˜¾ç¤ºé¢„æµ‹åˆ†æ
        if pred_stats is not None:
            # åˆ†æé¢„æµ‹è¡Œä¸ºå’Œç»™å‡ºå»ºè®®
            analysis, suggestion = self._analyze_predictions_with_suggestion(pred_stats, val_metrics, epoch, config)
            print(f"  ğŸ“Š é¢„æµ‹åˆ†æ: {analysis}")
            print(f"    - é¢„æµ‹æ¦‚ç‡: å¹³å‡={pred_stats['mean']:.3f}, æœ€å¤§={pred_stats['max']:.3f}, æœ€å°={pred_stats['min']:.3f}")
            print(f"    - æ ‡ç­¾æ¯”ä¾‹: {pred_stats['labels_mean']:.1%}")
            if suggestion:
                print(f"  ğŸ’¡ è®­ç»ƒå»ºè®®: {suggestion}")

        # å½“å‰æœ€ä½³æŒ‡æ ‡
        current_score = val_metrics[config['optimize_for']]
        print(f"  ğŸ¯ å½“å‰ {config['optimize_for']}: {current_score:.4f}")

        # å­¦ä¹ ç‡ï¼ˆä»ä¼˜åŒ–å™¨è·å–å½“å‰å­¦ä¹ ç‡ï¼‰
        if hasattr(self, '_current_lr'):
            print(f"  ğŸ“‰ å­¦ä¹ ç‡: {self._current_lr:.2e}")

    def _analyze_predictions_with_suggestion(self, pred_stats: Dict, val_metrics: Dict,
                                           epoch: int, config: Dict) -> tuple[str, str]:
        """åˆ†æé¢„æµ‹è¡Œä¸ºå¹¶æ ¹æ®è½®æ¬¡ç»™å‡ºè®­ç»ƒå»ºè®®"""
        mean_pred = pred_stats['mean']
        max_pred = pred_stats['max']
        min_pred = pred_stats['min']
        label_ratio = pred_stats['labels_mean']

        recall = val_metrics['recall']
        precision = val_metrics['precision']
        auc = val_metrics['auc_roc']
        f1 = val_metrics['f1_score']

        total_epochs = config['epochs']
        optimize_target = config['optimize_for']

        # åŸºç¡€æ€§èƒ½åˆ†æ
        if auc > 0.90:
            analysis = "ğŸŸ¢ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼ŒåŒºåˆ†èƒ½åŠ›å¼º"
        elif auc > 0.80:
            analysis = "ğŸŸ¡ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œç»§ç»­ä¼˜åŒ–"
        elif auc > 0.70:
            analysis = "ğŸŸ  æ¨¡å‹å­¦ä¹ ä¸­ï¼Œæœ‰æ”¹è¿›ç©ºé—´"
        elif auc > 0.60:
            analysis = "ğŸ”´ æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œéœ€è¦å…³æ³¨"
        else:
            analysis = "ğŸš¨ æ¨¡å‹æ€§èƒ½è¾ƒå·®ï¼Œè€ƒè™‘è°ƒæ•´"

        # æ ¹æ®è½®æ¬¡å’Œæ€§èƒ½ç»™å‡ºå»ºè®®
        suggestion = self._get_training_suggestion(epoch, total_epochs, val_metrics, pred_stats, optimize_target)

        return analysis, suggestion

    def _get_training_suggestion(self, epoch: int, total_epochs: int, val_metrics: Dict,
                               pred_stats: Dict, optimize_target: str) -> str:
        """æ ¹æ®å½“å‰è½®æ¬¡å’Œæ€§èƒ½ç»™å‡ºå…·ä½“çš„è®­ç»ƒå»ºè®®"""
        recall = val_metrics['recall']
        precision = val_metrics['precision']
        auc = val_metrics['auc_roc']
        f1 = val_metrics['f1_score']
        mean_pred = pred_stats['mean']

        progress = epoch / total_epochs

        # æ—©æœŸé˜¶æ®µ (å‰20%)
        if progress <= 0.2:
            if recall < 0.1:
                return "ğŸš¨ å»ºè®®ç«‹å³åœæ­¢ï¼å¬å›ç‡è¿‡ä½ï¼Œè€ƒè™‘ä½¿ç”¨ aggressive ç­–ç•¥æˆ–å¢åŠ å¼‚å¸¸æ¯”ä¾‹"
            elif auc < 0.6:
                return "âš ï¸  æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®è§‚å¯Ÿ2-3è½®ï¼Œå¦‚æ— æ”¹å–„è€ƒè™‘è°ƒæ•´ç­–ç•¥"
            elif recall > 0.3 and auc > 0.75:
                return "âœ… å¼€å±€è‰¯å¥½ï¼Œç»§ç»­è®­ç»ƒ"
            else:
                return "ğŸ“ˆ æ­£å¸¸å­¦ä¹ ä¸­ï¼Œç»§ç»­è§‚å¯Ÿ"

        # ä¸­æœŸé˜¶æ®µ (20%-60%)
        elif progress <= 0.6:
            if optimize_target == 'recall':
                if recall < 0.4:
                    return "ğŸ”„ å¬å›ç‡æå‡ç¼“æ…¢ï¼Œè€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡æˆ–åˆ‡æ¢åˆ° aggressive ç­–ç•¥"
                elif recall > 0.7:
                    return "ğŸ¯ å¬å›ç‡è¡¨ç°ä¼˜ç§€ï¼Œå¯è€ƒè™‘å¹³è¡¡ç²¾ç¡®ç‡"
                else:
                    return "ğŸ“Š å¬å›ç‡ç¨³æ­¥æå‡ï¼Œç»§ç»­å½“å‰ç­–ç•¥"

            elif optimize_target == 'precision':
                if precision < 0.4:
                    return "ğŸ¯ ç²¾ç¡®ç‡è¾ƒä½ï¼Œè€ƒè™‘é™ä½å¼‚å¸¸æ¯”ä¾‹æˆ–è°ƒæ•´é˜ˆå€¼"
                elif precision > 0.7:
                    return "âœ… ç²¾ç¡®ç‡è¡¨ç°ä¼˜ç§€ï¼Œå¯å°è¯•æå‡å¬å›ç‡"
                else:
                    return "ğŸ“Š ç²¾ç¡®ç‡ç¨³æ­¥æå‡ï¼Œç»§ç»­å½“å‰ç­–ç•¥"

            elif optimize_target == 'f1_score':
                if f1 < 0.4:
                    return "âš–ï¸  F1åˆ†æ•°è¾ƒä½ï¼Œéœ€è¦å¹³è¡¡å¬å›ç‡å’Œç²¾ç¡®ç‡"
                elif f1 > 0.6:
                    return "ğŸ¯ F1åˆ†æ•°è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¼˜åŒ–"
                else:
                    return "ğŸ“ˆ F1åˆ†æ•°ç¨³æ­¥æå‡ï¼Œä¿æŒå½“å‰ç­–ç•¥"

            # é€šç”¨å»ºè®®
            if auc > 0.85:
                return "ğŸš€ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå¯è€ƒè™‘æå‰ç»“æŸæˆ–å¾®è°ƒ"
            elif mean_pred < 0.1:
                return "ğŸ”§ æ¨¡å‹è¿‡äºä¿å®ˆï¼Œè€ƒè™‘å¢åŠ æ­£æ ·æœ¬æƒé‡"
            elif mean_pred > 0.8:
                return "ğŸ”§ æ¨¡å‹è¿‡äºæ¿€è¿›ï¼Œè€ƒè™‘é™ä½å¼‚å¸¸æ¯”ä¾‹"
            else:
                return "ğŸ“Š ç»§ç»­è®­ç»ƒï¼Œæ€§èƒ½ç¨³æ­¥æå‡"

        # åæœŸé˜¶æ®µ (60%-100%)
        else:
            if auc > 0.90:
                return "ğŸ† æ€§èƒ½å·²è¾¾åˆ°ä¼˜ç§€æ°´å¹³ï¼Œå»ºè®®æå‰ç»“æŸè®­ç»ƒ"
            elif auc > 0.85:
                return "âœ… æ€§èƒ½è‰¯å¥½ï¼Œå¯ç»§ç»­å¾®è°ƒæˆ–å‡†å¤‡ç»“æŸ"
            elif progress > 0.8 and auc < 0.75:
                return "â° è®­ç»ƒåæœŸæ€§èƒ½ä»ä¸ç†æƒ³ï¼Œå»ºè®®æå‰ç»“æŸå¹¶è°ƒæ•´ç­–ç•¥"
            elif optimize_target == 'recall' and recall < 0.6:
                return "ğŸ¯ å¬å›ç‡ä»éœ€æå‡ï¼Œè€ƒè™‘å»¶é•¿è®­ç»ƒæˆ–è°ƒæ•´ç­–ç•¥"
            elif optimize_target == 'precision' and precision < 0.6:
                return "ğŸ¯ ç²¾ç¡®ç‡ä»éœ€æå‡ï¼Œè€ƒè™‘è°ƒæ•´é˜ˆå€¼æˆ–ç­–ç•¥"
            else:
                return "ğŸ æ¥è¿‘è®­ç»ƒç»“æŸï¼Œå‡†å¤‡è¯„ä¼°æœ€ç»ˆæ€§èƒ½"

    def _check_intelligent_stopping(self, epoch: int, total_epochs: int, val_metrics: Dict,
                                   pred_stats: Dict, optimize_target: str, no_improve: int,
                                   patience: int) -> tuple[bool, str]:
        """æ™ºèƒ½åœæ­¢æ£€æŸ¥"""
        recall = val_metrics['recall']
        precision = val_metrics['precision']
        auc = val_metrics['auc_roc']
        f1 = val_metrics['f1_score']

        progress = epoch / total_epochs

        # 1. æ—©æœŸæ€§èƒ½è¿‡å·®ï¼Œç«‹å³åœæ­¢
        if epoch <= 5:
            if recall < 0.05 and auc < 0.55:
                return True, "æ—©æœŸæ€§èƒ½è¿‡å·®ï¼Œå»ºè®®è°ƒæ•´ç­–ç•¥é‡æ–°è®­ç»ƒ"

        # 2. æ€§èƒ½å·²è¾¾åˆ°ä¼˜ç§€æ°´å¹³ï¼Œæå‰ç»“æŸ
        if auc > 0.92 and epoch >= 10:
            return True, f"æ€§èƒ½å·²è¾¾åˆ°ä¼˜ç§€æ°´å¹³ (AUC: {auc:.3f})ï¼Œæå‰ç»“æŸ"

        # 3. é’ˆå¯¹ä¼˜åŒ–ç›®æ ‡çš„ç‰¹æ®Šåœæ­¢æ¡ä»¶
        if optimize_target == 'recall':
            if recall > 0.85 and precision > 0.4:
                return True, f"å¬å›ç‡å·²è¾¾åˆ°ä¼˜ç§€æ°´å¹³ ({recall:.3f})ï¼Œå»ºè®®ç»“æŸ"
        elif optimize_target == 'precision':
            if precision > 0.85 and recall > 0.4:
                return True, f"ç²¾ç¡®ç‡å·²è¾¾åˆ°ä¼˜ç§€æ°´å¹³ ({precision:.3f})ï¼Œå»ºè®®ç»“æŸ"
        elif optimize_target == 'f1_score':
            if f1 > 0.80:
                return True, f"F1åˆ†æ•°å·²è¾¾åˆ°ä¼˜ç§€æ°´å¹³ ({f1:.3f})ï¼Œå»ºè®®ç»“æŸ"

        # 4. åæœŸæ€§èƒ½ä¸ä½³ï¼Œæå‰ç»“æŸ
        if progress > 0.7 and auc < 0.70:
            return True, "è®­ç»ƒåæœŸæ€§èƒ½ä»ä¸ç†æƒ³ï¼Œå»ºè®®è°ƒæ•´ç­–ç•¥"

        # 5. å­¦ä¹ ç‡è¿‡ä½ï¼Œæ— æ³•ç»§ç»­æ”¹å–„
        if hasattr(self, '_current_lr') and self._current_lr < 1e-6:
            return True, "å­¦ä¹ ç‡è¿‡ä½ï¼Œæ¨¡å‹æ— æ³•ç»§ç»­æ”¹å–„"

        # 6. é¢„æµ‹è¡Œä¸ºå¼‚å¸¸
        if pred_stats:
            mean_pred = pred_stats['mean']
            if mean_pred < 0.01 or mean_pred > 0.99:
                return True, "é¢„æµ‹è¡Œä¸ºå¼‚å¸¸ï¼Œæ¨¡å‹å¯èƒ½å´©æºƒ"

        # 7. é•¿æœŸæ— æ”¹å–„ä¸”æ¥è¿‘ç»“æŸ
        if no_improve >= patience // 2 and progress > 0.8:
            return True, f"é•¿æœŸæ— æ”¹å–„ ({no_improve}è½®) ä¸”æ¥è¿‘è®­ç»ƒç»“æŸ"

        return False, ""

    def _save_milestone_models(self, epoch: int, optimizer, val_metrics: Dict):
        """ä¿å­˜é‡Œç¨‹ç¢‘æ¨¡å‹"""
        from datetime import datetime

        auc = val_metrics.get('auc_roc', 0)
        recall = val_metrics.get('recall', 0)
        f1 = val_metrics.get('f1_score', 0)
        precision = val_metrics.get('precision', 0)

        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'metrics': val_metrics,
            'timestamp': datetime.now().isoformat()
        }

        # ä¿å­˜AUC > 0.93çš„æ¨¡å‹
        if auc > 0.93:
            if not hasattr(self, '_saved_auc_milestones'):
                self._saved_auc_milestones = set()

            auc_key = f"{auc:.3f}"
            if auc_key not in self._saved_auc_milestones:
                milestone_path = os.path.join(self.save_dir, f'milestone_auc_{auc:.3f}_epoch_{epoch+1}.pt')
                try:
                    torch.save(checkpoint, milestone_path)
                    print(f"  ğŸ¯ é‡Œç¨‹ç¢‘æ¨¡å‹: AUC {auc:.3f} (å·²ä¿å­˜)")
                    self._saved_auc_milestones.add(auc_key)
                except Exception as e:
                    print(f"  âŒ ä¿å­˜é‡Œç¨‹ç¢‘æ¨¡å‹å¤±è´¥: {e}")

        # ä¿å­˜å¬å›ç‡ > 0.6çš„æ¨¡å‹
        if recall > 0.6:
            if not hasattr(self, '_saved_recall_milestones'):
                self._saved_recall_milestones = set()

            recall_key = f"{recall:.3f}"
            if recall_key not in self._saved_recall_milestones:
                recall_path = os.path.join(self.save_dir, f'high_recall_{recall:.3f}_epoch_{epoch+1}.pt')
                try:
                    torch.save(checkpoint, recall_path)
                    print(f"  ğŸ“ˆ é«˜å¬å›æ¨¡å‹: Recall {recall:.3f} (å·²ä¿å­˜)")
                    self._saved_recall_milestones.add(recall_key)
                except Exception as e:
                    print(f"  âŒ ä¿å­˜é«˜å¬å›æ¨¡å‹å¤±è´¥: {e}")

        # ä¿å­˜F1 > 0.55çš„æ¨¡å‹
        if f1 > 0.55:
            if not hasattr(self, '_saved_f1_milestones'):
                self._saved_f1_milestones = set()

            f1_key = f"{f1:.3f}"
            if f1_key not in self._saved_f1_milestones:
                f1_path = os.path.join(self.save_dir, f'high_f1_{f1:.3f}_epoch_{epoch+1}.pt')
                try:
                    torch.save(checkpoint, f1_path)
                    print(f"  âš–ï¸  é«˜F1æ¨¡å‹: F1 {f1:.3f} (å·²ä¿å­˜)")
                    self._saved_f1_milestones.add(f1_key)
                except Exception as e:
                    print(f"  âŒ ä¿å­˜é«˜F1æ¨¡å‹å¤±è´¥: {e}")

        print("-" * 60)

    def _aggressive_emergency_check(self, epoch: int, val_metrics: Dict, optimizer, config: Dict):
        """æ¿€è¿›ç­–ç•¥çš„ç´§æ€¥æ£€æŸ¥"""
        if epoch > 5 and val_metrics['recall'] < 0.3:
            print("\nğŸš¨ æ‰§è¡Œç´§æ€¥æªæ–½:")

            # é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´
            if hasattr(self.model, 'classifier'):
                for layer in self.model.classifier:
                    if isinstance(layer, nn.Linear):
                        nn.init.xavier_uniform_(layer.weight)
                        if layer.bias is not None:
                            nn.init.constant_(layer.bias, 0.1)
                print("    - é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´")

            # æå‡å­¦ä¹ ç‡
            new_lr = config['learning_rate'] * 3
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lr
            print(f"    - å­¦ä¹ ç‡æå‡åˆ°: {new_lr:.2e}")

            # å‡å°‘æ­£åˆ™åŒ–
            for param_group in optimizer.param_groups:
                param_group['weight_decay'] = 1e-6
            print("    - å‡å°‘æƒé‡è¡°å‡")

    def _get_batch_data(self, batch):
        """è·å–æ‰¹æ¬¡æ•°æ®"""
        if len(batch.data) == 2:
            q, s = batch.get("q", "s")
            pid = None
        else:
            q, s, pid = batch.get("q", "s", "pid")

        q = q.to(self.device)
        s = s.to(self.device)
        if pid is not None:
            pid = pid.to(self.device)

        return q, s, pid

    def get_strategy_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç­–ç•¥ä¿¡æ¯"""
        return {
            'name': self.strategy.get_strategy_name(),
            'default_params': self.strategy.get_default_params(),
            'description': self._get_strategy_description()
        }

    def _get_strategy_description(self) -> str:
        """è·å–ç­–ç•¥æè¿°"""
        descriptions = {
            'basic': 'æ ‡å‡†è®­ç»ƒç­–ç•¥ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯',
            'enhanced': 'å¢å¼ºè®­ç»ƒç­–ç•¥ï¼ŒåŒ…å«Focal Lossã€æ¸è¿›å¼è®­ç»ƒç­‰é«˜çº§æŠ€æœ¯',
            'improved': 'å¢å¼ºè®­ç»ƒç­–ç•¥ï¼ŒåŒ…å«Focal Lossã€æ¸è¿›å¼è®­ç»ƒç­‰é«˜çº§æŠ€æœ¯',  # å‘åå…¼å®¹
            'aggressive': 'æ¿€è¿›è®­ç»ƒç­–ç•¥ï¼Œä¸“é—¨å¤„ç†ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜'
        }
        return descriptions.get(self.strategy_name, 'æœªçŸ¥ç­–ç•¥')

    @staticmethod
    def list_available_strategies() -> Dict[str, str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç­–ç•¥"""
        return {
            'basic': 'åŸºç¡€ç­–ç•¥ - æ ‡å‡†è®­ç»ƒï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯',
            'enhanced': 'å¢å¼ºç­–ç•¥ - é«˜çº§æŠ€æœ¯ï¼Œæ›´å¥½çš„æ€§èƒ½',
            'aggressive': 'æ¿€è¿›ç­–ç•¥ - å¤„ç†ä¸¥é‡ä¸å¹³è¡¡ï¼Œé«˜å¬å›ç‡'
        }

    @staticmethod
    def get_strategy_recommendations(data_balance: str = 'balanced') -> str:
        """è·å–ç­–ç•¥æ¨è"""
        recommendations = {
            'balanced': 'basic',
            'slightly_imbalanced': 'enhanced',
            'severely_imbalanced': 'aggressive'
        }
        return recommendations.get(data_balance, 'basic')

    def _print_optimization_advice(self, optimize_for: str, strategy: str):
        """æ‰“å°ä¼˜åŒ–ç›®æ ‡å»ºè®®"""
        advice = {
            'recall': {
                'description': 'ğŸ¯ å¬å›ç‡ä¼˜åŒ– - ä¸“æ³¨äºæ‰¾åˆ°æ‰€æœ‰å¼‚å¸¸',
                'suitable': 'é«˜é£é™©åœºæ™¯ï¼šåŒ»ç–—è¯Šæ–­ã€å®‰å…¨ç›‘æ§ã€å­¦æœ¯è¯šä¿¡',
                'trade_off': 'å¯èƒ½äº§ç”Ÿè¾ƒå¤šè¯¯æŠ¥ï¼Œä½†ä¸ä¼šæ¼æ‰é‡è¦å¼‚å¸¸',
                'best_strategy': 'aggressive'
            },
            'precision': {
                'description': 'ğŸ¯ ç²¾ç¡®ç‡ä¼˜åŒ– - ä¸“æ³¨äºé¿å…è¯¯æŠ¥',
                'suitable': 'é«˜æˆæœ¬åœºæ™¯ï¼šé‡‘èåæ¬ºè¯ˆã€åƒåœ¾é‚®ä»¶ã€æ¨èç³»ç»Ÿ',
                'trade_off': 'å‡å°‘è¯¯æŠ¥ï¼Œä½†å¯èƒ½æ¼æ‰ä¸€äº›å¼‚å¸¸',
                'best_strategy': 'enhanced'
            },
            'f1_score': {
                'description': 'ğŸ¯ F1å€¼ä¼˜åŒ– - å¹³è¡¡æŸ¥å…¨å’ŒæŸ¥å‡†',
                'suitable': 'é€šç”¨åœºæ™¯ï¼šä¸šåŠ¡ç›‘æ§ã€å†…å®¹å®¡æ ¸ã€æµé‡åˆ†æ',
                'trade_off': 'åœ¨å¬å›ç‡å’Œç²¾ç¡®ç‡ä¹‹é—´å¯»æ±‚å¹³è¡¡',
                'best_strategy': 'basic'
            },
            'auc_roc': {
                'description': 'ğŸ¯ AUCä¼˜åŒ– - æ•´ä½“åˆ†ç±»æ€§èƒ½',
                'suitable': 'æ¨¡å‹æ¯”è¾ƒï¼šç ”ç©¶ã€åŸºå‡†æµ‹è¯•ã€ç®—æ³•éªŒè¯',
                'trade_off': 'è¯„ä¼°æ¨¡å‹åŒºåˆ†èƒ½åŠ›ï¼Œé€‚åˆæ¨¡å‹é€‰æ‹©',
                'best_strategy': 'enhanced'
            }
        }

        if optimize_for in advice:
            info = advice[optimize_for]
            print(f"\nğŸ’¡ ä¼˜åŒ–ç›®æ ‡åˆ†æ:")
            print(f"  {info['description']}")
            print(f"  ğŸ“‹ é€‚ç”¨åœºæ™¯: {info['suitable']}")
            print(f"  âš–ï¸  æƒè¡¡è€ƒè™‘: {info['trade_off']}")

            if strategy != info['best_strategy']:
                print(f"  ğŸ’­ å»ºè®®: è€ƒè™‘ä½¿ç”¨ '{info['best_strategy']}' ç­–ç•¥ä»¥è·å¾—æ›´å¥½çš„ {optimize_for} è¡¨ç°")
            else:
                print(f"  âœ… ç­–ç•¥åŒ¹é…: '{strategy}' ç­–ç•¥å¾ˆé€‚åˆä¼˜åŒ– {optimize_for}")

        print()
