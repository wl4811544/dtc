#!/usr/bin/env python
"""
ä¼˜åŒ–çš„å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒè„šæœ¬
é’ˆå¯¹Recallå’Œæ£€æµ‹æ€§èƒ½è¿›è¡Œä¸“é—¨ä¼˜åŒ–
"""

import argparse
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import tomlkit
import numpy as np
import random
from tqdm import tqdm
from collections import defaultdict
from typing import Dict, Optional, Tuple
import matplotlib.pyplot as plt

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from DTransformer.data import KTData
from anomaly_kt.generator import AnomalyGenerator
from anomaly_kt.detector import CausalAnomalyDetector
from anomaly_kt.trainer import BaseTrainer
from anomaly_kt.evaluator import AnomalyEvaluator


class EnhancedAnomalyTrainer(BaseTrainer):
    """å¢å¼ºçš„å¼‚å¸¸æ£€æµ‹è®­ç»ƒå™¨ - ä¸“é—¨ä¼˜åŒ–æ£€æµ‹æ€§èƒ½"""
    
    def __init__(self, model: nn.Module, device: str = 'cpu', 
                 save_dir: str = 'output/enhanced_detector', patience: int = 15):
        super().__init__(model, device, save_dir, patience)
        self.generator = AnomalyGenerator()
        self.evaluator = AnomalyEvaluator()
        
        # å¢å¼ºé…ç½®
        self.class_weights = None
        self.best_threshold = 0.5
        self.focal_alpha = 0.25
        self.focal_gamma = 2.0
        
    def train(self, 
              train_loader, 
              val_loader,
              epochs: int = 50,
              learning_rate: float = 5e-4,
              anomaly_ratio: float = 0.3,
              optimize_for: str = 'recall',
              use_focal_loss: bool = True,
              use_class_weights: bool = True,
              use_progressive_training: bool = True,
              gradient_accumulation_steps: int = 2,
              warmup_epochs: int = 5) -> Dict:
        """
        å¢å¼ºçš„è®­ç»ƒæ–¹æ³•
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            anomaly_ratio: å¼‚å¸¸ç”Ÿæˆæ¯”ä¾‹
            optimize_for: ä¼˜åŒ–ç›®æ ‡ ('recall', 'f1', 'precision', 'auc')
            use_focal_loss: ä½¿ç”¨Focal Loss
            use_class_weights: ä½¿ç”¨ç±»åˆ«æƒé‡
            use_progressive_training: ä½¿ç”¨æ¸è¿›å¼è®­ç»ƒ
            gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            warmup_epochs: é¢„çƒ­è½®æ•°
        """
        
        print(f"ğŸš€ å¼€å§‹å¢å¼ºå¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒ")
        print(f"  ğŸ“Š ä¼˜åŒ–ç›®æ ‡: {optimize_for}")
        print(f"  ğŸ¯ å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio}")
        print(f"  ğŸ”¥ Focal Loss: {use_focal_loss}")
        print(f"  âš–ï¸  ç±»åˆ«æƒé‡: {use_class_weights}")
        print(f"  ğŸ“ˆ æ¸è¿›è®­ç»ƒ: {use_progressive_training}")
        
        # 1. è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer = self._setup_optimizer(learning_rate, warmup_epochs, epochs)
        scheduler = self._setup_scheduler(optimizer, epochs)
        
        # 2. ä¿å­˜é…ç½®
        self._save_config({
            'epochs': epochs,
            'learning_rate': learning_rate,
            'anomaly_ratio': anomaly_ratio,
            'optimize_for': optimize_for,
            'use_focal_loss': use_focal_loss,
            'use_class_weights': use_class_weights,
            'use_progressive_training': use_progressive_training
        })
        
        # 3. è®­ç»ƒå¾ªç¯
        best_score = 0
        no_improve = 0
        
        for epoch in range(epochs):
            print(f"\n{'='*20} Epoch {epoch+1}/{epochs} {'='*20}")
            
            # åŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°
            current_anomaly_ratio = self._get_dynamic_anomaly_ratio(
                epoch, epochs, anomaly_ratio, use_progressive_training
            )
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self._train_epoch_enhanced(
                train_loader, optimizer, current_anomaly_ratio,
                use_focal_loss, use_class_weights, gradient_accumulation_steps
            )
            
            # éªŒè¯
            val_metrics = self._validate_epoch_enhanced(
                val_loader, current_anomaly_ratio, optimize_for
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(val_metrics[optimize_for])
            else:
                scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            self._record_metrics(epoch, train_metrics, val_metrics, optimizer)
            
            # æ‰“å°ç»“æœ
            self._print_epoch_results(epoch, train_metrics, val_metrics, current_anomaly_ratio)
            
            # æ£€æŸ¥æœ€ä½³æ¨¡å‹
            current_score = val_metrics[optimize_for]
            if current_score > best_score:
                best_score = current_score
                self.best_metrics = val_metrics.copy()
                self.best_epoch = epoch + 1
                self.best_threshold = val_metrics.get('best_threshold', 0.5)
                self.save_checkpoint(epoch, optimizer, val_metrics, is_best=True)
                no_improve = 0
                print(f"  âœ… æ–°çš„æœ€ä½³{optimize_for}: {current_score:.4f}")
            else:
                no_improve += 1
                print(f"  ğŸ“Š å½“å‰{optimize_for}: {current_score:.4f} (æœ€ä½³: {best_score:.4f})")
            
            # æ—©åœæ£€æŸ¥
            if no_improve >= self.patience:
                print(f"\nâ¹ï¸  æ—©åœè§¦å‘ - {self.patience}è½®æ— æ”¹å–„")
                break
        
        # è®­ç»ƒå®Œæˆ
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"  ğŸ† æœ€ä½³è½®æ¬¡: {self.best_epoch}")
        print(f"  ğŸ“ˆ æœ€ä½³{optimize_for}: {best_score:.4f}")
        print(f"  ğŸ¯ æœ€ä½³é˜ˆå€¼: {self.best_threshold:.3f}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves()
        
        return self.best_metrics
    
    def _setup_optimizer(self, learning_rate: float, warmup_epochs: int, total_epochs: int):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œæ›´å¥½çš„æƒé‡è¡°å‡
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-4,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        return optimizer
    
    def _setup_scheduler(self, optimizer, total_epochs: int):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=total_epochs, 
            eta_min=1e-6
        )
        return scheduler
    
    def _get_dynamic_anomaly_ratio(self, epoch: int, total_epochs: int, 
                                  base_ratio: float, use_progressive: bool) -> float:
        """åŠ¨æ€è°ƒæ•´å¼‚å¸¸ç”Ÿæˆæ¯”ä¾‹"""
        if not use_progressive:
            return base_ratio
        
        # å‰æœŸç”Ÿæˆæ›´å¤šå¼‚å¸¸ï¼ŒåæœŸé€æ¸å‡å°‘åˆ°æ­£å¸¸æ¯”ä¾‹
        progress = epoch / total_epochs
        if progress < 0.3:  # å‰30%è½®æ¬¡
            return min(base_ratio * 2.0, 0.5)  # æœ€å¤š50%
        elif progress < 0.7:  # ä¸­é—´40%è½®æ¬¡
            return base_ratio * 1.5
        else:  # å30%è½®æ¬¡
            return base_ratio
    
    def _train_epoch_enhanced(self, train_loader, optimizer, anomaly_ratio: float,
                             use_focal_loss: bool, use_class_weights: bool,
                             gradient_accumulation_steps: int) -> Dict:
        """å¢å¼ºçš„è®­ç»ƒepoch"""
        self.model.train()
        
        total_loss = 0
        total_samples = 0
        step_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        pos_samples = 0
        neg_samples = 0
        
        optimizer.zero_grad()
        
        progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­", leave=False)
        for batch_idx, batch in enumerate(progress_bar):
            q, s, pid = self._get_batch_data(batch)
            
            # ç”Ÿæˆå¹³è¡¡çš„å¼‚å¸¸æ•°æ®
            s_anomaly, labels = self._generate_balanced_anomalies(q, s, anomaly_ratio)
            labels = labels.to(self.device)
            
            # ç»Ÿè®¡æ ·æœ¬
            pos_samples += (labels == 1).sum().item()
            neg_samples += (labels == 0).sum().item()
            
            # å‰å‘ä¼ æ’­
            if use_focal_loss:
                loss = self._compute_focal_loss(q, s_anomaly, labels, pid)
            else:
                loss = self._compute_weighted_loss(q, s_anomaly, labels, pid, use_class_weights)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / gradient_accumulation_steps
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                step_count += 1
            
            total_loss += loss.item() * gradient_accumulation_steps
            total_samples += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            if batch_idx % 10 == 0:
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Pos%': f'{pos_samples/(pos_samples+neg_samples)*100:.1f}%'
                })
        
        # å¤„ç†å‰©ä½™çš„æ¢¯åº¦
        if total_samples % gradient_accumulation_steps != 0:
            optimizer.step()
            optimizer.zero_grad()
        
        avg_loss = total_loss / total_samples
        pos_ratio = pos_samples / (pos_samples + neg_samples) if (pos_samples + neg_samples) > 0 else 0
        
        return {
            'loss': avg_loss,
            'positive_ratio': pos_ratio,
            'total_samples': pos_samples + neg_samples
        }
    
    def _validate_epoch_enhanced(self, val_loader, anomaly_ratio: float, 
                                optimize_for: str) -> Dict:
        """å¢å¼ºçš„éªŒè¯epoch"""
        self.model.eval()
        self.evaluator.reset()
        
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="éªŒè¯ä¸­", leave=False):
                q, s, pid = self._get_batch_data(batch)
                
                # ç”Ÿæˆå¼‚å¸¸
                s_anomaly, labels = self._generate_balanced_anomalies(q, s, anomaly_ratio)
                
                # é¢„æµ‹
                predictions = self.model(q, s_anomaly, pid)
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                mask = (s >= 0)
                all_predictions.extend(predictions[mask].cpu().numpy())
                all_labels.extend(labels[mask].cpu().numpy())
                
                # æ›´æ–°è¯„ä¼°å™¨
                self.evaluator.update(predictions, labels, q, s)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.evaluator.compute_metrics()
        
        # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
        best_threshold = self._find_optimal_threshold(
            np.array(all_predictions), np.array(all_labels), optimize_for
        )
        metrics['best_threshold'] = best_threshold
        
        return metrics
    
    def _generate_balanced_anomalies(self, q: torch.Tensor, s: torch.Tensor, 
                                   anomaly_ratio: float) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ›´å¹³è¡¡çš„å¼‚å¸¸æ•°æ®"""
        # ä½¿ç”¨æ”¹è¿›çš„ç­–ç•¥æƒé‡
        strategy_weights = {
            'consecutive': 0.25,    # è¿ç»­å¼‚å¸¸
            'pattern': 0.25,        # æ¨¡å¼å¼‚å¸¸  
            'random_burst': 0.25,   # éšæœºçªå‘
            'difficulty_based': 0.25 # åŸºäºéš¾åº¦
        }
        
        return self.generator.generate_anomalies(
            q, s, anomaly_ratio=anomaly_ratio, strategy_weights=strategy_weights
        )
    
    def _compute_focal_loss(self, q: torch.Tensor, s: torch.Tensor, 
                           labels: torch.Tensor, pid: Optional[torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—Focal Loss"""
        # è·å–åŸå§‹logits
        predictions = self.model(q, s, pid)
        
        # åˆ›å»ºæœ‰æ•ˆæ ·æœ¬æ©ç 
        mask = (s >= 0)
        valid_preds = predictions[mask]
        valid_labels = labels[mask].float()
        
        if len(valid_preds) == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è®¡ç®—BCEæŸå¤±ï¼ˆä½¿ç”¨logitsç‰ˆæœ¬ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰
        # å…ˆè½¬æ¢sigmoidè¾“å‡ºå›logits
        epsilon = 1e-7
        valid_preds_clipped = torch.clamp(valid_preds, epsilon, 1 - epsilon)
        logits = torch.log(valid_preds_clipped / (1 - valid_preds_clipped))
        
        # BCE loss
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, valid_labels, reduction='none'
        )
        
        # è®¡ç®—pt
        pt = torch.exp(-bce_loss)
        
        # Alphaæƒé‡
        alpha_weight = torch.where(valid_labels == 1, self.focal_alpha, 1 - self.focal_alpha)
        
        # Focalæƒé‡
        focal_weight = alpha_weight * (1 - pt) ** self.focal_gamma
        
        # æœ€ç»ˆæŸå¤±
        focal_loss = focal_weight * bce_loss
        
        return focal_loss.mean()
    
    def _compute_weighted_loss(self, q: torch.Tensor, s: torch.Tensor,
                              labels: torch.Tensor, pid: Optional[torch.Tensor],
                              use_class_weights: bool) -> torch.Tensor:
        """è®¡ç®—åŠ æƒæŸå¤±"""
        predictions = self.model(q, s, pid)
        
        mask = (s >= 0)
        valid_preds = predictions[mask]
        valid_labels = labels[mask].float()
        
        if len(valid_preds) == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        if use_class_weights:
            pos_count = valid_labels.sum()
            neg_count = len(valid_labels) - pos_count
            
            if pos_count > 0:
                pos_weight = neg_count / pos_count
                pos_weight = torch.clamp(pos_weight, min=1.0, max=10.0)
            else:
                pos_weight = torch.tensor(5.0, device=self.device)
        else:
            pos_weight = torch.tensor(1.0, device=self.device)
        
        # ä½¿ç”¨BCEWithLogitsLoss
        epsilon = 1e-7
        valid_preds_clipped = torch.clamp(valid_preds, epsilon, 1 - epsilon)
        logits = torch.log(valid_preds_clipped / (1 - valid_preds_clipped))
        
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        loss = criterion(logits, valid_labels)
        
        return loss
    
    def _find_optimal_threshold(self, predictions: np.ndarray, labels: np.ndarray, 
                               metric: str = 'f1') -> float:
        """å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
        best_score = 0
        best_threshold = 0.5
        
        # å°è¯•æ›´å¤šé˜ˆå€¼
        thresholds = np.linspace(0.1, 0.9, 33)  # 33ä¸ªé˜ˆå€¼ç‚¹
        
        for threshold in thresholds:
            pred_binary = (predictions >= threshold).astype(int)
            
            # é¿å…é™¤é›¶é”™è¯¯
            tp = ((pred_binary == 1) & (labels == 1)).sum()
            fp = ((pred_binary == 1) & (labels == 0)).sum()
            fn = ((pred_binary == 0) & (labels == 1)).sum()
            
            if metric == 'recall':
                score = tp / (tp + fn) if (tp + fn) > 0 else 0
            elif metric == 'precision':
                score = tp / (tp + fp) if (tp + fp) > 0 else 0
            elif metric == 'f1':
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            else:
                score = 0
            
            if score > best_score:
                best_score = score
                best_threshold = threshold
        
        return best_threshold
    
    def _record_metrics(self, epoch: int, train_metrics: Dict, val_metrics: Dict, optimizer):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        for key, value in train_metrics.items():
            self.history[f'train_{key}'].append(value)
        
        # è®°å½•éªŒè¯æŒ‡æ ‡  
        for key, value in val_metrics.items():
            self.history[f'val_{key}'].append(value)
        
        # è®°å½•å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        self.history['learning_rate'].append(current_lr)
    
    def _print_epoch_results(self, epoch: int, train_metrics: Dict, val_metrics: Dict, anomaly_ratio: float):
        """æ‰“å°epochç»“æœ"""
        print(f"  ğŸ“Š è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, "
              f"å¼‚å¸¸æ¯”ä¾‹: {train_metrics['positive_ratio']:.1%}")
        print(f"  ğŸ“ˆ éªŒè¯ - Recall: {val_metrics['recall']:.3f}, "
              f"Precision: {val_metrics['precision']:.3f}, "
              f"F1: {val_metrics['f1_score']:.3f}, "
              f"AUC: {val_metrics['auc_roc']:.3f}")
        print(f"  ğŸ¯ æœ€ä½³é˜ˆå€¼: {val_metrics['best_threshold']:.3f}, "
              f"å½“å‰å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio:.1%}")
    
    def _plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.history:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Lossæ›²çº¿
        if 'train_loss' in self.history:
            axes[0, 0].plot(self.history['train_loss'], label='Train Loss', alpha=0.8)
            axes[0, 0].set_title('Training Loss')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if 'learning_rate' in self.history:
            axes[0, 1].plot(self.history['learning_rate'], color='orange', alpha=0.8)
            axes[0, 1].set_title('Learning Rate')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Learning Rate')
            axes[0, 1].set_yscale('log')
            axes[0, 1].grid(True, alpha=0.3)
        
        # éªŒè¯æŒ‡æ ‡
        if 'val_recall' in self.history:
            axes[1, 0].plot(self.history['val_recall'], label='Recall', marker='o', alpha=0.8)
            axes[1, 0].plot(self.history['val_precision'], label='Precision', marker='s', alpha=0.8)
            axes[1, 0].plot(self.history['val_f1_score'], label='F1 Score', marker='^', alpha=0.8)
            axes[1, 0].set_title('Validation Metrics')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # AUCæ›²çº¿
        if 'val_auc_roc' in self.history:
            axes[1, 1].plot(self.history['val_auc_roc'], color='green', marker='o', alpha=0.8)
            axes[1, 1].set_title('AUC-ROC Score')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('AUC')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, 'enhanced_training_curves.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {self.save_dir}/enhanced_training_curves.png")
    
    def _get_batch_data(self, batch):
        """è·å–æ‰¹æ¬¡æ•°æ®"""
        if len(batch.data) == 2:
            q, s = batch.get("q", "s")
            pid = None
        else:
            q, s, pid = batch.get("q", "s", "pid")
        
        q = q.to(self.device)
        s = s.to(self.device)
        if pid is not None:
            pid = pid.to(self.device)
        
        return q, s, pid


def train_enhanced_detector(args):
    """è®­ç»ƒå¢å¼ºçš„å¼‚å¸¸æ£€æµ‹å™¨"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºå¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒ")
    
    # åŠ è½½æ•°æ®é…ç½®
    datasets = tomlkit.load(open(os.path.join(args.data_dir, 'datasets.toml')))
    dataset_config = datasets[args.dataset]
    
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset}")
    print(f"  é—®é¢˜æ•°: {dataset_config['n_questions']}")
    print(f"  çŸ¥è¯†ç‚¹æ•°: {dataset_config['n_pid']}")
    
    # å‡†å¤‡æ•°æ®
    train_data = KTData(
        os.path.join(args.data_dir, dataset_config['train']),
        dataset_config['inputs'],
        batch_size=args.batch_size,
        shuffle=True
    )
    
    val_data = KTData(
        os.path.join(args.data_dir, dataset_config.get('valid', dataset_config['test'])),
        dataset_config['inputs'],
        batch_size=args.batch_size
    )
    
    print(f"ğŸ“ è®­ç»ƒæ‰¹æ¬¡: {len(train_data)}, éªŒè¯æ‰¹æ¬¡: {len(val_data)}")
    
    # åˆ›å»ºå¢å¼ºçš„æ£€æµ‹å™¨
    detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        dropout=args.dropout,
        window_size=args.window_size
    )
    
    print(f"ğŸ§  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in detector.parameters()):,}")
    
    # ä½¿ç”¨å¢å¼ºçš„è®­ç»ƒå™¨
    trainer = EnhancedAnomalyTrainer(
        model=detector,
        device=args.device,
        save_dir=args.save_dir,
        patience=args.patience
    )
    
    # è®­ç»ƒ
    best_metrics = trainer.train(
        train_loader=train_data,
        val_loader=val_data,
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        anomaly_ratio=args.anomaly_ratio,
        optimize_for=args.optimize_for,
        use_focal_loss=args.use_focal_loss,
        use_class_weights=args.use_class_weights,
        use_progressive_training=args.use_progressive_training,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_epochs=args.warmup_epochs
    )
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("ğŸ“Š æœ€ç»ˆç»“æœ:")
    for metric, value in best_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    return best_metrics


def main():
    parser = argparse.ArgumentParser(description='Enhanced Anomaly Detector Training')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('-d', '--dataset', required=True, 
                        choices=['assist09', 'assist17', 'algebra05', 'statics'])
    parser.add_argument('--data_dir', default='data')
    parser.add_argument('-p', '--with_pid', action='store_true')
    parser.add_argument('--batch_size', type=int, default=32)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128)
    parser.add_argument('--n_heads', type=int, default=8)
    parser.add_argument('--n_layers', type=int, default=2)
    parser.add_argument('--dropout', type=float, default=0.1)
    parser.add_argument('--window_size', type=int, default=15)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--learning_rate', type=float, default=5e-4)
    parser.add_argument('--anomaly_ratio', type=float, default=0.3)
    parser.add_argument('--optimize_for', default='recall', 
                        choices=['recall', 'precision', 'f1', 'auc_roc'])
    parser.add_argument('--patience', type=int, default=15)
    
    # å¢å¼ºç­–ç•¥
    parser.add_argument('--use_focal_loss', action='store_true', 
                        help='ä½¿ç”¨Focal Losså¤„ç†ç±»åˆ«ä¸å¹³è¡¡')
    parser.add_argument('--use_class_weights', action='store_true',
                        help='ä½¿ç”¨ç±»åˆ«æƒé‡')
    parser.add_argument('--use_progressive_training', action='store_true',
                        help='ä½¿ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=2,
                        help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--warmup_epochs', type=int, default=5,
                        help='å­¦ä¹ ç‡é¢„çƒ­è½®æ•°')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--save_dir', default='output/enhanced_detector')
    
    args = parser.parse_args()
    
    print("âš™ï¸  é…ç½®å‚æ•°:")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
    
    # è®­ç»ƒ
    try:
        metrics = train_enhanced_detector(args)
        print(f"\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())