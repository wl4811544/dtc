#!/usr/bin/env python
"""
å•ç‹¬è¿è¡Œç¬¬äºŒé˜¶æ®µï¼ˆè¯¾ç¨‹å­¦ä¹ å¼‚å¸¸æ£€æµ‹è®­ç»ƒï¼‰çš„è„šæœ¬

åŸºäºæˆ‘ä»¬è®¾è®¡çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œå®ç°å¼‚å¸¸æ£€æµ‹å™¨çš„è®­ç»ƒã€‚
æ”¯æŒé…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°ï¼Œä¸ç¬¬ä¸€é˜¶æ®µä¿æŒä¸€è‡´çš„è°ƒç”¨é£æ ¼ã€‚
"""

import os
import sys
import argparse
import torch
import tomlkit
import yaml
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from DTransformer.data import KTData
from anomaly_kt.stages.stage2_curriculum_anomaly import train_curriculum_anomaly_detector, test_curriculum_components


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    print(f"ğŸ“„ å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    return config


def merge_config_with_args(config: Dict[str, Any], args: argparse.Namespace) -> argparse.Namespace:
    """
    åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
    å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜ï¼ˆå¦‚æœç”¨æˆ·æ˜¾å¼æä¾›äº†å‚æ•°ï¼‰
    """
    # è·å–parserçš„é»˜è®¤å€¼
    parser = create_parser()

    # è·å–æ‰€æœ‰å‚æ•°çš„é»˜è®¤å€¼
    defaults = {}
    for action in parser._actions:
        if action.dest != 'help':
            defaults[action.dest] = action.default

    # åˆ›å»ºæ–°çš„argså¯¹è±¡
    merged_args = argparse.Namespace(**vars(args))

    # ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼è¦†ç›–é»˜è®¤å€¼ï¼ˆä½†ä¸è¦†ç›–ç”¨æˆ·æ˜¾å¼æä¾›çš„å‘½ä»¤è¡Œå‚æ•°ï¼‰
    for key, value in config.items():
        if hasattr(merged_args, key):
            current_value = getattr(merged_args, key)
            default_value = defaults.get(key)

            # å¦‚æœå½“å‰å€¼ç­‰äºé»˜è®¤å€¼ï¼Œè¯´æ˜ç”¨æˆ·æ²¡æœ‰æ˜¾å¼æä¾›ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
            if current_value == default_value:
                setattr(merged_args, key, value)

    return merged_args


def auto_detect_config(dataset: str) -> str:
    """æ ¹æ®æ•°æ®é›†è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶"""
    config_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'configs')
    config_file = f"{dataset}_curriculum.yaml"
    config_path = os.path.join(config_dir, config_file)

    if os.path.exists(config_path):
        return config_path
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ•°æ®é›† {dataset} çš„è¯¾ç¨‹å­¦ä¹ é…ç½®æ–‡ä»¶: {config_path}")
        return None


def prepare_data(dataset_name: str, data_dir: str, batch_size: int, test_batch_size: int):
    """å‡†å¤‡æ•°æ®é›†"""
    # åŠ è½½æ•°æ®é›†é…ç½®
    datasets = tomlkit.load(open(os.path.join(data_dir, 'datasets.toml')))
    dataset_config = datasets[dataset_name]

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_data = KTData(
        os.path.join(data_dir, dataset_config['train']),
        dataset_config['inputs'],
        batch_size=batch_size,
        shuffle=True
    )

    val_data = KTData(
        os.path.join(data_dir, dataset_config.get('valid', dataset_config['test'])),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    test_data = KTData(
        os.path.join(data_dir, dataset_config['test']),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    return train_data, val_data, test_data, dataset_config


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Stage 2: Curriculum Learning Anomaly Detection Training')

    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument('--config', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAMLæ ¼å¼)')
    parser.add_argument('--auto_config', action='store_true',
                       help='æ ¹æ®æ•°æ®é›†è‡ªåŠ¨é€‰æ‹©é…ç½®æ–‡ä»¶')

    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True,
                       choices=['assist09', 'assist17', 'algebra05', 'statics'],
                       help='Dataset name')
    parser.add_argument('--data_dir', default='data', help='Data directory')
    parser.add_argument('--output_dir', default=None, help='Output directory')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='Device to use')
    parser.add_argument('--baseline_model_path', required=True,
                       help='Path to the baseline model from stage 1')

    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=16, help='Training batch size')
    parser.add_argument('--test_batch_size', type=int, default=32, help='Test batch size')

    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    parser.add_argument('--curriculum_strategy', default='hybrid',
                       choices=['performance_driven', 'time_driven', 'hybrid'],
                       help='Curriculum scheduling strategy')
    parser.add_argument('--curriculum_epochs', type=int, default=100,
                       help='Total curriculum training epochs')
    parser.add_argument('--anomaly_ratio', type=float, default=0.1,
                       help='Anomaly ratio in training data')
    parser.add_argument('--baseline_ratio', type=float, default=0.05,
                       help='Baseline anomaly ratio')
    parser.add_argument('--max_patience', type=int, default=5,
                       help='Max patience for curriculum advancement')

    # å¼‚å¸¸æ£€æµ‹å™¨å‚æ•°
    parser.add_argument('--detector_hidden_dim', type=int, default=256,
                       help='Hidden dimension of anomaly detector')
    parser.add_argument('--detector_num_layers', type=int, default=3,
                       help='Number of layers in anomaly detector')
    parser.add_argument('--detector_num_heads', type=int, default=16,
                       help='Number of attention heads in anomaly detector')
    parser.add_argument('--detector_dropout', type=float, default=0.3,
                       help='Dropout rate for anomaly detector')
    parser.add_argument('--detector_window_size', type=int, default=10,
                       help='Window size for causal anomaly detection')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='Learning rate for anomaly detector')
    parser.add_argument('--patience', type=int, default=10,
                       help='Early stopping patience')

    # ç¬¬ä¸€é˜¶æ®µæ¨¡å‹å‚æ•°ï¼ˆç”¨äºåŠ è½½åŸºçº¿æ¨¡å‹ï¼‰
    parser.add_argument('-p', '--with_pid', action='store_true',
                       help='Use problem ID (must match stage 1)')
    parser.add_argument('--d_model', type=int, default=128,
                       help='Model dimension (must match stage 1)')
    parser.add_argument('--n_heads', type=int, default=8,
                       help='Number of attention heads (must match stage 1)')
    parser.add_argument('--n_know', type=int, default=16,
                       help='Number of knowledge concepts (must match stage 1)')
    parser.add_argument('--n_layers', type=int, default=3,
                       help='Number of layers (must match stage 1)')
    parser.add_argument('--dropout', type=float, default=0.2,
                       help='Dropout rate (must match stage 1)')
    parser.add_argument('--lambda_cl', type=float, default=0.1,
                       help='Contrastive learning weight (must match stage 1)')
    parser.add_argument('--proj', action='store_true',
                       help='Use projection (must match stage 1)')
    parser.add_argument('--hard_neg', action='store_true',
                       help='Use hard negatives (must match stage 1)')
    parser.add_argument('--window', type=int, default=1,
                       help='Window size (must match stage 1)')

    # å®éªŒå‚æ•°
    parser.add_argument('--difficulty_estimation', action='store_true', default=True,
                       help='Enable difficulty estimation')
    parser.add_argument('--dry_run', action='store_true',
                       help='Test components without training')

    return parser


def validate_baseline_model(baseline_model_path: str) -> bool:
    """éªŒè¯åŸºçº¿æ¨¡å‹æ–‡ä»¶"""
    if not baseline_model_path:
        print("âŒ é”™è¯¯: å¿…é¡»æä¾›åŸºçº¿æ¨¡å‹è·¯å¾„ (--baseline_model_path)")
        return False
        
    if not os.path.exists(baseline_model_path):
        print(f"âŒ é”™è¯¯: åŸºçº¿æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {baseline_model_path}")
        return False
        
    print(f"âœ“ åŸºçº¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {baseline_model_path}")
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    print("="*60)
    print("ç¬¬äºŒé˜¶æ®µï¼šè¯¾ç¨‹å­¦ä¹ å¼‚å¸¸æ£€æµ‹è®­ç»ƒ")
    print("="*60)

    # éªŒè¯åŸºçº¿æ¨¡å‹
    if not validate_baseline_model(args.baseline_model_path):
        sys.exit(1)

    # å¤„ç†é…ç½®æ–‡ä»¶
    config = {}
    if args.config:
        # ç”¨æˆ·æŒ‡å®šäº†é…ç½®æ–‡ä»¶
        config = load_config(args.config)
    elif args.auto_config:
        # è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶
        config_path = auto_detect_config(args.dataset)
        if config_path:
            config = load_config(config_path)
        else:
            print("ğŸ”§ ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆæœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼‰")
    else:
        print("ğŸ”§ ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°å’Œé»˜è®¤å€¼")

    # åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
    if config:
        print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
        args = merge_config_with_args(config, args)
        print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output_dir = f"output/stage2_{args.dataset}_{timestamp}"

    os.makedirs(args.output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # ä¿å­˜é…ç½®
    config_save_path = os.path.join(args.output_dir, 'config.yaml')
    with open(config_save_path, 'w') as f:
        yaml.dump(vars(args), f, default_flow_style=False)
    print(f"ğŸ“„ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

    # å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data, dataset_config = prepare_data(
        args.dataset, args.data_dir, args.batch_size, args.test_batch_size
    )

    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  é—®é¢˜æ•°é‡: {dataset_config['n_questions']}")

    # æ‰“å°é…ç½®
    print("\nğŸ”§ é…ç½®ä¿¡æ¯:")
    print(f"  è¯¾ç¨‹ç­–ç•¥: {args.curriculum_strategy}")
    print(f"  è®­ç»ƒè½®æ•°: {args.curriculum_epochs}")
    print(f"  å¼‚å¸¸æ¯”ä¾‹: {args.anomaly_ratio}")
    print(f"  åŸºçº¿å¼‚å¸¸æ¯”ä¾‹: {args.baseline_ratio}")
    print(f"  æ£€æµ‹å™¨éšè—ç»´åº¦: {args.detector_hidden_dim}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  è®¾å¤‡: {args.device}")

    if args.dry_run:
        print("\nğŸ§ª Dry Run æ¨¡å¼ - ä»…æµ‹è¯•ç»„ä»¶")
        success = test_curriculum_components(args, dataset_config)
        if success:
            print("\nâœ… ç»„ä»¶æµ‹è¯•æˆåŠŸï¼Œå¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒ")
            print("\nğŸš€ æ­£å¼è®­ç»ƒå‘½ä»¤:")
            cmd = f"python {__file__} --dataset {args.dataset} --baseline_model_path {args.baseline_model_path}"
            if args.auto_config:
                cmd += " --auto_config"
            print(f"   {cmd}")
        return

    try:
        # è¿è¡Œç¬¬äºŒé˜¶æ®µè®­ç»ƒ
        model_path = train_curriculum_anomaly_detector(args, dataset_config, train_data, val_data)

        print("\n" + "="*60)
        print("âœ… ç¬¬äºŒé˜¶æ®µå®Œæˆ!")
        print("="*60)
        print(f"ğŸ“ å¼‚å¸¸æ£€æµ‹å™¨ä¿å­˜è·¯å¾„: {model_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

        # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæ—¥å¿—å’Œé˜¶æ®µè½¬æ¢")
        print("2. åˆ†æå¼‚å¸¸æ£€æµ‹æ€§èƒ½å’Œéš¾åº¦åˆ†çº§æ•ˆæœ")
        print("3. è¿è¡Œç¬¬ä¸‰é˜¶æ®µæˆ–è¿›è¡Œæ¨¡å‹è¯„ä¼°")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
