#!/usr/bin/env python3
"""
ç»§ç»­è®­ç»ƒè„šæœ¬ - ä»æœ€åä¿å­˜çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ
"""

import os
import sys
import torch
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from anomaly_kt.unified_trainer import UnifiedTrainer
from anomaly_kt.training_strategies import EnhancedStrategy, AggressiveStrategy, BasicStrategy
from anomaly_kt.detector import CausalAnomalyDetector
from anomaly_kt.evaluator import AnomalyEvaluator
from data_loader import get_data_loader

def find_latest_checkpoint(base_dir="output/"):
    """æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
    
    all_checkpoints = []
    
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.pt'):
                full_path = os.path.join(root, file)
                try:
                    checkpoint = torch.load(full_path, map_location='cpu')
                    epoch = checkpoint.get('epoch', 0)
                    metrics = checkpoint.get('metrics', {})
                    
                    all_checkpoints.append({
                        'path': full_path,
                        'epoch': epoch,
                        'metrics': metrics,
                        'mtime': os.path.getmtime(full_path)
                    })
                except:
                    continue
    
    if not all_checkpoints:
        return None
    
    # æŒ‰è½®æ¬¡æ’åº
    all_checkpoints.sort(key=lambda x: x['epoch'], reverse=True)
    return all_checkpoints[0]

def continue_training():
    """ç»§ç»­è®­ç»ƒä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(description='ç»§ç»­è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨')
    parser.add_argument('--checkpoint', type=str, help='æŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='æ€»è®­ç»ƒè½®æ•°')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    parser.add_argument('--dataset', type=str, default='assist17', help='æ•°æ®é›†')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--test_batch_size', type=int, default=32, help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--with_pid', action='store_true', help='ä½¿ç”¨é—®é¢˜ID')
    parser.add_argument('--use_cl', action='store_true', help='ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--proj', action='store_true', help='ä½¿ç”¨æŠ•å½±')
    parser.add_argument('--n_know', type=int, default=32, help='çŸ¥è¯†ç‚¹æ•°é‡')
    
    args = parser.parse_args()
    
    # 1. æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹
    if args.checkpoint:
        checkpoint_path = args.checkpoint
    else:
        print("ğŸ” æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹...")
        latest = find_latest_checkpoint()
        if not latest:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return
        checkpoint_path = latest['path']
    
    print(f"ğŸ“ ä½¿ç”¨æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # 2. åŠ è½½æ£€æŸ¥ç‚¹
    print("ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹...")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    start_epoch = checkpoint.get('epoch', 0)
    saved_metrics = checkpoint.get('metrics', {})
    
    print(f"ğŸ”¢ ä»ç¬¬ {start_epoch} è½®ç»§ç»­")
    print(f"ğŸ“Š ä¸Šæ¬¡æ€§èƒ½:")
    for key, value in saved_metrics.items():
        if isinstance(value, (int, float)):
            print(f"  {key}: {value:.4f}")
    
    # 3. å‡†å¤‡æ•°æ®
    print("ğŸ“š å‡†å¤‡æ•°æ®...")
    dataset_config = {
        'dataset_name': args.dataset,
        'batch_size': args.batch_size,
        'test_batch_size': args.test_batch_size,
        'with_pid': args.with_pid,
        'use_cl': args.use_cl,
        'proj': args.proj,
        'n_know': args.n_know
    }
    
    train_data, val_data, test_data = get_data_loader(dataset_config)
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
    model = CausalAnomalyDetector(
        n_know=args.n_know,
        d_model=256,
        n_heads=8,
        n_layers=4,
        dropout=0.1,
        with_pid=args.with_pid,
        use_cl=args.use_cl,
        proj=args.proj
    )
    
    # 5. åŠ è½½æ¨¡å‹çŠ¶æ€
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(args.device)
    
    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    if 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # 7. åˆ›å»ºè®­ç»ƒç­–ç•¥
    print("âš™ï¸  åˆ›å»ºè®­ç»ƒç­–ç•¥...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"output/continue_{args.dataset}_{timestamp}/detector"
    os.makedirs(save_dir, exist_ok=True)
    
    # ä½¿ç”¨Enhancedç­–ç•¥ï¼ˆæ ¹æ®ä¹‹å‰çš„æˆåŠŸç»éªŒï¼‰
    strategy = EnhancedStrategy(model, args.device, save_dir, patience=15)
    
    # 8. åˆ›å»ºè®­ç»ƒå™¨
    evaluator = AnomalyEvaluator()
    trainer = UnifiedTrainer(strategy, evaluator, save_dir)
    
    # 9. è®¾ç½®è®­ç»ƒé…ç½®
    train_config = {
        'epochs': args.epochs,
        'learning_rate': 0.001,
        'optimize_for': 'recall',  # ç»§ç»­ä¼˜åŒ–å¬å›ç‡
        'anomaly_ratio': 0.3,
        'patience': 15,
        'start_epoch': start_epoch  # ä»æŒ‡å®šè½®æ¬¡å¼€å§‹
    }
    
    print(f"ğŸš€ ç»§ç»­è®­ç»ƒ...")
    print(f"ğŸ“Š ä»ç¬¬ {start_epoch} è½®å¼€å§‹ï¼Œç›®æ ‡ {args.epochs} è½®")
    print(f"ğŸ¯ ä¼˜åŒ–ç›®æ ‡: {train_config['optimize_for']}")
    
    # 10. å¼€å§‹è®­ç»ƒ
    try:
        final_metrics = trainer.train(train_data, val_data, train_config)
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print("ğŸ“Š æœ€ç»ˆæ€§èƒ½:")
        for key, value in final_metrics.items():
            if isinstance(value, (int, float)):
                print(f"  {key}: {value:.4f}")
                
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print("ğŸ’¾ æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ä»¥éšæ—¶ç»§ç»­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    continue_training()
