#!/usr/bin/env python
"""
å®Œæ•´çš„å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªè®­ç»ƒæµç¨‹ - å¢å¼ºç‰ˆ

åŒ…æ‹¬ï¼š
1. è®­ç»ƒåŸºçº¿DTransformeræ¨¡å‹ï¼ˆå¯é€‰ï¼‰
2. ä½¿ç”¨å¢å¼ºæ–¹æ³•è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨
3. è®­ç»ƒå¼‚å¸¸æ„ŸçŸ¥çš„çŸ¥è¯†è¿½è¸ªæ¨¡å‹
4. è¯„ä¼°æ€§èƒ½æå‡
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import tomlkit
import yaml
import json
import numpy as np
import random
from datetime import datetime
from tqdm import tqdm
from typing import Dict, Tuple
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from DTransformer.data import KTData
from DTransformer.model import DTransformer

from anomaly_kt.generator import AnomalyGenerator
from anomaly_kt.detector import CausalAnomalyDetector
from anomaly_kt.model import AnomalyAwareDTransformer
from anomaly_kt.trainer import AnomalyDetectorTrainer, KnowledgeTracingTrainer
from anomaly_kt.evaluator import ComparisonEvaluator, plot_training_curves

# å¯¼å…¥å¢å¼ºè®­ç»ƒå™¨
from anomaly_kt.trainer import AnomalyDetectorTrainer as ImprovedAnomalyTrainer


# æ¿€è¿›çš„å¼‚å¸¸è®­ç»ƒå™¨ç±»
class AggressiveAnomalyTrainer(ImprovedAnomalyTrainer):
    """æ¿€è¿›çš„å¼‚å¸¸æ£€æµ‹è®­ç»ƒå™¨ - å¤„ç†ä¸¥é‡ä¸å¹³è¡¡"""

    def train_aggressive(self, train_loader, val_loader, epochs=30,
                        learning_rate=1e-3, anomaly_ratio=0.3,
                        min_anomaly_ratio=0.2, max_anomaly_ratio=0.5,
                        force_balance=True, extreme_weights=True):
        """æ¿€è¿›çš„è®­ç»ƒç­–ç•¥"""

        print("\nğŸ”¥ æ¿€è¿›è®­ç»ƒæ¨¡å¼å¯åŠ¨!")
        print(f"  - åŠ¨æ€å¼‚å¸¸æ¯”ä¾‹: {max_anomaly_ratio:.0%} â†’ {min_anomaly_ratio:.0%}")
        print(f"  - å¼ºåˆ¶æ‰¹æ¬¡å¹³è¡¡: {force_balance}")
        print(f"  - æç«¯ç±»åˆ«æƒé‡: {extreme_weights}")

        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå¯¹å­¦ä¹ ç‡æ›´æ•æ„Ÿï¼‰
        optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=learning_rate,
            betas=(0.9, 0.999),
            weight_decay=1e-5
        )

        # æ¿€è¿›çš„å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=2,
            threshold=0.01
        )

        best_recall = 0
        no_improve = 0

        for epoch in range(epochs):
            print(f"\n{'='*20} Epoch {epoch+1}/{epochs} {'='*20}")

            # æ¿€è¿›çš„åŠ¨æ€å¼‚å¸¸æ¯”ä¾‹
            if epoch < 5:
                current_ratio = max_anomaly_ratio  # 50%
            elif epoch < 10:
                current_ratio = 0.4
            elif epoch < 15:
                current_ratio = 0.3
            else:
                progress = (epoch - 15) / (epochs - 15)
                current_ratio = max(min_anomaly_ratio,
                                  0.3 * (1 - progress) + min_anomaly_ratio * progress)

            # è®­ç»ƒ
            train_metrics = self._train_epoch_aggressive(
                train_loader, optimizer, current_ratio,
                force_balance, extreme_weights
            )

            # éªŒè¯
            val_metrics = self._validate_epoch_improved(
                val_loader, current_ratio, 'recall', use_ema=False
            )

            # æ‰“å°ç»“æœ
            print(f"  ğŸ“Š å¼‚å¸¸æ¯”ä¾‹: {current_ratio:.1%}")
            print(f"  ğŸ“ˆ è®­ç»ƒ - Loss: {train_metrics['loss']:.4f}, "
                  f"æ­£æ ·æœ¬æ¯”ä¾‹: {train_metrics['positive_ratio']:.1%}")
            print(f"  ğŸ“Š éªŒè¯ - Recall: {val_metrics['recall']:.3f}, "
                  f"Precision: {val_metrics['precision']:.3f}, "
                  f"F1: {val_metrics['f1_score']:.3f}, "
                  f"AUC: {val_metrics['auc_roc']:.3f}")

            # å­¦ä¹ ç‡è°ƒæ•´
            scheduler.step(val_metrics['recall'])
            current_lr = optimizer.param_groups[0]['lr']
            print(f"  ğŸ“‰ å­¦ä¹ ç‡: {current_lr:.2e}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡‡å–ç´§æ€¥æªæ–½
            if epoch > 5 and val_metrics['recall'] < 0.3:
                print("\nâš ï¸  Recallè¿‡ä½ï¼Œé‡‡å–ç´§æ€¥æªæ–½!")
                self._emergency_measures(optimizer, learning_rate)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['recall'] > best_recall:
                best_recall = val_metrics['recall']
                self.best_metrics = val_metrics
                self.best_epoch = epoch + 1
                self.save_checkpoint(epoch, optimizer, val_metrics, is_best=True)
                no_improve = 0
                print(f"  âœ… æ–°çš„æœ€ä½³Recall: {best_recall:.4f}")
            else:
                no_improve += 1

            # æ—©åœ
            if no_improve >= 8:
                print("\nâ¹ï¸  æ—©åœè§¦å‘")
                break

        return self.best_metrics

    def _train_epoch_aggressive(self, train_loader, optimizer, anomaly_ratio,
                               force_balance, extreme_weights):
        """æ¿€è¿›çš„è®­ç»ƒepoch"""
        self.model.train()
        total_loss = 0
        n_batches = 0

        # ç»Ÿè®¡
        total_pos = 0
        total_neg = 0

        optimizer.zero_grad()

        for batch_idx, batch in enumerate(tqdm(train_loader, desc="æ¿€è¿›è®­ç»ƒ")):
            q, s, pid = self._get_batch_data(batch)

            # ç”Ÿæˆå¼‚å¸¸
            if force_balance:
                s_anomaly, labels = self._force_balanced_generation(q, s, anomaly_ratio)
            else:
                s_anomaly, labels = self._generate_smart_anomalies(q, s, anomaly_ratio, 0)

            labels = labels.to(self.device)

            # ç»Ÿè®¡
            mask = (s >= 0)
            pos_count = (labels[mask] == 1).sum().item()
            neg_count = (labels[mask] == 0).sum().item()
            total_pos += pos_count
            total_neg += neg_count

            # å‰å‘ä¼ æ’­
            predictions = self.model(q, s_anomaly, pid)

            # è®¡ç®—æŸå¤±
            if extreme_weights:
                loss = self._compute_extreme_weighted_loss(predictions, labels, s)
            else:
                loss = self._compute_focal_loss(predictions, labels, s, gamma=3.0, alpha=0.3)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
            if (batch_idx + 1) % 4 == 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()

            total_loss += loss.item()
            n_batches += 1

            # å®šæœŸæ‰“å°é¢„æµ‹åˆ†å¸ƒ
            if batch_idx % 100 == 0:
                with torch.no_grad():
                    pred_mean = predictions[mask].mean().item()
                    pred_std = predictions[mask].std().item()
                    tqdm.write(f"Batch {batch_idx} - Pred mean: {pred_mean:.3f}, "
                             f"std: {pred_std:.3f}, pos/neg: {pos_count}/{neg_count}")

        # æœ€åçš„æ¢¯åº¦æ›´æ–°
        if n_batches % 4 != 0:
            optimizer.step()
            optimizer.zero_grad()

        print(f"  ğŸ“Š æ‰¹æ¬¡ç»Ÿè®¡ - æ­£æ ·æœ¬: {total_pos}, è´Ÿæ ·æœ¬: {total_neg}, "
              f"æ­£æ ·æœ¬æ¯”ä¾‹: {total_pos/(total_pos+total_neg)*100:.1f}%")

        return {
            'loss': total_loss / n_batches,
            'positive_ratio': total_pos / (total_pos + total_neg)
        }

    def _force_balanced_generation(self, q, s, target_ratio):
        """å¼ºåˆ¶ç”Ÿæˆå¹³è¡¡çš„æ‰¹æ¬¡"""
        batch_size, seq_len = s.shape
        s_anomaly = s.clone()
        labels = torch.zeros_like(s)

        # ç¡®ä¿æ¯ä¸ªåºåˆ—éƒ½æœ‰å¼‚å¸¸
        for i in range(batch_size):
            valid_mask = (s[i] >= 0)
            valid_indices = torch.where(valid_mask)[0]

            if len(valid_indices) < 5:
                continue

            # æ¯ä¸ªåºåˆ—è‡³å°‘20%çš„ä½ç½®æ˜¯å¼‚å¸¸
            n_anomalies = max(1, int(len(valid_indices) * max(0.2, target_ratio)))
            anomaly_positions = torch.randperm(len(valid_indices))[:n_anomalies]

            for pos in anomaly_positions:
                idx = valid_indices[pos]
                # éšæœºé€‰æ‹©å¼‚å¸¸ç±»å‹
                if np.random.random() < 0.5:
                    # ç¿»è½¬ç­”æ¡ˆ
                    s_anomaly[i, idx] = 1 - s_anomaly[i, idx]
                else:
                    # éšæœºç­”æ¡ˆ
                    s_anomaly[i, idx] = np.random.randint(0, 2)
                labels[i, idx] = 1

        return s_anomaly, labels

    def _compute_extreme_weighted_loss(self, predictions, labels, s):
        """æç«¯åŠ æƒçš„æŸå¤±å‡½æ•°"""
        mask = (s >= 0)
        valid_preds = predictions[mask]
        valid_labels = labels[mask].float()

        if len(valid_preds) == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)

        # è®¡ç®—æç«¯çš„ç±»åˆ«æƒé‡
        pos_count = valid_labels.sum()
        neg_count = len(valid_labels) - pos_count

        if pos_count > 0:
            # ä½¿ç”¨æ›´æç«¯çš„æƒé‡
            pos_weight = (neg_count / pos_count) * 2
            pos_weight = torch.clamp(pos_weight, min=5.0, max=50.0)
        else:
            pos_weight = torch.tensor(20.0, device=self.device)

        # ä½¿ç”¨åŠ æƒBCE
        loss = F.binary_cross_entropy(valid_preds, valid_labels, reduction='none')
        weights = torch.where(valid_labels == 1, pos_weight, 1.0)

        # é¢å¤–æƒ©ç½šå‡é˜´æ€§ï¼ˆæ¼æ£€ï¼‰
        fn_penalty = torch.where(
            (valid_labels == 1) & (valid_preds < 0.5),
            torch.tensor(2.0, device=self.device),
            torch.tensor(1.0, device=self.device)
        )

        weighted_loss = (loss * weights * fn_penalty).mean()

        return weighted_loss

    def _emergency_measures(self, optimizer, base_lr):
        """ç´§æ€¥æªæ–½ï¼šå½“æ€§èƒ½å¤ªå·®æ—¶"""
        print("  ğŸš¨ æ‰§è¡Œç´§æ€¥æªæ–½:")

        # 1. é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´
        if hasattr(self.model, 'classifier'):
            for layer in self.model.classifier:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    if layer.bias is not None:
                        nn.init.constant_(layer.bias, 0.1)  # è½»å¾®æ­£åç½®
            print("    - é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´")

        # 2. å¢åŠ å­¦ä¹ ç‡
        new_lr = base_lr * 3
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_lr
        print(f"    - å­¦ä¹ ç‡æå‡åˆ°: {new_lr:.2e}")

        # 3. å‡å°‘æ­£åˆ™åŒ–
        for param_group in optimizer.param_groups:
            param_group['weight_decay'] = 1e-6
        print("    - å‡å°‘æƒé‡è¡°å‡")

    def _validate_epoch_improved(self, val_loader, anomaly_ratio, optimize_for='recall', use_ema=False):
        """æ”¹è¿›çš„éªŒè¯epoch"""
        self.model.eval()
        self.evaluator.reset()

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="éªŒè¯ä¸­", leave=False):
                q, s, pid = self._get_batch_data(batch)

                # ç”Ÿæˆå¼‚å¸¸
                s_anomaly, labels = self.generator.generate_anomalies(
                    q, s, anomaly_ratio=anomaly_ratio
                )

                # é¢„æµ‹
                predictions = self.model(q, s_anomaly, pid)

                # æ›´æ–°è¯„ä¼°å™¨
                self.evaluator.update(predictions, labels, q, s)

        return self.evaluator.compute_metrics()

    def _generate_smart_anomalies(self, q, s, anomaly_ratio, strategy_id):
        """ç”Ÿæˆæ™ºèƒ½å¼‚å¸¸"""
        return self.generator.generate_anomalies(q, s, anomaly_ratio=anomaly_ratio)

    def _compute_focal_loss(self, predictions, labels, s, gamma=2.0, alpha=0.25):
        """è®¡ç®—Focal Loss"""
        mask = (s >= 0)
        valid_preds = predictions[mask]
        valid_labels = labels[mask].float()

        if len(valid_preds) == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)

        # è®¡ç®—BCEæŸå¤±
        epsilon = 1e-7
        valid_preds_clipped = torch.clamp(valid_preds, epsilon, 1 - epsilon)
        bce_loss = -(valid_labels * torch.log(valid_preds_clipped) +
                    (1 - valid_labels) * torch.log(1 - valid_preds_clipped))

        # è®¡ç®—pt
        pt = torch.where(valid_labels == 1, valid_preds_clipped, 1 - valid_preds_clipped)

        # Alphaæƒé‡
        alpha_weight = torch.where(valid_labels == 1, alpha, 1 - alpha)

        # Focalæƒé‡
        focal_weight = alpha_weight * (1 - pt) ** gamma

        # æœ€ç»ˆæŸå¤±
        focal_loss = focal_weight * bce_loss

        return focal_loss.mean()


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        if config_path.endswith('.yaml') or config_path.endswith('.yml'):
            config = yaml.safe_load(f)
        else:
            raise ValueError("Only YAML config files are supported")
    return config


def prepare_data(dataset_name: str, data_dir: str, batch_size: int, test_batch_size: int):
    """å‡†å¤‡æ•°æ®é›†"""
    # åŠ è½½æ•°æ®é›†é…ç½®
    datasets = tomlkit.load(open(os.path.join(data_dir, 'datasets.toml')))
    dataset_config = datasets[dataset_name]

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_data = KTData(
        os.path.join(data_dir, dataset_config['train']),
        dataset_config['inputs'],
        batch_size=batch_size,
        shuffle=True
    )

    val_data = KTData(
        os.path.join(data_dir, dataset_config.get('valid', dataset_config['test'])),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    test_data = KTData(
        os.path.join(data_dir, dataset_config['test']),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    return train_data, val_data, test_data, dataset_config


def train_baseline_model(args, dataset_config, train_data, val_data):
    """è®­ç»ƒåŸºçº¿DTransformeræ¨¡å‹"""
    print("\n" + "="*60)
    print("PHASE 1: Training Baseline DTransformer")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹
    model = DTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_know=args.n_know,
        n_layers=args.n_layers,
        dropout=args.dropout,
        lambda_cl=args.lambda_cl,
        proj=args.proj,
        hard_neg=args.hard_neg,
        window=args.window
    )

    # è®­ç»ƒå™¨
    trainer = KnowledgeTracingTrainer(
        model=model,
        device=args.device,
        save_dir=os.path.join(args.output_dir, 'baseline'),
        patience=args.patience
    )

    # è®­ç»ƒ
    baseline_metrics = trainer.train(
        train_loader=train_data,
        val_loader=val_data,
        epochs=args.kt_epochs,
        learning_rate=args.learning_rate,
        use_cl=args.use_cl
    )

    print(f"\nBaseline training completed!")
    print(f"Best AUC: {baseline_metrics['auc']:.4f}")

    return os.path.join(args.output_dir, 'baseline', 'best_model.pt')


def train_anomaly_detector_enhanced(args, dataset_config, train_data, val_data):
    """ä½¿ç”¨å¢å¼ºæ–¹æ³•è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨"""
    print("\n" + "="*60)
    print("PHASE 2: Training Anomaly Detector (Enhanced Version)")
    print("="*60)

    # åˆ›å»ºæ£€æµ‹å™¨
    detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.detector_d_model,
        n_heads=args.detector_n_heads,
        n_layers=args.detector_n_layers,
        dropout=args.detector_dropout,
        window_size=args.window_size
    )

    print(f"ğŸ§  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in detector.parameters()):,}")

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ¿€è¿›ç­–ç•¥
    if args.use_aggressive_strategy:
        print("\nâš ï¸  ä½¿ç”¨æ¿€è¿›è®­ç»ƒç­–ç•¥ï¼")
        print("  - åŠ¨æ€å¼‚å¸¸æ¯”ä¾‹: 50% â†’ 20%")
        print("  - å¼ºåˆ¶æ‰¹æ¬¡å¹³è¡¡")
        print("  - æç«¯ç±»åˆ«æƒé‡")

        trainer = AggressiveAnomalyTrainer(
            model=detector,
            device=args.device,
            save_dir=os.path.join(args.output_dir, 'detector'),
            patience=args.detector_patience
        )

        # ä½¿ç”¨æ¿€è¿›çš„è®­ç»ƒå‚æ•°
        detector_metrics = trainer.train_aggressive(
            train_loader=train_data,
            val_loader=val_data,
            epochs=args.detector_epochs,
            learning_rate=args.detector_lr * 2,  # æ›´é«˜çš„å­¦ä¹ ç‡
            anomaly_ratio=args.anomaly_ratio,
            min_anomaly_ratio=0.2,  # æœ€å°20%å¼‚å¸¸
            max_anomaly_ratio=0.5,  # æœ€å¤§50%å¼‚å¸¸
            force_balance=True,
            extreme_weights=True
        )
    else:
        # ä½¿ç”¨æ ‡å‡†å¢å¼ºè®­ç»ƒå™¨
        trainer = ImprovedAnomalyTrainer(
            model=detector,
            device=args.device,
            save_dir=os.path.join(args.output_dir, 'detector'),
            patience=args.detector_patience
        )

        print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
        print(f"  - ä¼˜åŒ–ç›®æ ‡: {args.optimize_for}")
        print(f"  - å¼‚å¸¸æ¯”ä¾‹: {args.anomaly_ratio}")
        print(f"  - Focal Loss: {args.use_focal_loss}")
        print(f"  - Mixup: {args.use_mixup}")
        print(f"  - Label Smoothing: {args.use_label_smoothing}")
        print(f"  - æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation_steps}æ­¥")

        # è®­ç»ƒ
        detector_metrics = trainer.train(
            train_loader=train_data,
            val_loader=val_data,
            epochs=args.detector_epochs,
            learning_rate=args.detector_lr,
            anomaly_ratio=args.anomaly_ratio,
            optimize_for=args.optimize_for
        )

    print(f"\nâœ… Detector training completed!")
    print(f"Best {args.optimize_for}: {detector_metrics[args.optimize_for]:.4f}")
    print(f"F1 Score: {detector_metrics['f1_score']:.4f}")
    print(f"AUC-ROC: {detector_metrics['auc_roc']:.4f}")
    print(f"Best Threshold: {detector_metrics.get('best_threshold', 0.5):.3f}")

    # ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦
    summary = {
        'metrics': detector_metrics,
        'config': {
            'anomaly_ratio': args.anomaly_ratio,
            'optimize_for': args.optimize_for,
            'use_focal_loss': args.use_focal_loss,
            'use_mixup': args.use_mixup,
            'use_label_smoothing': args.use_label_smoothing
        }
    }

    with open(os.path.join(args.output_dir, 'detector', 'training_summary.json'), 'w') as f:
        json.dump(summary, f, indent=2)

    # ä½¿ç”¨EMAæ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    ema_model_path = os.path.join(args.output_dir, 'detector', f'ema_model_epoch_{trainer.best_epoch}.pt')
    if os.path.exists(ema_model_path):
        print(f"ğŸ“Œ ä½¿ç”¨EMAæ¨¡å‹ä½œä¸ºæœ€ç»ˆæ£€æµ‹å™¨")
        return ema_model_path
    else:
        return os.path.join(args.output_dir, 'detector', 'best_model.pt')


def train_anomaly_aware_model(args, dataset_config, train_data, val_data, detector_path):
    """è®­ç»ƒå¼‚å¸¸æ„ŸçŸ¥çš„çŸ¥è¯†è¿½è¸ªæ¨¡å‹"""
    print("\n" + "="*60)
    print("PHASE 3: Training Anomaly-Aware DTransformer")
    print("="*60)

    # åŠ è½½å¼‚å¸¸æ£€æµ‹å™¨
    detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.detector_d_model,
        n_heads=args.detector_n_heads,
        n_layers=args.detector_n_layers,
        dropout=args.detector_dropout,
        window_size=args.window_size
    )

    # åŠ è½½æ£€æµ‹å™¨æƒé‡
    if 'ema_model' in detector_path:
        print("ğŸ“Œ åŠ è½½EMAæ£€æµ‹å™¨æ¨¡å‹")
        detector.load_state_dict(torch.load(detector_path, map_location=args.device))
    else:
        checkpoint = torch.load(detector_path, map_location=args.device)
        if 'model_state_dict' in checkpoint:
            detector.load_state_dict(checkpoint['model_state_dict'])
        else:
            detector.load_state_dict(checkpoint)

    detector.to(args.device)
    detector.eval()

    # åˆ›å»ºå¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹
    model = AnomalyAwareDTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_know=args.n_know,
        n_layers=args.n_layers,
        dropout=args.dropout,
        lambda_cl=args.lambda_cl,
        proj=args.proj,
        hard_neg=args.hard_neg,
        window=args.window,
        anomaly_detector=detector,
        anomaly_weight=args.anomaly_weight
    )

    print(f"ğŸ§  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"âš–ï¸  å¼‚å¸¸æƒé‡: {args.anomaly_weight}")

    # è®­ç»ƒå™¨
    trainer = KnowledgeTracingTrainer(
        model=model,
        device=args.device,
        save_dir=os.path.join(args.output_dir, 'anomaly_aware'),
        patience=args.patience
    )

    # è®­ç»ƒ
    anomaly_metrics = trainer.train(
        train_loader=train_data,
        val_loader=val_data,
        epochs=args.kt_epochs,
        learning_rate=args.learning_rate,
        use_cl=args.use_cl
    )

    print(f"\nâœ… Anomaly-aware training completed!")
    print(f"Best AUC: {anomaly_metrics['auc']:.4f}")

    return os.path.join(args.output_dir, 'anomaly_aware', 'best_model.pt')


def evaluate_models(args, dataset_config, test_data, baseline_path, anomaly_path, detector_path):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("\n" + "="*60)
    print("PHASE 4: Model Evaluation")
    print("="*60)

    # åŠ è½½åŸºçº¿æ¨¡å‹
    baseline_model = DTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_know=args.n_know,
        n_layers=args.n_layers
    )

    # å¤„ç†åŸºçº¿æ¨¡å‹çš„checkpointæ ¼å¼
    baseline_checkpoint = torch.load(baseline_path, map_location=args.device)
    if isinstance(baseline_checkpoint, dict) and 'model_state_dict' in baseline_checkpoint:
        baseline_model.load_state_dict(baseline_checkpoint['model_state_dict'])
    else:
        baseline_model.load_state_dict(baseline_checkpoint)
    baseline_model.to(args.device)

    # åŠ è½½å¼‚å¸¸æ£€æµ‹å™¨
    detector = CausalAnomalyDetector(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.detector_d_model,
        n_heads=args.detector_n_heads,
        n_layers=args.detector_n_layers,
        dropout=args.detector_dropout,
        window_size=args.window_size
    )

    if 'ema_model' in detector_path:
        detector.load_state_dict(torch.load(detector_path, map_location=args.device))
    else:
        checkpoint = torch.load(detector_path, map_location=args.device)
        if 'model_state_dict' in checkpoint:
            detector.load_state_dict(checkpoint['model_state_dict'])
        else:
            detector.load_state_dict(checkpoint)
    detector.to(args.device)

    # åŠ è½½å¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹
    anomaly_model = AnomalyAwareDTransformer(
        n_questions=dataset_config['n_questions'],
        n_pid=dataset_config['n_pid'] if args.with_pid else 0,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_know=args.n_know,
        n_layers=args.n_layers,
        anomaly_detector=detector,
        anomaly_weight=args.anomaly_weight
    )

    # å¤„ç†å¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹çš„checkpointæ ¼å¼
    anomaly_checkpoint = torch.load(anomaly_path, map_location=args.device)
    if isinstance(anomaly_checkpoint, dict) and 'model_state_dict' in anomaly_checkpoint:
        anomaly_model.load_state_dict(anomaly_checkpoint['model_state_dict'])
    else:
        anomaly_model.load_state_dict(anomaly_checkpoint)
    anomaly_model.to(args.device)

    # è¯„ä¼°
    evaluator = ComparisonEvaluator()
    results = evaluator.evaluate_models(test_data, baseline_model, anomaly_model, args.device)

    # æ‰“å°ç»“æœ
    evaluator.print_comparison(results)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = os.path.join(args.output_dir, 'evaluation_results.json')
    results['config'] = {
        'anomaly_weight': args.anomaly_weight,
        'detector_epochs': args.detector_epochs,
        'anomaly_ratio': args.anomaly_ratio,
        'optimize_for': args.optimize_for
    }

    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nğŸ“Š Results saved to: {results_path}")

    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    generate_evaluation_report(args.output_dir, results)

    return results


def generate_evaluation_report(output_dir: str, results: Dict):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    report_path = os.path.join(output_dir, 'evaluation_report.md')

    with open(report_path, 'w') as f:
        f.write("# å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªè¯„ä¼°æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("### åŸºçº¿æ¨¡å‹\n")
        for metric, value in results['baseline'].items():
            f.write(f"- {metric.upper()}: {value:.4f}\n")

        f.write("\n### å¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹\n")
        for metric, value in results['anomaly_aware'].items():
            f.write(f"- {metric.upper()}: {value:.4f}\n")

        f.write("\n## æ€§èƒ½æå‡\n")
        for metric, improvement in results['improvements'].items():
            symbol = "â†‘" if improvement > 0 else "â†“"
            f.write(f"- {metric.upper()}: {improvement:+.2f}% {symbol}\n")

        f.write("\n## é…ç½®ä¿¡æ¯\n")
        if 'config' in results:
            for key, value in results['config'].items():
                f.write(f"- {key}: {value}\n")

    print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='Full Anomaly-Aware KT Pipeline (Enhanced)')

    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True, choices=['assist09', 'assist17', 'algebra05', 'statics'])
    parser.add_argument('--data_dir', default='data')
    parser.add_argument('--output_dir', default=None)
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--config', help='Config file path')
    parser.add_argument('-p', '--with_pid', action='store_true')
    parser.add_argument('--seed', type=int, default=42)

    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    args, _ = parser.parse_known_args()

    if args.config:
        config = load_config(args.config)
        parser.set_defaults(**config)

    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--test_batch_size', type=int, default=64)

    # åŸºçº¿æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128)
    parser.add_argument('--n_heads', type=int, default=8)
    parser.add_argument('--n_know', type=int, default=16)
    parser.add_argument('--n_layers', type=int, default=3)
    parser.add_argument('--dropout', type=float, default=0.2)
    parser.add_argument('--lambda_cl', type=float, default=0.1)
    parser.add_argument('--proj', action='store_true')
    parser.add_argument('--hard_neg', action='store_true')
    parser.add_argument('--window', type=int, default=1)

    # å¼‚å¸¸æ£€æµ‹å™¨å‚æ•°
    parser.add_argument('--detector_d_model', type=int, default=128)
    parser.add_argument('--detector_n_heads', type=int, default=8)
    parser.add_argument('--detector_n_layers', type=int, default=2)
    parser.add_argument('--detector_dropout', type=float, default=0.1)
    parser.add_argument('--window_size', type=int, default=15)
    parser.add_argument('--anomaly_ratio', type=float, default=0.25)
    parser.add_argument('--optimize_for', default='recall', choices=['f1_score', 'auc_roc', 'recall', 'precision'])

    # å¢å¼ºè®­ç»ƒå‚æ•°
    parser.add_argument('--use_focal_loss', action='store_true', default=True)
    parser.add_argument('--focal_gamma', type=float, default=2.0)
    parser.add_argument('--focal_alpha', type=float, default=0.25)
    parser.add_argument('--use_mixup', action='store_true', default=True)
    parser.add_argument('--mixup_alpha', type=float, default=0.2)
    parser.add_argument('--use_label_smoothing', action='store_true', default=True)
    parser.add_argument('--label_smoothing', type=float, default=0.1)
    parser.add_argument('--use_gradient_accumulation', action='store_true', default=True)
    parser.add_argument('--gradient_accumulation_steps', type=int, default=4)
    parser.add_argument('--use_warmup', action='store_true', default=True)
    parser.add_argument('--warmup_epochs', type=int, default=5)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--kt_epochs', type=int, default=100)
    parser.add_argument('--detector_epochs', type=int, default=40)
    parser.add_argument('--learning_rate', type=float, default=1e-3)
    parser.add_argument('--detector_lr', type=float, default=5e-4)
    parser.add_argument('--patience', type=int, default=10)
    parser.add_argument('--detector_patience', type=int, default=15)
    parser.add_argument('--use_cl', action='store_true')

    # å¼‚å¸¸æ„ŸçŸ¥å‚æ•°
    parser.add_argument('--anomaly_weight', type=float, default=0.5)

    # æ§åˆ¶å‚æ•°
    parser.add_argument('--skip_baseline', action='store_true', help='Skip baseline training')
    parser.add_argument('--skip_detector', action='store_true', help='Skip detector training')
    parser.add_argument('--baseline_path', help='Path to existing baseline model')
    parser.add_argument('--detector_path', help='Path to existing detector model')
    parser.add_argument('--use_aggressive_strategy', action='store_true', default=False,
                        help='ä½¿ç”¨æ¿€è¿›ç­–ç•¥å¤„ç†ä¸¥é‡ä¸å¹³è¡¡')

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if args.skip_baseline:
        if not args.baseline_path:
            print("ERROR: --skip_baseline requires --baseline_path to be specified")
            sys.exit(1)
        if not os.path.exists(args.baseline_path):
            print(f"ERROR: Baseline model file not found: {args.baseline_path}")
            sys.exit(1)
        print(f"âœ“ Baseline model found: {args.baseline_path}")

    if args.skip_detector:
        if not args.detector_path:
            print("ERROR: --skip_detector requires --detector_path to be specified")
            sys.exit(1)
        if not os.path.exists(args.detector_path):
            print(f"ERROR: Detector model file not found: {args.detector_path}")
            sys.exit(1)
        print(f"âœ“ Detector model found: {args.detector_path}")

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output_dir = f"output/{args.dataset}_{timestamp}_enhanced"

    os.makedirs(args.output_dir, exist_ok=True)

    # ä¿å­˜é…ç½®
    config_save_path = os.path.join(args.output_dir, 'config.yaml')
    with open(config_save_path, 'w') as f:
        yaml.dump(vars(args), f, default_flow_style=False)

    print("\nğŸš€ å¼‚å¸¸æ„ŸçŸ¥çŸ¥è¯†è¿½è¸ªè®­ç»ƒæµç¨‹ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("="*60)
    print("é…ç½®ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  è®¾å¤‡: {args.device}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  ä¼˜åŒ–ç›®æ ‡: {args.optimize_for}")
    print(f"  å¼‚å¸¸æ¯”ä¾‹: {args.anomaly_ratio}")
    print(f"  å¼‚å¸¸æƒé‡: {args.anomaly_weight}")

    # å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data, dataset_config = prepare_data(
        args.dataset, args.data_dir, args.batch_size, args.test_batch_size
    )
    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_data)}")
    print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_data)}")
    print(f"  æµ‹è¯•æ‰¹æ¬¡: {len(test_data)}")

    # 1. è®­ç»ƒåŸºçº¿æ¨¡å‹
    if not args.skip_baseline:
        baseline_path = train_baseline_model(args, dataset_config, train_data, val_data)
    else:
        baseline_path = args.baseline_path
        print(f"\nğŸ“Œ ä½¿ç”¨å·²æœ‰åŸºçº¿æ¨¡å‹: {baseline_path}")

    # 2. è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    if not args.skip_detector:
        detector_path = train_anomaly_detector_enhanced(args, dataset_config, train_data, val_data)
    else:
        detector_path = args.detector_path
        print(f"\nğŸ“Œ ä½¿ç”¨å·²æœ‰æ£€æµ‹å™¨: {detector_path}")

    # 3. è®­ç»ƒå¼‚å¸¸æ„ŸçŸ¥æ¨¡å‹
    anomaly_path = train_anomaly_aware_model(args, dataset_config, train_data, val_data, detector_path)

    # 4. è¯„ä¼°ç»“æœ
    results = evaluate_models(args, dataset_config, test_data, baseline_path, anomaly_path, detector_path)

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
    print("="*60)

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    improvement = results['improvements']['auc']
    if improvement >= 1.0:
        print(f"âœ… æˆåŠŸ: ç›®æ ‡è¾¾æˆ! AUCæå‡ {improvement:.2f}%")
    else:
        print(f"âš ï¸  æœªè¾¾æ ‡: AUCæå‡ {improvement:.2f}% (ç›®æ ‡: â‰¥1%)")
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        print("  1. è°ƒæ•´å¼‚å¸¸æƒé‡ (--anomaly_weight)ï¼Œå½“å‰å€¼: {}".format(args.anomaly_weight))
        print("  2. å¢åŠ æ£€æµ‹å™¨è®­ç»ƒè½®æ•° (--detector_epochs)")
        print("  3. å°è¯•ä¸åŒçš„å¼‚å¸¸æ¯”ä¾‹ (--anomaly_ratio)")
        print("  4. è°ƒæ•´ä¼˜åŒ–ç›®æ ‡ (--optimize_for)")
        print("  5. å¢åŠ è®­ç»ƒæ•°æ®çš„æ‰¹æ¬¡å¤§å°")

    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {args.output_dir}")


if __name__ == '__main__':
    main()