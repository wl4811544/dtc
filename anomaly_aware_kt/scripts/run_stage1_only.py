#!/usr/bin/env python
"""
å•ç‹¬è¿è¡Œç¬¬ä¸€é˜¶æ®µï¼ˆåŸºçº¿æ¨¡å‹è®­ç»ƒï¼‰çš„ç¤ºä¾‹è„šæœ¬

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æå–çš„ç¬¬ä¸€é˜¶æ®µä»£ç æ¥å•ç‹¬è®­ç»ƒåŸºçº¿æ¨¡å‹ã€‚
"""

import os
import sys
import argparse
import torch
import tomlkit
import yaml
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from DTransformer.data import KTData
from anomaly_kt.stages.stage1_baseline import train_baseline_model


def prepare_data(dataset_name: str, data_dir: str, batch_size: int, test_batch_size: int):
    """å‡†å¤‡æ•°æ®é›†"""
    # åŠ è½½æ•°æ®é›†é…ç½®
    datasets = tomlkit.load(open(os.path.join(data_dir, 'datasets.toml')))
    dataset_config = datasets[dataset_name]

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_data = KTData(
        os.path.join(data_dir, dataset_config['train']),
        dataset_config['inputs'],
        batch_size=batch_size,
        shuffle=True
    )

    val_data = KTData(
        os.path.join(data_dir, dataset_config.get('valid', dataset_config['test'])),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    test_data = KTData(
        os.path.join(data_dir, dataset_config['test']),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    return train_data, val_data, test_data, dataset_config


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Stage 1: Baseline Model Training')

    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True,
                       choices=['assist09', 'assist17', 'algebra05', 'statics'],
                       help='Dataset name')
    parser.add_argument('--data_dir', default='data', help='Data directory')
    parser.add_argument('--output_dir', default=None, help='Output directory')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='Device to use')
    parser.add_argument('-p', '--with_pid', action='store_true',
                       help='Use problem ID')

    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=32, help='Training batch size')
    parser.add_argument('--test_batch_size', type=int, default=64, help='Test batch size')

    # åŸºçº¿æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128, help='Model dimension')
    parser.add_argument('--n_heads', type=int, default=8, help='Number of attention heads')
    parser.add_argument('--n_know', type=int, default=16, help='Number of knowledge concepts')
    parser.add_argument('--n_layers', type=int, default=3, help='Number of layers')
    parser.add_argument('--dropout', type=float, default=0.2, help='Dropout rate')
    parser.add_argument('--lambda_cl', type=float, default=0.1, help='Contrastive learning weight')
    parser.add_argument('--proj', action='store_true', help='Use projection')
    parser.add_argument('--hard_neg', action='store_true', help='Use hard negatives')
    parser.add_argument('--window', type=int, default=1, help='Window size')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--kt_epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Learning rate')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience')
    parser.add_argument('--use_cl', action='store_true', help='Use contrastive learning')

    return parser


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    print("="*60)
    print("ç¬¬ä¸€é˜¶æ®µï¼šåŸºçº¿æ¨¡å‹è®­ç»ƒ")
    print("="*60)

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output_dir = f"output/stage1_{args.dataset}_{timestamp}"

    os.makedirs(args.output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # ä¿å­˜é…ç½®
    config_save_path = os.path.join(args.output_dir, 'config.yaml')
    with open(config_save_path, 'w') as f:
        yaml.dump(vars(args), f, default_flow_style=False)
    print(f"ğŸ“„ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

    # å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data, dataset_config = prepare_data(
        args.dataset, args.data_dir, args.batch_size, args.test_batch_size
    )

    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  é—®é¢˜æ•°é‡: {dataset_config['n_questions']}")

    # æ‰“å°é…ç½®
    print("\nğŸ”§ é…ç½®ä¿¡æ¯:")
    print(f"  æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {args.n_heads}")
    print(f"  çŸ¥è¯†æ¦‚å¿µæ•°: {args.n_know}")
    print(f"  å±‚æ•°: {args.n_layers}")
    print(f"  è®­ç»ƒè½®æ•°: {args.kt_epochs}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  è®¾å¤‡: {args.device}")

    try:
        # è¿è¡Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
        model_path = train_baseline_model(args, dataset_config, train_data, val_data)

        print("\n" + "="*60)
        print("âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆ!")
        print("="*60)
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

        # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æ€§èƒ½")
        print("2. è¿è¡Œç¬¬äºŒé˜¶æ®µè®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨")
        print("3. æˆ–è€…ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ¨ç†")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
