#!/usr/bin/env python
"""
å•ç‹¬è¿è¡Œç¬¬ä¸€é˜¶æ®µï¼ˆåŸºçº¿æ¨¡å‹è®­ç»ƒï¼‰çš„ç¤ºä¾‹è„šæœ¬

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æå–çš„ç¬¬ä¸€é˜¶æ®µä»£ç æ¥å•ç‹¬è®­ç»ƒåŸºçº¿æ¨¡å‹ã€‚
"""

import os
import sys
import argparse
import torch
import tomlkit
import yaml
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from DTransformer.data import KTData
from anomaly_kt.stages.stage1_baseline import train_baseline_model


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    print(f"ğŸ“„ å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    return config


def merge_config_with_args(config: Dict[str, Any], args: argparse.Namespace) -> argparse.Namespace:
    """
    åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
    å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜ï¼ˆå¦‚æœç”¨æˆ·æ˜¾å¼æä¾›äº†å‚æ•°ï¼‰
    """
    # è·å–parserçš„é»˜è®¤å€¼ï¼ˆä¸è§£æå‚æ•°ï¼Œé¿å…requiredå‚æ•°é”™è¯¯ï¼‰
    parser = create_parser()

    # è·å–æ‰€æœ‰å‚æ•°çš„é»˜è®¤å€¼
    defaults = {}
    for action in parser._actions:
        if action.dest != 'help':
            defaults[action.dest] = action.default

    # åˆ›å»ºæ–°çš„argså¯¹è±¡ï¼Œä»åŸargså¼€å§‹
    merged_args = argparse.Namespace(**vars(args))

    # ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼è¦†ç›–é»˜è®¤å€¼ï¼ˆä½†ä¸è¦†ç›–ç”¨æˆ·æ˜¾å¼æä¾›çš„å‘½ä»¤è¡Œå‚æ•°ï¼‰
    for key, value in config.items():
        if hasattr(merged_args, key):  # åªè®¾ç½®è„šæœ¬æ”¯æŒçš„å‚æ•°
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¾å¼æä¾›äº†è¿™ä¸ªå‚æ•°
            current_value = getattr(merged_args, key)
            default_value = defaults.get(key)

            # å¦‚æœå½“å‰å€¼ç­‰äºé»˜è®¤å€¼ï¼Œè¯´æ˜ç”¨æˆ·æ²¡æœ‰æ˜¾å¼æä¾›ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
            if current_value == default_value:
                setattr(merged_args, key, value)
            # å¦åˆ™ä¿æŒç”¨æˆ·æä¾›çš„å‘½ä»¤è¡Œå‚æ•°å€¼

    return merged_args


def auto_detect_config(dataset: str) -> str:
    """æ ¹æ®æ•°æ®é›†è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶"""
    config_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'configs')
    config_file = f"{dataset}_baseline.yaml"
    config_path = os.path.join(config_dir, config_file)

    if os.path.exists(config_path):
        return config_path
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°æ•°æ®é›† {dataset} çš„é»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
        return None


def prepare_data(dataset_name: str, data_dir: str, batch_size: int, test_batch_size: int):
    """å‡†å¤‡æ•°æ®é›†"""
    # åŠ è½½æ•°æ®é›†é…ç½®
    datasets = tomlkit.load(open(os.path.join(data_dir, 'datasets.toml')))
    dataset_config = datasets[dataset_name]

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_data = KTData(
        os.path.join(data_dir, dataset_config['train']),
        dataset_config['inputs'],
        batch_size=batch_size,
        shuffle=True
    )

    val_data = KTData(
        os.path.join(data_dir, dataset_config.get('valid', dataset_config['test'])),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    test_data = KTData(
        os.path.join(data_dir, dataset_config['test']),
        dataset_config['inputs'],
        batch_size=test_batch_size
    )

    return train_data, val_data, test_data, dataset_config


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Stage 1: Baseline Model Training')

    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument('--config', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAMLæ ¼å¼)')
    parser.add_argument('--auto_config', action='store_true',
                       help='æ ¹æ®æ•°æ®é›†è‡ªåŠ¨é€‰æ‹©é…ç½®æ–‡ä»¶')

    # åŸºæœ¬å‚æ•°
    parser.add_argument('--dataset', required=True,
                       choices=['assist09', 'assist17', 'algebra05', 'statics'],
                       help='Dataset name')
    parser.add_argument('--data_dir', default='data', help='Data directory')
    parser.add_argument('--output_dir', default=None, help='Output directory')
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='Device to use')
    parser.add_argument('-p', '--with_pid', action='store_true',
                       help='Use problem ID')

    # æ•°æ®å‚æ•°
    parser.add_argument('--batch_size', type=int, default=32, help='Training batch size')
    parser.add_argument('--test_batch_size', type=int, default=64, help='Test batch size')

    # åŸºçº¿æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128, help='Model dimension')
    parser.add_argument('--n_heads', type=int, default=8, help='Number of attention heads')
    parser.add_argument('--n_know', type=int, default=16, help='Number of knowledge concepts')
    parser.add_argument('--n_layers', type=int, default=3, help='Number of layers')
    parser.add_argument('--dropout', type=float, default=0.2, help='Dropout rate')
    parser.add_argument('--lambda_cl', type=float, default=0.1, help='Contrastive learning weight')
    parser.add_argument('--proj', action='store_true', help='Use projection')
    parser.add_argument('--hard_neg', action='store_true', help='Use hard negatives')
    parser.add_argument('--window', type=int, default=1, help='Window size')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--kt_epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--learning_rate', type=float, default=1e-3, help='Learning rate')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience')
    parser.add_argument('--use_cl', action='store_true', help='Use contrastive learning')

    return parser


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    print("="*60)
    print("ç¬¬ä¸€é˜¶æ®µï¼šåŸºçº¿æ¨¡å‹è®­ç»ƒ")
    print("="*60)

    # å¤„ç†é…ç½®æ–‡ä»¶
    config = {}
    if args.config:
        # ç”¨æˆ·æŒ‡å®šäº†é…ç½®æ–‡ä»¶
        config = load_config(args.config)
    elif args.auto_config:
        # è‡ªåŠ¨æ£€æµ‹é…ç½®æ–‡ä»¶
        config_path = auto_detect_config(args.dataset)
        if config_path:
            config = load_config(config_path)
        else:
            print("ğŸ”§ ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆæœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼‰")
    else:
        print("ğŸ”§ ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°å’Œé»˜è®¤å€¼")

    # åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
    if config:
        print("ğŸ”„ åˆå¹¶é…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°...")
        args = merge_config_with_args(config, args)
        print("âœ… å‚æ•°åˆå¹¶å®Œæˆï¼Œå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜")

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output_dir = f"output/stage1_{args.dataset}_{timestamp}"

    os.makedirs(args.output_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # ä¿å­˜é…ç½®
    config_save_path = os.path.join(args.output_dir, 'config.yaml')
    with open(config_save_path, 'w') as f:
        yaml.dump(vars(args), f, default_flow_style=False)
    print(f"ğŸ“„ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

    # å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data, dataset_config = prepare_data(
        args.dataset, args.data_dir, args.batch_size, args.test_batch_size
    )

    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  é—®é¢˜æ•°é‡: {dataset_config['n_questions']}")

    # æ‰“å°é…ç½®
    print("\nğŸ”§ é…ç½®ä¿¡æ¯:")
    print(f"  æ¨¡å‹ç»´åº¦: {args.d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {args.n_heads}")
    print(f"  çŸ¥è¯†æ¦‚å¿µæ•°: {args.n_know}")
    print(f"  å±‚æ•°: {args.n_layers}")
    print(f"  è®­ç»ƒè½®æ•°: {args.kt_epochs}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  è®¾å¤‡: {args.device}")

    try:
        # è¿è¡Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
        model_path = train_baseline_model(args, dataset_config, train_data, val_data)

        print("\n" + "="*60)
        print("âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆ!")
        print("="*60)
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

        # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æ€§èƒ½")
        print("2. è¿è¡Œç¬¬äºŒé˜¶æ®µè®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨")
        print("3. æˆ–è€…ä½¿ç”¨æ­¤æ¨¡å‹è¿›è¡Œæ¨ç†")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
