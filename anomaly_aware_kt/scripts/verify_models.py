#!/usr/bin/env python3
"""
æ¨¡å‹éªŒè¯è„šæœ¬ - æ£€æŸ¥æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
"""

import os
import torch
import glob
from datetime import datetime

def verify_all_models():
    """éªŒè¯æ‰€æœ‰æ¨¡å‹æ–‡ä»¶"""
    
    print("ğŸ” å¼€å§‹éªŒè¯æ¨¡å‹æ–‡ä»¶...")
    print("=" * 60)
    
    # æœç´¢æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºç›®å½•
    search_patterns = [
        "output/**/*.pt",
        "../output/**/*.pt",
        "../../output/**/*.pt"
    ]
    
    all_models = []
    
    for pattern in search_patterns:
        models = glob.glob(pattern, recursive=True)
        all_models.extend(models)
    
    # å»é‡
    all_models = list(set(all_models))
    
    if not all_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        print("\nğŸ” è¯·æ£€æŸ¥ä»¥ä¸‹ä½ç½®:")
        print("  - output/")
        print("  - ../output/")
        print("  - ../../output/")
        return []
    
    print(f"âœ… æ‰¾åˆ° {len(all_models)} ä¸ªæ¨¡å‹æ–‡ä»¶")
    print()
    
    valid_models = []
    
    for i, model_path in enumerate(sorted(all_models), 1):
        print(f"ğŸ“ æ¨¡å‹ {i}: {os.path.basename(model_path)}")
        print(f"   è·¯å¾„: {model_path}")
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            size_mb = os.path.getsize(model_path) / 1024 / 1024
            mtime = datetime.fromtimestamp(os.path.getmtime(model_path))
            
            print(f"   ğŸ“Š å¤§å°: {size_mb:.1f} MB")
            print(f"   ğŸ•’ æ—¶é—´: {mtime}")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # æ£€æŸ¥å†…å®¹
            epoch = checkpoint.get('epoch', 'Unknown')
            print(f"   ğŸ”¢ è½®æ¬¡: {epoch}")
            
            if 'metrics' in checkpoint:
                metrics = checkpoint['metrics']
                print(f"   ğŸ“ˆ æ€§èƒ½:")
                
                for key in ['recall', 'precision', 'f1_score', 'auc_roc']:
                    if key in metrics:
                        value = metrics[key]
                        if isinstance(value, (int, float)):
                            print(f"     {key}: {value:.4f}")
                
                valid_models.append({
                    'path': model_path,
                    'epoch': epoch,
                    'metrics': metrics,
                    'size_mb': size_mb,
                    'mtime': mtime
                })
            else:
                print("   âš ï¸  æ²¡æœ‰ä¿å­˜çš„æŒ‡æ ‡")
            
            print(f"   âœ… æ¨¡å‹æœ‰æ•ˆ")
            
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        
        print()
    
    # æ€»ç»“æœ€ä½³æ¨¡å‹
    if valid_models:
        print("=" * 60)
        print("ğŸ† æœ€ä½³æ¨¡å‹æ€»ç»“:")
        
        # æ‰¾åˆ°å„é¡¹æœ€ä½³
        best_recall = max(valid_models, key=lambda x: x['metrics'].get('recall', 0))
        best_auc = max(valid_models, key=lambda x: x['metrics'].get('auc_roc', 0))
        best_f1 = max(valid_models, key=lambda x: x['metrics'].get('f1_score', 0))
        
        print(f"\nğŸ“ˆ æœ€ä½³å¬å›ç‡: {best_recall['metrics'].get('recall', 0):.4f}")
        print(f"   æ¨¡å‹: {os.path.basename(best_recall['path'])}")
        print(f"   è½®æ¬¡: {best_recall['epoch']}")
        
        print(f"\nğŸ¯ æœ€ä½³AUC: {best_auc['metrics'].get('auc_roc', 0):.4f}")
        print(f"   æ¨¡å‹: {os.path.basename(best_auc['path'])}")
        print(f"   è½®æ¬¡: {best_auc['epoch']}")
        
        print(f"\nâš–ï¸  æœ€ä½³F1: {best_f1['metrics'].get('f1_score', 0):.4f}")
        print(f"   æ¨¡å‹: {os.path.basename(best_f1['path'])}")
        print(f"   è½®æ¬¡: {best_f1['epoch']}")
        
        # æ¨èæœ€ä½³æ¨¡å‹
        print("\n" + "=" * 60)
        print("ğŸ’¡ æ¨èä½¿ç”¨çš„æ¨¡å‹:")
        
        # ç»¼åˆè¯„åˆ†
        scored_models = []
        for model in valid_models:
            metrics = model['metrics']
            recall = metrics.get('recall', 0)
            auc = metrics.get('auc_roc', 0)
            f1 = metrics.get('f1_score', 0)
            
            # ç»¼åˆè¯„åˆ† (å¯ä»¥è°ƒæ•´æƒé‡)
            score = recall * 0.4 + auc * 0.4 + f1 * 0.2
            scored_models.append((score, model))
        
        scored_models.sort(reverse=True)
        best_overall = scored_models[0][1]
        
        print(f"\nğŸ† ç»¼åˆæœ€ä½³æ¨¡å‹:")
        print(f"   æ–‡ä»¶: {best_overall['path']}")
        print(f"   è½®æ¬¡: {best_overall['epoch']}")
        print(f"   å¬å›ç‡: {best_overall['metrics'].get('recall', 0):.4f}")
        print(f"   AUC: {best_overall['metrics'].get('auc_roc', 0):.4f}")
        print(f"   F1: {best_overall['metrics'].get('f1_score', 0):.4f}")
        
        print(f"\nğŸš€ ç»§ç»­è®­ç»ƒå‘½ä»¤:")
        print(f"python scripts/continue_training.py \\")
        print(f"    --checkpoint \"{best_overall['path']}\" \\")
        print(f"    --dataset assist17 \\")
        print(f"    --device cuda \\")
        print(f"    --with_pid --use_cl --proj \\")
        print(f"    --n_know 32 \\")
        print(f"    --batch_size 16 \\")
        print(f"    --test_batch_size 32 \\")
        print(f"    --epochs 50")
    
    return valid_models

if __name__ == "__main__":
    models = verify_all_models()
    
    if models:
        print(f"\nâœ… éªŒè¯å®Œæˆ! æ‰¾åˆ° {len(models)} ä¸ªæœ‰æ•ˆæ¨¡å‹")
    else:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶")
        print("ğŸ’¡ å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒ")
