#!/usr/bin/env python3
"""
å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥å·¥å…·

ç¡®ä¿ç¬¬ä¸€é˜¶æ®µã€ç¬¬äºŒé˜¶æ®µã€ç¬¬ä¸‰é˜¶æ®µçš„å…³é”®å‚æ•°ä¿æŒä¸€è‡´ï¼Œ
ä»¥ä¿è¯æœ€ç»ˆå¯¹æ¯”è¯„ä¼°çš„å…¬å¹³æ€§ã€‚
"""

import yaml
import os
from typing import Dict, List, Tuple


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        return {}
    
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def extract_critical_parameters(config: Dict, stage: str) -> Dict:
    """æå–å…³é”®å‚æ•°"""
    critical_params = {}
    
    if stage == "stage1":
        # ç¬¬ä¸€é˜¶æ®µå…³é”®å‚æ•°
        critical_params = {
            'batch_size': config.get('batch_size', None),
            'learning_rate': config.get('learning_rate', None),
            'd_model': config.get('d_model', None),
            'n_heads': config.get('n_heads', None),
            'n_layers': config.get('n_layers', None),
            'dropout': config.get('dropout', None),
            'lambda_cl': config.get('lambda_cl', None),
            'with_pid': config.get('with_pid', None),
            'n_know': config.get('n_know', None),
        }
    
    elif stage == "stage2":
        # ç¬¬äºŒé˜¶æ®µå…³é”®å‚æ•°ï¼ˆéœ€è¦ä¸ç¬¬ä¸€é˜¶æ®µä¸€è‡´çš„éƒ¨åˆ†ï¼‰
        critical_params = {
            'batch_size': config.get('batch_size', config.get('training', {}).get('batch_size', None)),
            'learning_rate': config.get('learning_rate', config.get('training', {}).get('learning_rate', None)),
            # ç¬¬äºŒé˜¶æ®µé€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ é€’ç¬¬ä¸€é˜¶æ®µçš„æ¨¡å‹å‚æ•°
            # è¿™äº›å‚æ•°åœ¨é…ç½®æ–‡ä»¶ä¸­å¯èƒ½ä¸å­˜åœ¨ï¼Œä½†åœ¨è„šæœ¬ä¸­æœ‰é»˜è®¤å€¼
        }
    
    # ç§»é™¤Noneå€¼
    return {k: v for k, v in critical_params.items() if v is not None}


def check_consistency(stage1_config: Dict, stage2_config: Dict) -> Tuple[bool, List[str]]:
    """æ£€æŸ¥å‚æ•°ä¸€è‡´æ€§"""
    stage1_params = extract_critical_parameters(stage1_config, "stage1")
    stage2_params = extract_critical_parameters(stage2_config, "stage2")
    
    inconsistencies = []
    
    # æ£€æŸ¥å…±åŒå‚æ•°
    common_params = set(stage1_params.keys()) & set(stage2_params.keys())
    
    for param in common_params:
        val1 = stage1_params[param]
        val2 = stage2_params[param]
        
        if val1 != val2:
            inconsistencies.append(f"{param}: Stage1={val1}, Stage2={val2}")
    
    return len(inconsistencies) == 0, inconsistencies


def print_parameter_summary(config: Dict, stage: str):
    """æ‰“å°å‚æ•°æ‘˜è¦"""
    params = extract_critical_parameters(config, stage)
    
    print(f"\nğŸ“‹ {stage.upper()} å…³é”®å‚æ•°:")
    for param, value in params.items():
        print(f"  {param}: {value}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥å·¥å…·")
    print("=" * 50)
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    stage1_config_path = "anomaly_aware_kt/configs/assist17_baseline.yaml"
    stage2_config_path = "anomaly_aware_kt/configs/assist17_curriculum.yaml"
    
    # åŠ è½½é…ç½®
    stage1_config = load_config(stage1_config_path)
    stage2_config = load_config(stage2_config_path)
    
    if not stage1_config:
        print(f"âŒ æ— æ³•åŠ è½½ç¬¬ä¸€é˜¶æ®µé…ç½®: {stage1_config_path}")
        return
    
    if not stage2_config:
        print(f"âŒ æ— æ³•åŠ è½½ç¬¬äºŒé˜¶æ®µé…ç½®: {stage2_config_path}")
        return
    
    print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶:")
    print(f"  Stage1: {stage1_config_path}")
    print(f"  Stage2: {stage2_config_path}")
    
    # æ‰“å°å‚æ•°æ‘˜è¦
    print_parameter_summary(stage1_config, "stage1")
    print_parameter_summary(stage2_config, "stage2")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    print(f"\nğŸ” ä¸€è‡´æ€§æ£€æŸ¥:")
    is_consistent, inconsistencies = check_consistency(stage1_config, stage2_config)
    
    if is_consistent:
        print("âœ… å‚æ•°ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡!")
        print("   æ‰€æœ‰å…³é”®å‚æ•°åœ¨ä¸¤ä¸ªé˜¶æ®µé—´ä¿æŒä¸€è‡´")
    else:
        print("âŒ å‘ç°å‚æ•°ä¸ä¸€è‡´:")
        for inconsistency in inconsistencies:
            print(f"   â€¢ {inconsistency}")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        print("1. ä¿®å¤é…ç½®æ–‡ä»¶ä¸­çš„ä¸ä¸€è‡´å‚æ•°")
        print("2. ç¡®ä¿ç¬¬äºŒé˜¶æ®µè„šæœ¬æ­£ç¡®ä¼ é€’ç¬¬ä¸€é˜¶æ®µçš„æ¨¡å‹å‚æ•°")
        print("3. é‡æ–°è¿è¡Œæ­¤æ£€æŸ¥å·¥å…·éªŒè¯ä¿®å¤")
    
    # é¢å¤–æ£€æŸ¥ï¼šç¬¬äºŒé˜¶æ®µç‰¹æœ‰å‚æ•°
    print(f"\nğŸ“Š ç¬¬äºŒé˜¶æ®µç‰¹æœ‰å‚æ•°:")
    stage2_specific = {
        'curriculum_strategy': stage2_config.get('curriculum_strategy'),
        'curriculum_epochs': stage2_config.get('curriculum_epochs'),
        'anomaly_ratio': stage2_config.get('anomaly_ratio'),
        'detector_hidden_dim': stage2_config.get('detector_hidden_dim'),
    }
    
    for param, value in stage2_specific.items():
        if value is not None:
            print(f"  {param}: {value}")
    
    # é‡è¦æé†’
    print(f"\nâš ï¸  é‡è¦æé†’:")
    print("1. ç¬¬äºŒé˜¶æ®µå¿…é¡»ä½¿ç”¨ä¸ç¬¬ä¸€é˜¶æ®µç›¸åŒçš„æ¨¡å‹æ¶æ„å‚æ•°")
    print("2. å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰è®­ç»ƒå‚æ•°ä¹Ÿåº”ä¿æŒä¸€è‡´")
    print("3. å¼‚å¸¸æ£€æµ‹å™¨çš„å‚æ•°å¯ä»¥ç‹¬ç«‹ä¼˜åŒ–")
    print("4. è¯¾ç¨‹å­¦ä¹ çš„å‚æ•°ä¸å½±å“æœ€ç»ˆå¯¹æ¯”çš„å…¬å¹³æ€§")
    
    # ä½¿ç”¨å»ºè®®
    print(f"\nğŸš€ ä½¿ç”¨å»ºè®®:")
    print("è¿è¡Œç¬¬äºŒé˜¶æ®µæ—¶ï¼Œç¡®ä¿ä¼ é€’æ­£ç¡®çš„ç¬¬ä¸€é˜¶æ®µå‚æ•°:")
    print("python anomaly_aware_kt/scripts/run_stage2_curriculum.py \\")
    print("  --dataset assist17 \\")
    print("  --baseline_model_path output/stage1_xxx/baseline/best_model.pt \\")
    print("  --auto_config \\")
    print(f"  --d_model {stage1_config.get('d_model', 128)} \\")
    print(f"  --n_heads {stage1_config.get('n_heads', 8)} \\")
    print(f"  --n_layers {stage1_config.get('n_layers', 3)} \\")
    print(f"  --dropout {stage1_config.get('dropout', 0.2)}")
    if stage1_config.get('with_pid', False):
        print("  --with_pid")


if __name__ == "__main__":
    main()
